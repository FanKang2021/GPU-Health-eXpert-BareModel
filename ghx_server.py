#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GHX (GPU Health Expert) ç»Ÿä¸€æœåŠ¡ - é›†æˆæ•°æ®æ”¶é›†ã€èŠ‚ç‚¹çŠ¶æ€æŸ¥è¯¢å’ŒJobç®¡ç†
æ•´åˆäº†åŸgpu_collector_serviceå’Œgpu_cliçš„åŠŸèƒ½
æä¾›å®Œæ•´çš„GPUå¥åº·æ£€æŸ¥è§£å†³æ–¹æ¡ˆ
"""

import sqlite3
from datetime import datetime, timedelta
from datetime import timezone
from functools import wraps
import json
import logging
import os
import subprocess
import time
import uuid
import glob
import queue
import threading
import yaml
from typing import Dict, Any, List, Optional
from flask import Flask, request, jsonify, Response
from flask_cors import CORS

# æ·»åŠ kuberneteså®¢æˆ·ç«¯å¯¼å…¥
try:
    from kubernetes import client, config, watch
    from kubernetes.client.rest import ApiException
    KUBERNETES_AVAILABLE = True
except ImportError:
    KUBERNETES_AVAILABLE = False
    print("Warning: kubernetes package not available, falling back to kubectl commands")

# å¯¼å…¥backend_rate_limitæ¨¡å—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/var/log/ghx_service.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

try:
    from backend_rate_limit import (
        get_rate_limit_decorator,
        setup_rate_limit_error_handlers,
        init_rate_limit,
        get_rate_limit_stats,
        log_rate_limit_event
    )
    logger.info("æˆåŠŸå¯¼å…¥backend_rate_limitæ¨¡å—")
except ImportError:
    logger.warning("æ— æ³•å¯¼å…¥backend_rate_limitæ¨¡å—ï¼Œä½¿ç”¨ç®€å•é™æµ")
    
    def get_rate_limit_decorator():
        """ç®€å•çš„é™æµè£…é¥°å™¨"""
        def decorator(f):
            @wraps(f)
            def decorated_function(*args, **kwargs):
                return f(*args, **kwargs)
            return decorated_function
        return decorator

    def setup_rate_limit_error_handlers(app):
        """è®¾ç½®é™æµé”™è¯¯å¤„ç†å™¨"""
        pass

    def init_rate_limit(app, use_redis=False, use_flask_limiter=False):
        """åˆå§‹åŒ–é™æµ"""
        return "simple"

    def get_rate_limit_stats():
        """è·å–é™æµç»Ÿè®¡"""
        return {}

    def log_rate_limit_event(client_ip, action, result):
        """è®°å½•é™æµäº‹ä»¶"""
        logger.info(f"é™æµäº‹ä»¶: {client_ip} - {action} - {result}")

# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__)

# ==================== CORSé…ç½® ====================

def get_cors_origins():
    """è·å–CORSå…è®¸çš„æºåœ°å€"""
    # é»˜è®¤çš„CORSåœ°å€ï¼ˆå¼€å‘ç¯å¢ƒå¸¸ç”¨åœ°å€ï¼‰
    origins = [
        "http://localhost:3000",
        "http://localhost:31033",
        "http://127.0.0.1:3000",
        "http://127.0.0.1:31033"
    ]
    
    # ä»ç¯å¢ƒå˜é‡è·å–CORS_ORIGINSï¼Œæ”¯æŒå¤šä¸ªåœ°å€ç”¨é€—å·åˆ†éš”
    cors_origins_env = os.getenv('CORS_ORIGINS', '')
    
    if cors_origins_env:
        # å¦‚æœè®¾ç½®äº†ç¯å¢ƒå˜é‡ï¼Œåœ¨é»˜è®¤åœ°å€åŸºç¡€ä¸Šæ·»åŠ ç¯å¢ƒå˜é‡ä¸­çš„åœ°å€
        additional_origins = [origin.strip() for origin in cors_origins_env.split(',') if origin.strip()]
        origins.extend(additional_origins)
    
    # å»é‡å¹¶è¿‡æ»¤ç©ºå€¼
    origins = list(set([origin for origin in origins if origin]))
    
    logger.info(f"CORSå…è®¸çš„æºåœ°å€: {origins}")
    return origins

# é…ç½®CORS
CORS(app, 
     origins=get_cors_origins(),
     methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
     allow_headers=["Content-Type", "Authorization", "X-Requested-With"],
     supports_credentials=True)

# ==================== å®æ—¶JobçŠ¶æ€ç›‘å¬ ====================

# å­˜å‚¨æ‰€æœ‰SSEè¿æ¥çš„å®¢æˆ·ç«¯
sse_clients = set()

# å®šæ—¶çŠ¶æ€æ£€æŸ¥çº¿ç¨‹
status_check_thread = None
status_check_running = False

# æ·»åŠ å…¨å±€å˜é‡ç”¨äºInformeræœºåˆ¶
pod_cache = {}  # æœ¬åœ°Podç¼“å­˜
last_resource_version = None  # èµ„æºç‰ˆæœ¬æ§åˆ¶
last_sync_time = 0  # ä¸Šæ¬¡åŒæ­¥æ—¶é—´
sync_interval = 300  # åŒæ­¥é—´éš”ï¼ˆ5åˆ†é’Ÿï¼‰

def notify_job_status_change(job_id: str, status: str, node_name: str = None):
    """é€šçŸ¥æ‰€æœ‰SSEå®¢æˆ·ç«¯JobçŠ¶æ€å˜åŒ–"""
    global sse_clients
    
    message = {
        "type": "job_status_change",
        "job_id": job_id,
        "status": status,
        "node_name": node_name,
        "timestamp": time.time()
    }
    
    logger.info(f"å‡†å¤‡é€šçŸ¥SSEå®¢æˆ·ç«¯: {message}")
    
    if not sse_clients:
        logger.warning("æ²¡æœ‰SSEå®¢æˆ·ç«¯è¿æ¥ï¼Œæ— æ³•å‘é€çŠ¶æ€æ›´æ–°")
        return
    
    # ç§»é™¤æ–­å¼€çš„è¿æ¥
    disconnected_clients = set()
    for client in sse_clients:
        try:
            client.put(f"data: {json.dumps(message)}\n\n")
            logger.debug(f"å·²å‘é€çŠ¶æ€æ›´æ–°åˆ°SSEå®¢æˆ·ç«¯: {job_id} -> {status}")
        except Exception as e:
            logger.warning(f"å‘é€çŠ¶æ€æ›´æ–°åˆ°SSEå®¢æˆ·ç«¯å¤±è´¥: {e}")
            disconnected_clients.add(client)
    
    # æ¸…ç†æ–­å¼€çš„è¿æ¥
    sse_clients -= disconnected_clients
    if disconnected_clients:
        logger.info(f"æ¸…ç†äº† {len(disconnected_clients)} ä¸ªæ–­å¼€çš„SSEè¿æ¥")
    
    logger.info(f"æˆåŠŸé€šçŸ¥ {len(sse_clients) - len(disconnected_clients)} ä¸ªSSEå®¢æˆ·ç«¯JobçŠ¶æ€å˜åŒ–")

def notify_diagnostic_results_update():
    """é€šçŸ¥SSEå®¢æˆ·ç«¯è¯Šæ–­ç»“æœå·²æ›´æ–°"""
    global sse_clients
    
    message = {
        "type": "diagnostic_results_updated",
        "message": "è¯Šæ–­ç»“æœå·²æ›´æ–°ï¼Œè¯·åˆ·æ–°æŸ¥çœ‹",
        "timestamp": time.time()
    }
    
    logger.info("å‡†å¤‡é€šçŸ¥SSEå®¢æˆ·ç«¯è¯Šæ–­ç»“æœå·²æ›´æ–°")
    
    if not sse_clients:
        logger.warning("æ²¡æœ‰SSEå®¢æˆ·ç«¯è¿æ¥ï¼Œæ— æ³•å‘é€è¯Šæ–­ç»“æœæ›´æ–°é€šçŸ¥")
        return
    
    # ç§»é™¤æ–­å¼€çš„è¿æ¥
    disconnected_clients = set()
    for client in sse_clients:
        try:
            client.put(f"data: {json.dumps(message)}\n\n")
            logger.debug(f"å·²å‘é€è¯Šæ–­ç»“æœæ›´æ–°é€šçŸ¥åˆ°SSEå®¢æˆ·ç«¯")
        except Exception as e:
            logger.warning(f"å‘é€è¯Šæ–­ç»“æœæ›´æ–°é€šçŸ¥åˆ°SSEå®¢æˆ·ç«¯å¤±è´¥: {e}")
            disconnected_clients.add(client)
    
    # æ¸…ç†æ–­å¼€çš„è¿æ¥
    sse_clients -= disconnected_clients
    if disconnected_clients:
        logger.info(f"æ¸…ç†äº† {len(disconnected_clients)} ä¸ªæ–­å¼€çš„SSEè¿æ¥")
    
    logger.info(f"æˆåŠŸé€šçŸ¥ {len(sse_clients) - len(disconnected_clients)} ä¸ªSSEå®¢æˆ·ç«¯è¯Šæ–­ç»“æœå·²æ›´æ–°")

def get_kubernetes_job_status(job_id: str):
    """è·å–Kubernetes Jobçš„å®æ—¶çŠ¶æ€"""
    try:
        # é¦–å…ˆæŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„Jobï¼ˆå› ä¸ºä¸€ä¸ªjob_idå¯èƒ½å¯¹åº”å¤šä¸ªèŠ‚ç‚¹çš„Jobï¼‰
        # å°è¯•å¤šç§æ ‡ç­¾é€‰æ‹©å™¨
        job_found = False
        jobs = []
        
        # ç­–ç•¥1: ä½¿ç”¨job-idæ ‡ç­¾
        result = subprocess.run([
            'kubectl', 'get', 'jobs', '-n', 'gpu-health-expert', 
            '-l', f'job-id={job_id}', '-o', 'json'
        ], capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0:
            jobs_data = json.loads(result.stdout)
            jobs = jobs_data.get('items', [])
            if jobs:
                logger.info(f"é€šè¿‡job-idæ ‡ç­¾æ‰¾åˆ° {len(jobs)} ä¸ªJob")
                job_found = True
        
        # ç­–ç•¥2: å¦‚æœç­–ç•¥1å¤±è´¥ï¼Œå°è¯•é€šè¿‡Jobåç§°æ¨¡å¼æŸ¥æ‰¾
        if not job_found:
            logger.info(f"é€šè¿‡job-idæ ‡ç­¾æœªæ‰¾åˆ°Jobï¼Œå°è¯•é€šè¿‡åç§°æ¨¡å¼æŸ¥æ‰¾: {job_id}")
            result = subprocess.run([
                'kubectl', 'get', 'jobs', '-n', 'gpu-health-expert', 
                '-o', 'json'
            ], capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                all_jobs_data = json.loads(result.stdout)
                all_jobs = all_jobs_data.get('items', [])
                
                # æŸ¥æ‰¾åç§°åŒ…å«job_idçš„Job
                for job in all_jobs:
                    job_name = job.get('metadata', {}).get('name', '')
                    if job_id in job_name:
                        jobs.append(job)
                        logger.info(f"é€šè¿‡åç§°æ¨¡å¼æ‰¾åˆ°Job: {job_name}")
                
                if jobs:
                    job_found = True
                    logger.info(f"é€šè¿‡åç§°æ¨¡å¼æ‰¾åˆ° {len(jobs)} ä¸ªJob")
        
        if not job_found:
            logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…çš„Job: job-id={job_id}")
            return None
        
        # åˆå¹¶æ‰€æœ‰Jobçš„çŠ¶æ€
        total_completions = 0
        total_failed = 0
        total_active = 0
        all_pod_statuses = []
        
        for job in jobs:
            job_status = job.get('status', {})
            total_completions += job_status.get('succeeded', 0)
            total_failed += job_status.get('failed', 0)
            total_active += job_status.get('active', 0)
            
            # è·å–æ¯ä¸ªJobçš„PodçŠ¶æ€
            job_name = job.get('metadata', {}).get('name', '')
            if job_name:
                try:
                    pod_result = subprocess.run([
                        'kubectl', 'get', 'pods', '-n', 'gpu-health-expert', 
                        '-l', f'job-name={job_name}', '-o', 'json'
                    ], capture_output=True, text=True, timeout=30)
                    
                    if pod_result.returncode == 0:
                        pods_data = json.loads(pod_result.stdout)
                        pods = pods_data.get('items', [])
                        
                        for pod in pods:
                            pod_phase = pod.get('status', {}).get('phase', 'Unknown')
                            container_statuses = pod.get('status', {}).get('containerStatuses', [])
                            
                            logger.info(f"Pod {pod.get('metadata', {}).get('name', 'unknown')} çŠ¶æ€åˆ†æ:")
                            logger.info(f"  - pod_phase: {pod_phase}")
                            logger.info(f"  - container_statuses: {len(container_statuses)} ä¸ªå®¹å™¨")
                            
                            # é¦–å…ˆæ£€æŸ¥å®¹å™¨çŠ¶æ€ï¼Œå› ä¸ºå®ƒæ›´å‡†ç¡®åæ˜ å®é™…è¿è¡ŒçŠ¶æ€
                            if container_statuses:
                                container_status = container_statuses[0]
                                container_state = container_status.get('state', {})
                                logger.info(f"  - container_state: {container_state}")
                                
                                if container_state.get('running'):
                                    all_pod_statuses.append('Running')
                                    logger.info(f"  - æ£€æµ‹åˆ°RunningçŠ¶æ€ (æ¥è‡ªcontainer_state)")
                                elif container_state.get('terminated'):
                                    exit_code = container_state['terminated'].get('exitCode', 0)
                                    if exit_code == 0:
                                        all_pod_statuses.append('Completed')
                                        logger.info(f"  - æ£€æµ‹åˆ°CompletedçŠ¶æ€ (æ¥è‡ªcontainer_state)")
                                    else:
                                        reason = container_state['terminated'].get('reason', 'Error')
                                        all_pod_statuses.append(f'Failed: {reason}')
                                        logger.info(f"  - æ£€æµ‹åˆ°FailedçŠ¶æ€ (æ¥è‡ªcontainer_state): {reason}")
                                elif container_state.get('waiting'):
                                    reason = container_state['waiting'].get('reason', 'Waiting')
                                    all_pod_statuses.append(f'Waiting: {reason}')
                                    logger.info(f"  - æ£€æµ‹åˆ°WaitingçŠ¶æ€ (æ¥è‡ªcontainer_state): {reason}")
                                else:
                                    # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å®¹å™¨çŠ¶æ€ï¼Œä½¿ç”¨pod_phase
                                    if pod_phase == 'Running':
                                        all_pod_statuses.append('Running')
                                        logger.info(f"  - æ£€æµ‹åˆ°RunningçŠ¶æ€ (æ¥è‡ªpod_phase)")
                                    elif pod_phase == 'Completed':
                                        all_pod_statuses.append('Completed')
                                        logger.info(f"  - æ£€æµ‹åˆ°CompletedçŠ¶æ€ (æ¥è‡ªpod_phase)")
                                    elif pod_phase == 'Failed':
                                        all_pod_statuses.append('Failed')
                                        logger.info(f"  - æ£€æµ‹åˆ°FailedçŠ¶æ€ (æ¥è‡ªpod_phase)")
                                    else:
                                        all_pod_statuses.append(pod_phase)
                                        logger.info(f"  - ä½¿ç”¨pod_phase: {pod_phase}")
                            else:
                                # æ²¡æœ‰å®¹å™¨çŠ¶æ€ï¼Œä½¿ç”¨pod_phase
                                if pod_phase == 'Running':
                                    all_pod_statuses.append('Running')
                                    logger.info(f"  - æ£€æµ‹åˆ°RunningçŠ¶æ€ (æ¥è‡ªpod_phase)")
                                elif pod_phase == 'Completed':
                                    all_pod_statuses.append('Completed')
                                    logger.info(f"  - æ£€æµ‹åˆ°CompletedçŠ¶æ€ (æ¥è‡ªpod_phase)")
                                elif pod_phase == 'Failed':
                                    all_pod_statuses.append('Failed')
                                    logger.info(f"  - æ£€æµ‹åˆ°FailedçŠ¶æ€ (æ¥è‡ªpod_phase)")
                                elif pod_phase == 'Pending':
                                    all_pod_statuses.append('Pending')
                                    logger.info(f"  - æ£€æµ‹åˆ°PendingçŠ¶æ€ (æ¥è‡ªpod_phase)")
                                else:
                                    all_pod_statuses.append(pod_phase)
                                    logger.info(f"  - ä½¿ç”¨pod_phase: {pod_phase}")
                                    
                                # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœpod_phaseæ˜¯Runningä½†å®¹å™¨çŠ¶æ€ä¸ºç©ºï¼Œå¯èƒ½æ˜¯å®¹å™¨åˆšå¯åŠ¨
                                if pod_phase == 'Running' and not container_statuses:
                                    logger.info(f"  - PodçŠ¶æ€ä¸ºRunningä½†å®¹å™¨çŠ¶æ€ä¸ºç©ºï¼Œå¯èƒ½æ˜¯å®¹å™¨åˆšå¯åŠ¨")
                                    all_pod_statuses.append('Running')
                                
                except Exception as e:
                    logger.warning(f"è·å–PodçŠ¶æ€å¤±è´¥: {e}")
        
        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if total_failed > 0:
            status = 'Failed'
        elif total_completions > 0 and total_active == 0:
            status = 'Completed'
        elif total_active > 0:
            status = 'Running'
        else:
            status = 'Pending'
        
        # ç¡®å®šPodçŠ¶æ€ - ä¼˜å…ˆä½¿ç”¨Podçš„å®é™…çŠ¶æ€
        if not all_pod_statuses:
            pod_status = 'Unknown'
        else:
            # ç»Ÿè®¡å„ç§çŠ¶æ€çš„æ•°é‡
            running_count = sum(1 for s in all_pod_statuses if 'Running' in s)
            completed_count = sum(1 for s in all_pod_statuses if 'Completed' in s)
            failed_count = sum(1 for s in all_pod_statuses if 'Failed' in s)
            waiting_count = sum(1 for s in all_pod_statuses if 'Waiting' in s)
            pending_count = sum(1 for s in all_pod_statuses if s == 'Pending')
            
            logger.info(f"PodçŠ¶æ€ç»Ÿè®¡: Running={running_count}, Completed={completed_count}, Failed={failed_count}, Waiting={waiting_count}, Pending={pending_count}")
            
            if failed_count > 0:
                pod_status = 'Failed'
            elif completed_count > 0 and running_count == 0 and waiting_count == 0 and pending_count == 0:
                pod_status = 'Completed'
            elif running_count > 0:
                pod_status = 'Running'
            elif waiting_count > 0 or pending_count > 0:
                pod_status = 'Pending'  # ç»Ÿä¸€ä½¿ç”¨PendingçŠ¶æ€
            else:
                pod_status = 'Unknown'
        
        logger.info(f"Job {job_id} æœ€ç»ˆçŠ¶æ€: {status}, PodçŠ¶æ€: {pod_status}")
        logger.info(f"Jobç»Ÿè®¡: completions={total_completions}, failed={total_failed}, active={total_active}")
        logger.info(f"PodçŠ¶æ€åˆ—è¡¨: {all_pod_statuses}")
        
        return {
            'pod_status': pod_status,
            'job_status': status,
            'total_completions': total_completions,
            'total_failed': total_failed,
            'total_active': total_active,
            'all_pod_statuses': all_pod_statuses
        }
        
    except Exception as e:
        logger.error(f"è·å–Kubernetes JobçŠ¶æ€å¤±è´¥: {e}")
        return None

def extract_job_id_from_pod_name(pod_name):
    """ä»Podåç§°ä¸­æå–job_id"""
    try:
        # Podåç§°æ ¼å¼: ghx-manual-job-{job_id}-{node_name}-{random_suffix}
        # ä¾‹å¦‚: ghx-manual-job-manual-1756721527-21039310-hd03-gpu2-0062-mdtlt
        parts = pod_name.split('-')
        if len(parts) >= 7:  # ghx, manual, job, manual, timestamp, random_id, node_name, ...
            # æå–job_idéƒ¨åˆ†: manual-1756721527-21039310
            job_id_parts = parts[3:6]  # manual, timestamp, random_id
            job_id = '-'.join(job_id_parts)
            return job_id
        return None
    except Exception as e:
        logger.warning(f"ä»Podåç§°æå–job_idå¤±è´¥: {pod_name}, é”™è¯¯: {e}")
        return None

def convert_kubectl_status_to_standard(kubectl_status, ready):
    """å°†kubectlçŠ¶æ€è½¬æ¢ä¸ºæ ‡å‡†çŠ¶æ€"""
    status = kubectl_status.lower()
    
    # æ˜ å°„kubectlçŠ¶æ€åˆ°æ ‡å‡†çŠ¶æ€
    status_mapping = {
        'pending': 'Pending',
        'running': 'Running',
        'succeeded': 'Completed',
        'failed': 'Failed',
        'unknown': 'Unknown',
        'crashloopbackoff': 'Failed',
        'error': 'Failed',
        'completed': 'Completed'
    }
    
    # æ£€æŸ¥readyçŠ¶æ€
    if ready and '/' in ready:
        ready_parts = ready.split('/')
        if len(ready_parts) == 2:
            ready_count = int(ready_parts[0])
            total_count = int(ready_parts[1])
            # å¦‚æœæ‰€æœ‰å®¹å™¨éƒ½readyä¸”çŠ¶æ€æ˜¯runningï¼Œåˆ™è®¤ä¸ºæ˜¯Running
            if ready_count == total_count and status == 'running':
                return 'Running'
            # å¦‚æœéƒ¨åˆ†readyä¸”çŠ¶æ€æ˜¯runningï¼Œåˆ™è®¤ä¸ºæ˜¯Runningï¼ˆå®¹å™¨å¯åŠ¨ä¸­ï¼‰
            elif ready_count > 0 and status == 'running':
                return 'Running'
    
    # è¿”å›æ˜ å°„çš„çŠ¶æ€ï¼Œå¦‚æœæ²¡æœ‰æ˜ å°„åˆ™è¿”å›åŸçŠ¶æ€
    return status_mapping.get(status, kubectl_status)

def get_job_from_db(job_id):
    """ä»æ•°æ®åº“ä¸­è·å–Jobä¿¡æ¯"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('SELECT * FROM diagnostic_jobs WHERE job_id = ?', (job_id,))
        job = cursor.fetchone()
        conn.close()
        
        if job:
            return {
                'job_id': job[1],  # job_id
                'status': job[7],  # status
                'node_name': job[4],  # selected_nodes (ä½œä¸ºnode_name)
                'created_at': job[8],  # created_at
                'updated_at': job[9]  # updated_at
            }
        return None
    except Exception as e:
        logger.warning(f"ä»æ•°æ®åº“è·å–Jobå¤±è´¥: {e}")
        return None

def get_pod_cache_key(pod_name):
    """ç”ŸæˆPodç¼“å­˜é”®"""
    return pod_name

def update_pod_cache(pod_name, pod_data):
    """æ›´æ–°Podç¼“å­˜"""
    global pod_cache
    cache_key = get_pod_cache_key(pod_name)
    pod_cache[cache_key] = {
        'data': pod_data,
        'timestamp': time.time(),
        'resource_version': pod_data.metadata.resource_version if hasattr(pod_data.metadata, 'resource_version') else None
    }

def get_pod_from_cache(pod_name):
    """ä»ç¼“å­˜è·å–Podæ•°æ®"""
    global pod_cache
    cache_key = get_pod_cache_key(pod_name)
    return pod_cache.get(cache_key)

def sync_pod_cache_from_api():
    """ä»APIåŒæ­¥Podç¼“å­˜ï¼ˆå®šæœŸå…¨é‡åŒæ­¥ï¼‰"""
    global last_sync_time, last_resource_version, pod_cache
    
    try:
        if not kubernetes_client:
            logger.warning("Kuberneteså®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè·³è¿‡ç¼“å­˜åŒæ­¥")
            return
        
        v1, batch_v1 = kubernetes_client
        current_time = time.time()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åŒæ­¥
        if current_time - last_sync_time < sync_interval:
            return
        
        logger.info("å¼€å§‹å®šæœŸåŒæ­¥Podç¼“å­˜...")
        
        # è·å–æ‰€æœ‰ç›¸å…³Pod
        pods = v1.list_namespaced_pod(
            namespace='gpu-health-expert',
            label_selector='app=ghx-manual,job-type=manual'
        )
        
        # æ›´æ–°ç¼“å­˜
        new_cache = {}
        for pod in pods.items:
            cache_key = get_pod_cache_key(pod.metadata.name)
            new_cache[cache_key] = {
                'data': pod,
                'timestamp': current_time,
                'resource_version': pod.metadata.resource_version if hasattr(pod.metadata, 'resource_version') else None
            }
        
        # æ£€æŸ¥ç¼“å­˜å˜åŒ–
        cache_changes = []
        for pod_name, pod_info in new_cache.items():
            old_pod_info = pod_cache.get(pod_name)
            if not old_pod_info or old_pod_info['resource_version'] != pod_info['resource_version']:
                cache_changes.append(pod_name)
        
        # æ›´æ–°å…¨å±€ç¼“å­˜
        pod_cache = new_cache
        last_sync_time = current_time
        
        if pods.metadata and hasattr(pods.metadata, 'resource_version'):
            last_resource_version = pods.metadata.resource_version
        
        logger.info(f"Podç¼“å­˜åŒæ­¥å®Œæˆï¼Œå…± {len(new_cache)} ä¸ªPodï¼Œ{len(cache_changes)} ä¸ªå˜åŒ–")
        
    except Exception as e:
        logger.error(f"åŒæ­¥Podç¼“å­˜å¤±è´¥: {e}")

def handle_pod_status_change(pod):
    """å¤„ç†PodçŠ¶æ€å˜åŒ–ï¼ˆInformeré£æ ¼ï¼‰"""
    try:
        pod_name = pod.metadata.name
        pod_status = pod.status.phase if pod.status else 'Unknown'
        
        # ä»Podåç§°ä¸­æå–job_id
        job_id = extract_job_id_from_pod_name(pod_name)
        if not job_id:
            logger.debug(f"æ— æ³•ä»Podåç§°æå–job_id: {pod_name}")
            return
        
        # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨è¿™ä¸ªJob
        db_job = get_job_from_db(job_id)
        if not db_job:
            logger.debug(f"æ•°æ®åº“ä¸­ä¸å­˜åœ¨Job: {job_id}")
            return
        
        # è½¬æ¢çŠ¶æ€æ ¼å¼ - å°†PodçŠ¶æ€è½¬æ¢ä¸ºå°å†™åä¼ ç»™è½¬æ¢å‡½æ•°
        pod_status_standard = convert_kubectl_status_to_standard(pod_status.lower(), "1/1" if pod_status == "Running" else "0/1")
        
        # æ£€æŸ¥çŠ¶æ€æ˜¯å¦æœ‰å˜åŒ– - ç»Ÿä¸€è½¬æ¢ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒ
        if pod_status_standard.lower() != db_job['status'].lower():
            logger.info(f"ğŸ”„ PodçŠ¶æ€å˜åŒ–: {job_id} {db_job['status']} -> {pod_status_standard}")
            
            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute('''
                UPDATE diagnostic_jobs 
                SET status = ?, updated_at = datetime('now', 'localtime')
                WHERE job_id = ?
            ''', (pod_status_standard, job_id))
            conn.commit()
            conn.close()
            
            # é€šçŸ¥SSEå®¢æˆ·ç«¯
            notify_job_status_change(job_id, pod_status_standard, db_job['node_name'] if 'node_name' in db_job.keys() else None)
            
            # å¦‚æœPodå®Œæˆï¼Œå¤„ç†ç»“æœæ”¶é›†
            if pod_status_standard in ['Succeeded', 'Failed', 'Completed']:
                handle_job_completion(job_id)
        else:
            logger.debug(f"Pod {pod_name} çŠ¶æ€æ— å˜åŒ–: {pod_status_standard}")
            
    except Exception as e:
        logger.warning(f"å¤„ç†PodçŠ¶æ€å˜åŒ–å¤±è´¥: {e}")

def pod_status_callback(event):
    """PodçŠ¶æ€å˜åŒ–å›è°ƒå‡½æ•°ï¼ˆInformeré£æ ¼ï¼‰"""
    try:
        event_type = event['type']
        pod = event['object']
        pod_name = pod.metadata.name
        
        logger.info(f"ğŸ”„ æ”¶åˆ°Podäº‹ä»¶: {event_type} - {pod_name}")
        
        # æ›´æ–°æœ¬åœ°ç¼“å­˜
        update_pod_cache(pod_name, pod)
        
        # æ ¹æ®äº‹ä»¶ç±»å‹å¤„ç†
        if event_type == 'MODIFIED':
            handle_pod_status_change(pod)
        elif event_type == 'ADDED':
            logger.info(f"æ–°å¢Pod: {pod_name}")
            handle_pod_status_change(pod)
        elif event_type == 'DELETED':
            logger.info(f"åˆ é™¤Pod: {pod_name}")
            # ä»ç¼“å­˜ä¸­ç§»é™¤
            cache_key = get_pod_cache_key(pod_name)
            if cache_key in pod_cache:
                del pod_cache[cache_key]
        
    except Exception as e:
        logger.warning(f"å¤„ç†Watchäº‹ä»¶å¤±è´¥: {e}")

def start_kubernetes_watch_thread():
    """å¯åŠ¨Kubernetes Watchçº¿ç¨‹ï¼ˆåŸºäºInformeræœºåˆ¶ä¼˜åŒ–ï¼‰"""
    global status_check_thread, status_check_running, last_resource_version
    
    if status_check_running:
        logger.info("çŠ¶æ€æ£€æŸ¥çº¿ç¨‹å·²åœ¨è¿è¡Œä¸­")
        return
    
    logger.info("æ­£åœ¨å¯åŠ¨Kubernetes Watchçº¿ç¨‹ï¼ˆInformerä¼˜åŒ–ç‰ˆï¼‰...")
    status_check_running = True
    
    def kubernetes_watch_worker():
        """Kubernetes Watchå·¥ä½œçº¿ç¨‹ï¼ˆInformeré£æ ¼ï¼‰"""
        global last_resource_version
        
        thread_id = threading.current_thread().ident
        logger.info(f"Kubernetes Watchå·¥ä½œçº¿ç¨‹å·²å¯åŠ¨ (çº¿ç¨‹ID: {thread_id})")
        
        v1, batch_v1 = kubernetes_client
        retry_count = 0
        max_retries = 10  # æœ€å¤§é‡è¯•æ¬¡æ•°
        
        # åˆå§‹åŒ–æ—¶è¿›è¡Œå…¨é‡åŒæ­¥
        sync_pod_cache_from_api()
        
        while status_check_running:
            try:
                # å®šæœŸåŒæ­¥ç¼“å­˜
                sync_pod_cache_from_api()
                
                # æ„å»ºWatchå‚æ•°
                watch_params = {
                    'namespace': 'gpu-health-expert',
                    'label_selector': 'app=ghx-manual,job-type=manual'
                }
                
                # å¦‚æœæœ‰èµ„æºç‰ˆæœ¬ï¼Œä»è¯¥ç‰ˆæœ¬å¼€å§‹Watch
                if last_resource_version:
                    watch_params['resource_version'] = last_resource_version
                    logger.info(f"ä»èµ„æºç‰ˆæœ¬ {last_resource_version} å¼€å§‹Watch")
                
                # åˆ›å»ºWatchå¯¹è±¡
                from kubernetes import watch
                w = watch.Watch()
                
                # å¼€å§‹Watchæµ
                for event in w.stream(v1.list_namespaced_pod, **watch_params):
                    if not status_check_running:
                        logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢Watch")
                        break
                    
                    # æ›´æ–°èµ„æºç‰ˆæœ¬
                    if hasattr(event['object'].metadata, 'resource_version'):
                        last_resource_version = event['object'].metadata.resource_version
                    
                    # è°ƒç”¨å›è°ƒå‡½æ•°å¤„ç†äº‹ä»¶
                    pod_status_callback(event)
                
                # Watchæµç»“æŸï¼ˆé€šå¸¸æ˜¯ç½‘ç»œé—®é¢˜æˆ–è¶…æ—¶ï¼‰
                logger.info("Kubernetes Watchæµç»“æŸï¼Œå‡†å¤‡é‡æ–°å¯åŠ¨...")
                w.stop()
                
                # çŸ­æš‚ç­‰å¾…åé‡æ–°å¯åŠ¨Watch
                if status_check_running:
                    logger.info("5ç§’åé‡æ–°å¯åŠ¨Watch...")
                    time.sleep(5)
                
            except Exception as e:
                retry_count += 1
                logger.error(f"Kubernetes Watchå¼‚å¸¸ (é‡è¯• {retry_count}/{max_retries}): {e}")
                
                if retry_count >= max_retries:
                    logger.error(f"Watché‡è¯•æ¬¡æ•°è¾¾åˆ°ä¸Šé™ï¼Œå›é€€åˆ°kubectl watchæ¨¡å¼...")
                    start_kubectl_watch_thread()
                    break
                
                # æŒ‡æ•°é€€é¿ï¼š1ç§’, 2ç§’, 4ç§’, 8ç§’, 16ç§’...
                wait_time = min(2 ** retry_count, 30)  # æœ€å¤§ç­‰å¾…30ç§’
                logger.info(f"{wait_time}ç§’åé‡è¯•Watch...")
                try:
                    w.stop()
                except:
                    pass
                time.sleep(wait_time)
                continue  # ç»§ç»­å¾ªç¯ï¼Œé‡è¯•Watch
        
        logger.info(f"Kubernetes Watchå·¥ä½œçº¿ç¨‹å·²é€€å‡º (çº¿ç¨‹ID: {thread_id})")
    
    # å¯åŠ¨çº¿ç¨‹
    status_check_thread = threading.Thread(target=kubernetes_watch_worker, daemon=True)
    status_check_thread.start()
    time.sleep(0.1)
    
    if status_check_thread.is_alive():
        logger.info(f"Kubernetes Watchçº¿ç¨‹å·²æˆåŠŸå¯åŠ¨ (çº¿ç¨‹ID: {status_check_thread.ident})")
    else:
        logger.error("Kubernetes Watchçº¿ç¨‹å¯åŠ¨å¤±è´¥")
        status_check_running = False

def start_kubectl_watch_thread():
    """å¯åŠ¨kubectl watch Podç›‘å¬çº¿ç¨‹"""
    global status_check_thread, status_check_running
    
    if status_check_running:
        logger.info("çŠ¶æ€æ£€æŸ¥çº¿ç¨‹å·²åœ¨è¿è¡Œä¸­")
        return
    
    logger.info("æ­£åœ¨å¯åŠ¨kubectl watch Podç›‘å¬çº¿ç¨‹...")
    status_check_running = True
    
    def kubectl_watch_worker():
        """kubectl watchå·¥ä½œçº¿ç¨‹"""
        thread_id = threading.current_thread().ident
        logger.info(f"kubectl watchå·¥ä½œçº¿ç¨‹å·²å¯åŠ¨ (çº¿ç¨‹ID: {thread_id})")
        
        retry_count = 0
        max_retries = 10  # æœ€å¤§é‡è¯•æ¬¡æ•°
        
        while status_check_running:
            try:
                # ä½¿ç”¨kubectl watchç›‘å¬Podå˜åŒ–
                logger.info("å¼€å§‹ä½¿ç”¨kubectl watchç›‘å¬gpu-health-expertå‘½åç©ºé—´çš„Podå˜åŒ–...")
                
                # æ„å»ºkubectl get --watchå‘½ä»¤
                cmd = [
                    'kubectl', 'get', 'pods', '-n', 'gpu-health-expert',
                    '-l', 'app=ghx-manual,job-type=manual',
                    '--no-headers', '--watch'
                ]
                
                logger.info(f"æ‰§è¡Œkubectl watchå‘½ä»¤: {' '.join(cmd)}")
                
                # å¯åŠ¨kubectl watchè¿›ç¨‹
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    bufsize=1,
                    universal_newlines=True
                )
                
                logger.info(f"kubectl watchè¿›ç¨‹å·²å¯åŠ¨ (PID: {process.pid})")
                
                # è¯»å–è¾“å‡º
                for line in iter(process.stdout.readline, ''):
                    if not status_check_running:
                        logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢kubectl watch")
                        break
                    
                    if line.strip():
                        try:
                            # è§£ækubectl watchè¾“å‡º
                            # æ ¼å¼: NAME READY STATUS RESTARTS AGE
                            # ä¾‹å¦‚: ghx-manual-job-xxx-yyy-zzz 0/1 Pending 0 5s
                            parts = line.strip().split()
                            if len(parts) >= 4:
                                pod_name = parts[0]
                                ready = parts[1]  # ä¾‹å¦‚: 0/1
                                status = parts[2]  # ä¾‹å¦‚: Pending, Running, Completed, Failed
                                restarts = parts[3]
                                
                                logger.info(f"ğŸ”„ kubectl watchæ£€æµ‹åˆ°Podå˜åŒ–: {pod_name} -> {status} (Ready: {ready}, Restarts: {restarts})")
                                
                                # ä»Podåç§°ä¸­æå–job_id
                                job_id = extract_job_id_from_pod_name(pod_name)
                                if not job_id:
                                    logger.debug(f"æ— æ³•ä»Podåç§°æå–job_id: {pod_name}")
                                    continue
                                
                                # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨è¿™ä¸ªJob
                                db_job = get_job_from_db(job_id)
                                if not db_job:
                                    logger.debug(f"æ•°æ®åº“ä¸­ä¸å­˜åœ¨Job: {job_id}")
                                    continue
                                
                                # è½¬æ¢çŠ¶æ€æ ¼å¼ - å°†kubectlçŠ¶æ€è½¬æ¢ä¸ºå°å†™åä¼ ç»™è½¬æ¢å‡½æ•°
                                pod_status = convert_kubectl_status_to_standard(status.lower(), ready)
                                
                                # æ£€æŸ¥çŠ¶æ€æ˜¯å¦æœ‰å˜åŒ– - ç»Ÿä¸€è½¬æ¢ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒ
                                if pod_status.lower() != db_job['status'].lower():
                                    logger.info(f"ğŸ”„ PodçŠ¶æ€å˜åŒ–: {job_id} {db_job['status']} -> {pod_status}")
                                    
                                    # æ›´æ–°æ•°æ®åº“çŠ¶æ€
                                    conn = get_db_connection()
                                    cursor = conn.cursor()
                                    cursor.execute('''
                                        UPDATE diagnostic_jobs 
                                        SET status = ?, updated_at = datetime('now', 'localtime')
                                        WHERE job_id = ?
                                    ''', (pod_status, job_id))
                                    conn.commit()
                                    conn.close()
                                    
                                    # é€šçŸ¥SSEå®¢æˆ·ç«¯
                                    notify_job_status_change(job_id, pod_status, db_job['node_name'] if 'node_name' in db_job.keys() else None)
                                    
                                    # å¦‚æœPodå®Œæˆï¼Œå¤„ç†ç»“æœæ”¶é›†
                                    if pod_status in ['Succeeded', 'Failed', 'Completed']:
                                        handle_job_completion(job_id)
                                else:
                                    logger.debug(f"Pod {pod_name} çŠ¶æ€æ— å˜åŒ–: {pod_status}")
                            
                        except Exception as e:
                            logger.warning(f"è§£ækubectl watchè¾“å‡ºå¤±è´¥: {e}, åŸå§‹è¡Œ: {line.strip()}")
                            continue
                
                # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å¼‚å¸¸é€€å‡º
                if process.poll() is not None:
                    stderr_output = process.stderr.read()
                    if stderr_output:
                        logger.warning(f"kubectl watchè¿›ç¨‹å¼‚å¸¸é€€å‡º: {stderr_output}")
                    else:
                        logger.info("kubectl watchè¿›ç¨‹æ­£å¸¸é€€å‡º")
                
                # æ¸…ç†è¿›ç¨‹
                try:
                    process.terminate()
                    process.wait(timeout=5)
                except subprocess.TimeoutExpired:
                    process.kill()
                    process.wait()
                except Exception as e:
                    logger.warning(f"æ¸…ç†kubectl watchè¿›ç¨‹å¤±è´¥: {e}")
                
                if not status_check_running:
                    break
                
                # ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡æ–°å¯åŠ¨
                logger.info("kubectl watchè¿æ¥æ–­å¼€ï¼Œ5ç§’åé‡æ–°å¯åŠ¨...")
                time.sleep(5)
                
            except Exception as e:
                retry_count += 1
                logger.error(f"kubectl watchå¼‚å¸¸ (é‡è¯• {retry_count}/{max_retries}): {e}")
                
                if retry_count >= max_retries:
                    logger.error(f"kubectl watché‡è¯•æ¬¡æ•°è¾¾åˆ°ä¸Šé™ï¼Œå›é€€åˆ°å®šæ—¶è½®è¯¢æ¨¡å¼...")
                    start_polling_status_check_thread()
                    break
                
                # æŒ‡æ•°é€€é¿ï¼š1ç§’, 2ç§’, 4ç§’, 8ç§’, 16ç§’...
                wait_time = min(2 ** retry_count, 30)  # æœ€å¤§ç­‰å¾…30ç§’
                logger.info(f"{wait_time}ç§’åé‡è¯•kubectl watch...")
                time.sleep(wait_time)
        
        logger.info(f"kubectl watchå·¥ä½œçº¿ç¨‹å·²é€€å‡º (çº¿ç¨‹ID: {thread_id})")
    
    # å¯åŠ¨çº¿ç¨‹
    status_check_thread = threading.Thread(target=kubectl_watch_worker, daemon=True)
    status_check_thread.start()
    time.sleep(0.1)
    
    if status_check_thread.is_alive():
        logger.info(f"kubectl watchçº¿ç¨‹å·²æˆåŠŸå¯åŠ¨ (çº¿ç¨‹ID: {status_check_thread.ident})")
    else:
        logger.error("kubectl watchçº¿ç¨‹å¯åŠ¨å¤±è´¥")
        status_check_running = False

def start_status_check_thread():
    """å¯åŠ¨çŠ¶æ€æ£€æŸ¥çº¿ç¨‹ï¼ˆå¤šæ–¹æ¡ˆå¤‡é€‰ï¼‰"""
    global status_check_thread, status_check_running
    
    if status_check_running:
        logger.info("çŠ¶æ€æ£€æŸ¥çº¿ç¨‹å·²åœ¨è¿è¡Œä¸­")
        return
    
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    logger.info(f"kubernetes_client: {kubernetes_client}")
    
    # ä¼˜å…ˆå°è¯•Kuberneteså®¢æˆ·ç«¯Watch
    if kubernetes_client:
        logger.info("æ­£åœ¨å¯åŠ¨Kubernetes WatchçŠ¶æ€æ£€æŸ¥çº¿ç¨‹...")
        start_kubernetes_watch_thread()
    else:
        logger.warning("Kuberneteså®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨kubectl watch...")
        start_kubectl_watch_thread()
        
        # å¦‚æœkubectl watchä¹Ÿå¤±è´¥ï¼Œå›é€€åˆ°è½®è¯¢æ¨¡å¼
        if not status_check_running:
            logger.warning("kubectl watchå¯åŠ¨å¤±è´¥ï¼Œå›é€€åˆ°å®šæ—¶è½®è¯¢æ¨¡å¼")
            start_polling_status_check_thread()

def start_polling_status_check_thread():
    """å¯åŠ¨å®šæ—¶è½®è¯¢çŠ¶æ€æ£€æŸ¥çº¿ç¨‹ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
    global status_check_thread, status_check_running
    
    logger.info("æ­£åœ¨å¯åŠ¨å®šæ—¶è½®è¯¢çŠ¶æ€æ£€æŸ¥çº¿ç¨‹...")
    status_check_running = True
    
    def polling_status_check_worker():
        """å®šæ—¶è½®è¯¢å·¥ä½œçº¿ç¨‹"""
        thread_id = threading.current_thread().ident
        logger.info(f"å®šæ—¶è½®è¯¢å·¥ä½œçº¿ç¨‹å·²å¯åŠ¨ (çº¿ç¨‹ID: {thread_id})")
        
        while status_check_running:
            try:
                # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡æ´»è·ƒJobçš„çŠ¶æ€
                time.sleep(10)
                
                # è·å–æ‰€æœ‰æ´»è·ƒJob
                conn = get_db_connection()
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT job_id, status FROM diagnostic_jobs 
                    WHERE status IN ('pending', 'running', 'Pending', 'Running', 'unknown', 'Unknown')
                    OR status LIKE '%pending%' OR status LIKE '%running%'
                    OR status LIKE '%waiting%' OR status LIKE '%creating%'
                    OR status LIKE '%ContainerCreating%'
                ''')
                
                active_jobs = cursor.fetchall()
                conn.close()
                
                if active_jobs:
                    logger.info(f"å®šæ—¶æ£€æŸ¥: æ‰¾åˆ° {len(active_jobs)} ä¸ªæ´»è·ƒJob")
                    
                    for job_id, current_status in active_jobs:
                        try:
                            # è·å–æœ€æ–°çš„KubernetesçŠ¶æ€
                            k8s_status = get_kubernetes_job_status(job_id)
                            if k8s_status:
                                new_status = k8s_status['pod_status']
                                
                                # æ ‡å‡†åŒ–çŠ¶æ€æ¯”è¾ƒ
                                current_normalized = current_status.lower().strip()
                                new_normalized = new_status.lower().strip()
                                
                                if current_normalized != new_normalized:
                                    logger.info(f"ğŸ”„ çŠ¶æ€å˜åŒ–: {job_id} {current_status} -> {new_status}")
                                    
                                    # é€šçŸ¥å‰ç«¯çŠ¶æ€å˜åŒ–
                                    notify_job_status_change(job_id, new_status)
                                    
                                    # æ›´æ–°æ•°æ®åº“ä¸­çš„çŠ¶æ€
                                    try:
                                        conn = get_db_connection()
                                        cursor = conn.cursor()
                                        cursor.execute('''
                                            UPDATE diagnostic_jobs 
                                            SET status = ?, updated_at = datetime('now', 'localtime')
                                            WHERE job_id = ?
                                        ''', (new_status, job_id))
                                        conn.commit()
                                        conn.close()
                                    except Exception as db_error:
                                        logger.warning(f"âŒ æ›´æ–°æ•°æ®åº“å¤±è´¥: {db_error}")
                                    
                                    # å¦‚æœJobå·²å®Œæˆï¼Œè‡ªåŠ¨è§¦å‘è¯Šæ–­ç»“æœå…¥åº“
                                    if new_status in ['Completed', 'Succeeded', 'Failed']:
                                        handle_job_completion(job_id)
                            else:
                                # å¦‚æœæ— æ³•è·å–KubernetesçŠ¶æ€ï¼Œè¯´æ˜Jobå¯èƒ½å·²è¢«åˆ é™¤
                                new_status = 'unknown'
                                logger.warning(f"æ— æ³•è·å–Job {job_id} çš„KubernetesçŠ¶æ€ï¼Œè®¾ä¸ºunknown")
                                
                                # é€šçŸ¥å‰ç«¯çŠ¶æ€å˜åŒ–
                                notify_job_status_change(job_id, new_status)
                                
                                # æ›´æ–°æ•°æ®åº“ä¸­çš„çŠ¶æ€
                                try:
                                    conn = get_db_connection()
                                    cursor = conn.cursor()
                                    cursor.execute('''
                                        UPDATE diagnostic_jobs 
                                        SET status = ?, updated_at = datetime('now', 'localtime')
                                        WHERE job_id = ?
                                    ''', (new_status, job_id))
                                    conn.commit()
                                    conn.close()
                                except Exception as db_error:
                                    logger.warning(f"âŒ æ›´æ–°æ•°æ®åº“å¤±è´¥: {db_error}")
                        except Exception as e:
                            logger.warning(f"æ£€æŸ¥Job {job_id} çŠ¶æ€å¤±è´¥: {e}")
                else:
                    logger.debug("æ²¡æœ‰æ´»è·ƒJobéœ€è¦æ£€æŸ¥")
                    # æ²¡æœ‰æ´»è·ƒJobæ—¶ï¼Œå‡å°‘æ£€æŸ¥é¢‘ç‡
                    time.sleep(30)  # ç­‰å¾…30ç§’å†æ£€æŸ¥
                    continue
                
                logger.info(f"å®šæ—¶çŠ¶æ€æ£€æŸ¥å®Œæˆï¼Œæ£€æŸ¥äº† {len(active_jobs)} ä¸ªæ´»è·ƒJob")
                
            except Exception as e:
                logger.error(f"å®šæ—¶çŠ¶æ€æ£€æŸ¥å¼‚å¸¸: {e}")
                time.sleep(60)  # å‡ºé”™æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´
        
        logger.info(f"å®šæ—¶è½®è¯¢å·¥ä½œçº¿ç¨‹å·²é€€å‡º (çº¿ç¨‹ID: {thread_id})")
    
    # å¯åŠ¨çº¿ç¨‹
    status_check_thread = threading.Thread(target=polling_status_check_worker, daemon=True)
    status_check_thread.start()
    time.sleep(0.1)
    
    if status_check_thread.is_alive():
        logger.info(f"å®šæ—¶è½®è¯¢çŠ¶æ€æ£€æŸ¥çº¿ç¨‹å·²æˆåŠŸå¯åŠ¨ (çº¿ç¨‹ID: {status_check_thread.ident})")
    else:
        logger.error("å®šæ—¶è½®è¯¢çŠ¶æ€æ£€æŸ¥çº¿ç¨‹å¯åŠ¨å¤±è´¥")
        status_check_running = False

def handle_job_completion(job_id: str):
    """å¤„ç†Jobå®Œæˆåçš„æ“ä½œï¼ˆåœ¨åå°çº¿ç¨‹ä¸­å®‰å…¨è°ƒç”¨ï¼‰"""
    try:
        logger.info(f"å¼€å§‹å¤„ç†Jobå®Œæˆ: {job_id}")
        
        # ä»PVCè¯»å–manualç±»å‹çš„æ–‡ä»¶
        pvc_path = '/shared/gpu-inspection-results/manual'
        if not os.path.exists(pvc_path):
            return {"success": False, "error": "PVCè·¯å¾„ä¸å­˜åœ¨"}
        
        # è·å–Jobä¿¡æ¯ï¼Œäº†è§£æ¶‰åŠå“ªäº›èŠ‚ç‚¹
        db_job = get_job_from_db(job_id)
        if db_job and 'node_name' in db_job.keys() and db_job['node_name']:
            try:
                # è§£æèŠ‚ç‚¹åç§°ï¼ˆJSONæ ¼å¼çš„å­—ç¬¦ä¸²ï¼‰
                node_names = json.loads(db_job['node_name'])
                logger.info(f"Jobæ¶‰åŠèŠ‚ç‚¹: {node_names}")
            except:
                node_names = [db_job['node_name']]
        else:
            node_names = []
        
        # ç­‰å¾…æ‰€æœ‰èŠ‚ç‚¹çš„æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼ˆæœ€å¤šç­‰å¾…60ç§’ï¼‰
        max_wait_time = 60  # ç§’
        wait_interval = 5   # ç§’
        total_wait_time = 0
        
        while total_wait_time < max_wait_time:
            # ç›´æ¥æŸ¥æ‰¾æ‰€æœ‰manualæ–‡ä»¶
            pattern = f"{pvc_path}/*.json"
            json_files = glob.glob(pattern)
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰èŠ‚ç‚¹çš„æ–‡ä»¶éƒ½å·²ç”Ÿæˆ
            if node_names:
                expected_files = []
                nodes_with_files = set()
                for node_name in node_names:
                    # æ£€æŸ¥æ˜¯å¦æœ‰è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆåŒ…æ‹¬å¸¦æ—¶é—´æˆ³çš„å’Œlatestæ–‡ä»¶ï¼‰
                    node_pattern = f"{pvc_path}/{node_name}_*.json"
                    node_files = glob.glob(node_pattern)
                    if node_files:
                        expected_files.extend(node_files)
                        nodes_with_files.add(node_name)
                
                logger.info(f"å½“å‰æ‰¾åˆ°æ–‡ä»¶: {len(json_files)} ä¸ªï¼ŒæœŸæœ›èŠ‚ç‚¹æ–‡ä»¶: {len(expected_files)} ä¸ª")
                logger.info(f"æœ‰æ–‡ä»¶çš„èŠ‚ç‚¹: {list(nodes_with_files)}ï¼Œæ€»èŠ‚ç‚¹: {node_names}")
                
                # å¦‚æœæ‰€æœ‰èŠ‚ç‚¹çš„æ–‡ä»¶éƒ½å·²ç”Ÿæˆï¼Œæˆ–è€…ç­‰å¾…æ—¶é—´å·²åˆ°ï¼Œåˆ™å¼€å§‹å¤„ç†
                if len(nodes_with_files) >= len(node_names) or total_wait_time >= max_wait_time - wait_interval:
                    json_files = expected_files
                    break
                else:
                    logger.info(f"ç­‰å¾…æ‰€æœ‰èŠ‚ç‚¹æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼Œå·²ç­‰å¾… {total_wait_time} ç§’...")
                    time.sleep(wait_interval)
                    total_wait_time += wait_interval
                    continue
            else:
                # å¦‚æœæ²¡æœ‰èŠ‚ç‚¹ä¿¡æ¯ï¼Œç›´æ¥å¤„ç†ç°æœ‰æ–‡ä»¶
                break
        
        if not json_files:
            return {"success": False, "error": "æœªæ‰¾åˆ°ä»»ä½•manualç»“æœæ–‡ä»¶"}
        
        logger.info(f"å¼€å§‹å¤„ç† {len(json_files)} ä¸ªæ–‡ä»¶")
        logger.info(f"æ–‡ä»¶åˆ—è¡¨: {json_files}")
        
        processed_count = 0
        for file_path in json_files:
            try:
                logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å¤„ç†è¿‡
                is_processed = collector.is_manual_file_processed(file_path)
                logger.info(f"æ£€æŸ¥æ–‡ä»¶å¤„ç†çŠ¶æ€: {file_path} -> å·²å¤„ç†: {is_processed}")
                if is_processed:
                    logger.info(f"æ–‡ä»¶å·²å¤„ç†è¿‡ï¼Œè·³è¿‡: {file_path}")
                    continue
                
                # è¯»å–JSONæ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # éªŒè¯æ•°æ®æ ¼å¼
                if collector.validate_manual_result_data(data):
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    collector.save_manual_diagnostic_result(data, file_path)
                    processed_count += 1
                    logger.info(f"æˆåŠŸå¤„ç†manualæ–‡ä»¶: {file_path}")
                else:
                    logger.warning(f"manualæ•°æ®æ ¼å¼æ— æ•ˆï¼Œè·³è¿‡æ–‡ä»¶: {file_path}")
                    
            except Exception as e:
                logger.error(f"å¤„ç†manualæ–‡ä»¶ {file_path} å¤±è´¥: {e}")
                continue
        
        # å¤„ç†å®Œæˆåé€šçŸ¥å‰ç«¯æ›´æ–°
        if processed_count > 0:
            notify_diagnostic_results_update()
        
        return {
            "success": True,
            "message": f"æˆåŠŸå¤„ç† {processed_count} ä¸ªmanualç»“æœæ–‡ä»¶",
            "processedCount": processed_count,
            "totalFiles": len(json_files)
        }
                
    except Exception as e:
        logger.error(f"å¤„ç†Jobå®Œæˆå¤±è´¥: {e}")
        return {
            "success": False,
            "error": f"å¤„ç†Jobå®Œæˆå¤±è´¥: {str(e)}"
        }

def stop_status_check_thread():
    """åœæ­¢å®šæ—¶çŠ¶æ€æ£€æŸ¥çº¿ç¨‹"""
    global status_check_running
    status_check_running = False
    if status_check_thread:
        status_check_thread.join(timeout=5)
    logger.info("å®šæ—¶çŠ¶æ€æ£€æŸ¥çº¿ç¨‹å·²åœæ­¢")

# æ•°æ®åº“é…ç½®
DB_PATH = '/shared/gpu_inspection.db'
SHARED_PVC_PATH = '/shared/gpu-inspection-results/cron'

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    try:
        # ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
        
        conn = sqlite3.connect(DB_PATH)
        conn.row_factory = sqlite3.Row  # ä½¿ç»“æœå¯ä»¥é€šè¿‡åˆ—åè®¿é—®
        return conn
    except Exception as e:
        logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        raise

def init_db():
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # åˆ›å»ºGPUæ£€æŸ¥ç»“æœè¡¨ï¼ˆæ¥è‡ªgpu_collector_serviceï¼‰
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS gpu_inspections (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                hostname TEXT NOT NULL,
                node_name TEXT,
                pod_name TEXT,
                gpu_type TEXT,
                bandwidth_test TEXT,
                p2p_bandwidth_latency_test TEXT,
                nccl_tests TEXT,
                dcgm_diag TEXT,
                ib_check TEXT,
                inspection_result TEXT,
                timestamp TEXT,
                execution_time TEXT,
                execution_log TEXT,
                benchmark TEXT,
                created_at TIMESTAMP DEFAULT (datetime(CURRENT_TIMESTAMP, 'localtime'))
            )
        ''')
        
        # åˆ›å»ºè¯Šæ–­ç»“æœè¡¨ï¼ˆæ¥è‡ªgpu_cliï¼‰
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS diagnostic_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                job_id TEXT NOT NULL,
                job_type TEXT NOT NULL DEFAULT 'manual',
                node_name TEXT NOT NULL,
                gpu_type TEXT,
                enabled_tests TEXT,
                dcgm_level INTEGER DEFAULT 1,
                inspection_result TEXT,
                performance_pass BOOLEAN,
                health_pass BOOLEAN,
                execution_time TEXT,
                execution_log TEXT,
                benchmark_data TEXT,
                test_results TEXT,
                file_path TEXT,
                expires_at TIMESTAMP,
                created_at TIMESTAMP DEFAULT (datetime(CURRENT_TIMESTAMP, 'localtime')),
                updated_at TIMESTAMP DEFAULT (datetime(CURRENT_TIMESTAMP, 'localtime')),
                UNIQUE(job_id, node_name)
            )
        ''')
        
        # åˆ›å»ºJobçŠ¶æ€è¡¨ï¼ˆæ¥è‡ªgpu_cliï¼‰
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS diagnostic_jobs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                job_id TEXT NOT NULL UNIQUE,
                job_name TEXT NOT NULL,
                job_type TEXT NOT NULL DEFAULT 'manual',
                selected_nodes TEXT,
                enabled_tests TEXT,
                dcgm_level INTEGER DEFAULT 1,
                status TEXT DEFAULT 'pending',
                created_at TIMESTAMP DEFAULT (datetime(CURRENT_TIMESTAMP, 'localtime')),
                updated_at TIMESTAMP DEFAULT (datetime(CURRENT_TIMESTAMP, 'localtime')),
                started_at TIMESTAMP,
                completed_at TIMESTAMP,
                error_message TEXT,
                expires_at TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_gpu_inspections_hostname 
            ON gpu_inspections(hostname)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_gpu_inspections_created_at 
            ON gpu_inspections(created_at)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_diagnostic_results_job_id 
            ON diagnostic_results(job_id)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_diagnostic_results_created_at 
            ON diagnostic_results(created_at)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_diagnostic_jobs_job_id 
            ON diagnostic_jobs(job_id)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_diagnostic_jobs_status 
            ON diagnostic_jobs(status)
        ''')
        
        conn.commit()
        conn.close()
        
        logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
        
    except Exception as e:
        logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        raise

def safe_json_loads(json_str: str, default_value: Any = None) -> Any:
    """
    å®‰å…¨åœ°è§£æJSONå­—ç¬¦ä¸²ï¼Œå¦‚æœè§£æå¤±è´¥åˆ™è¿”å›é»˜è®¤å€¼
    """
    if not json_str:
        return default_value
    
    try:
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        logger.warning(f"JSONè§£æå¤±è´¥ï¼Œå°è¯•æ¸…ç†åé‡æ–°è§£æ: {e}")
        try:
            cleaned_str = json_str.strip()
            if cleaned_str.startswith('\ufeff'):
                cleaned_str = cleaned_str[1:]
            return json.loads(cleaned_str)
        except json.JSONDecodeError as e2:
            logger.error(f"æ¸…ç†åJSONè§£æä»ç„¶å¤±è´¥: {e2}")
            return default_value

# ============================================================================
# GPUæ•°æ®æ”¶é›†å™¨ç±» (æ¥è‡ªgpu_collector_service)
# ============================================================================
class GPUDataCollector:
    def __init__(self):
        logger.info("=== GPUDataCollectoråˆå§‹åŒ–å¼€å§‹ ===")
        logger.info(f"SHARED_PVC_PATH: {SHARED_PVC_PATH}")
        self.init_database()
        logger.info("=== GPUDataCollectoråˆå§‹åŒ–å®Œæˆ ===")
        
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(DB_PATH)
            cursor = conn.cursor()
            
            # åˆ›å»ºæ£€æŸ¥ç»“æœè¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS gpu_inspections (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    hostname TEXT NOT NULL,
                    node_name TEXT,
                    pod_name TEXT,
                    gpu_type TEXT,
                    bandwidth_test TEXT,
                    p2p_bandwidth_latency_test TEXT,
                    nccl_tests TEXT,
                    dcgm_diag TEXT,
                    ib_check TEXT,
                    inspection_result TEXT,
                    timestamp TEXT,
                    execution_time TEXT,
                    execution_log TEXT,
                    benchmark TEXT,
                    performance_pass BOOLEAN,
                    raw_results TEXT,
                    file_path TEXT,
                    created_at TIMESTAMP DEFAULT (datetime(CURRENT_TIMESTAMP, 'localtime'))
                )
            ''')
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ æ–°å­—æ®µ
            cursor.execute("PRAGMA table_info(gpu_inspections)")
            columns = [column[1] for column in cursor.fetchall()]
            
            if 'execution_time' not in columns:
                cursor.execute('ALTER TABLE gpu_inspections ADD COLUMN execution_time TEXT')
                logger.info("å·²æ·»åŠ execution_timeå­—æ®µ")
            
            if 'execution_log' not in columns:
                cursor.execute('ALTER TABLE gpu_inspections ADD COLUMN execution_log TEXT')
                logger.info("å·²æ·»åŠ execution_logå­—æ®µ")
            
            # åˆ›å»ºç´¢å¼•
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_hostname ON gpu_inspections(hostname)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_timestamp ON gpu_inspections(timestamp)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_result ON gpu_inspections(inspection_result)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_file_path ON gpu_inspections(file_path)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_execution_time ON gpu_inspections(execution_time)')
            
            # åˆ›å»ºè¯Šæ–­ä»»åŠ¡è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS diagnostic_jobs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    job_id TEXT UNIQUE NOT NULL,
                    job_name TEXT NOT NULL,
                    job_type TEXT NOT NULL DEFAULT 'manual',
                    selected_nodes TEXT NOT NULL,
                    enabled_tests TEXT NOT NULL,
                    dcgm_level INTEGER NOT NULL DEFAULT 1,
                    status TEXT NOT NULL DEFAULT 'pending',
                    created_at TIMESTAMP DEFAULT (datetime(CURRENT_TIMESTAMP, 'localtime')),
                    started_at TIMESTAMP,
                    completed_at TIMESTAMP,
                    updated_at TIMESTAMP,
                    expires_at TIMESTAMP,
                    error_message TEXT
                )
            ''')
            
            # åˆ›å»ºè¯Šæ–­ç»“æœè¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS diagnostic_results (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    job_id TEXT NOT NULL,
                    job_type TEXT NOT NULL DEFAULT 'manual',
                    node_name TEXT NOT NULL,
                    gpu_type TEXT,
                    enabled_tests TEXT,
                    dcgm_level INTEGER,
                    inspection_result TEXT,
                    performance_pass BOOLEAN,
                    health_pass BOOLEAN,
                    execution_time TEXT,
                    execution_log TEXT,
                    benchmark_data TEXT,
                    test_results TEXT,
                    file_path TEXT,
                    created_at TIMESTAMP DEFAULT (datetime(CURRENT_TIMESTAMP, 'localtime')),
                    updated_at TIMESTAMP DEFAULT (datetime(CURRENT_TIMESTAMP, 'localtime')),
                    expires_at TIMESTAMP,
                    UNIQUE(job_id, node_name)
                )
            ''')
            
            # åˆ›å»ºè¯Šæ–­ç›¸å…³ç´¢å¼•
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_diagnostic_jobs_job_id ON diagnostic_jobs(job_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_diagnostic_jobs_status ON diagnostic_jobs(status)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_diagnostic_jobs_created_at ON diagnostic_jobs(created_at)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_diagnostic_results_job_id ON diagnostic_results(job_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_diagnostic_results_node_name ON diagnostic_results(node_name)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_diagnostic_results_created_at ON diagnostic_results(created_at)')
            
            conn.commit()
            conn.close()
            logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def cleanup_old_files(self, retention_days: int = 7):
        """æ¸…ç†è¶…è¿‡ retention_days çš„ JSON æ–‡ä»¶å’Œæ•°æ®åº“è®°å½•"""
        try:
            # æ¸…ç†æ–‡ä»¶
            if not os.path.exists(SHARED_PVC_PATH):
                logger.warning(f"å…±äº«PVCè·¯å¾„ä¸å­˜åœ¨: {SHARED_PVC_PATH}")
            else:
                now = time.time()
                pattern = f"{SHARED_PVC_PATH}/*.json"
                json_files = glob.glob(pattern)
                removed_count = 0
                for file_path in json_files:
                    try:
                        mtime = os.path.getmtime(file_path)
                        age_days = (now - mtime) / 86400
                        if age_days > retention_days:
                            os.remove(file_path)
                            removed_count += 1
                            logger.info(f"å·²åˆ é™¤è¿‡æœŸæ–‡ä»¶: {file_path}")
                    except Exception as e:
                        logger.error(f"åˆ é™¤æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
                logger.info(f"æ¸…ç†å®Œæˆï¼Œå…±åˆ é™¤ {removed_count} ä¸ªè¿‡æœŸæ–‡ä»¶")

            # æ¸…ç†æ•°æ®åº“
            cutoff_time = datetime.now() - timedelta(days=retention_days)
            try:
                conn = sqlite3.connect(DB_PATH)
                cursor = conn.cursor()
                cursor.execute('DELETE FROM gpu_inspections WHERE created_at < ?', (cutoff_time.isoformat(),))
                deleted_rows = cursor.rowcount
                conn.commit()
                conn.close()
                logger.info(f"æ¸…ç†æ•°æ®åº“å®Œæˆï¼Œå…±åˆ é™¤ {deleted_rows} æ¡è¿‡æœŸè®°å½•")
            except Exception as e:
                logger.error(f"æ¸…ç†æ•°æ®åº“è¿‡æœŸè®°å½•å¤±è´¥: {e}")
        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡æœŸæ–‡ä»¶å’Œæ•°æ®åº“å¤±è´¥: {e}")
    
    def collect_from_shared_pvc(self):
        """ä»å…±äº«PVCä¸­æ”¶é›†æ•°æ®"""
        logger.info("=== å¼€å§‹æ‰§è¡Œcollect_from_shared_pvcå‡½æ•° ===")
        try:
            if not os.path.exists(SHARED_PVC_PATH):
                logger.warning(f"å…±äº«PVCè·¯å¾„ä¸å­˜åœ¨: {SHARED_PVC_PATH}")
                return
            
            pattern = f"{SHARED_PVC_PATH}/*.json"
            json_files = glob.glob(pattern)
            logger.info(f"åœ¨PVCä¸­æ‰¾åˆ° {len(json_files)} ä¸ªç»“æœæ–‡ä»¶")
            
            for file_path in json_files:
                try:
                    if self.is_file_processed(file_path):
                        continue
                    
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    if self.validate_result_data(data):
                        self.save_inspection_result(data, file_path)
                        logger.info(f"æˆåŠŸå¤„ç†æ–‡ä»¶: {file_path}")
                    else:
                        logger.warning(f"æ•°æ®æ ¼å¼æ— æ•ˆï¼Œè·³è¿‡æ–‡ä»¶: {file_path}")
                        
                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
                    continue
                
        except Exception as e:
            logger.error(f"ä»å…±äº«PVCæ”¶é›†æ•°æ®å¤±è´¥: {e}")
        finally:
            logger.info("=== collect_from_shared_pvcå‡½æ•°æ‰§è¡Œå®Œæˆ ===")
    
    def collect_manual_results_from_pvc_internal(self):
        """å†…éƒ¨å‡½æ•°ï¼šä»PVCæ”¶é›†manualç±»å‹çš„è¯Šæ–­ç»“æœæ–‡ä»¶å¹¶å…¥åº“"""
        try:
            # ä»PVCè¯»å–manualç±»å‹çš„æ–‡ä»¶
            pvc_path = '/shared/gpu-inspection-results/manual'
            if not os.path.exists(pvc_path):
                return {"success": False, "error": "PVCè·¯å¾„ä¸å­˜åœ¨"}
            
            # ç›´æ¥æŸ¥æ‰¾manualç›®å½•ä¸‹çš„JSONæ–‡ä»¶
            pattern = f"{pvc_path}/*.json"
            json_files = glob.glob(pattern)
            
            logger.info(f"åœ¨manual PVCä¸­æ‰¾åˆ° {len(json_files)} ä¸ªç»“æœæ–‡ä»¶")
            if json_files:
                logger.info(f"æ‰¾åˆ°çš„manualæ–‡ä»¶åˆ—è¡¨: {json_files}")
            
            processed_count = 0
            for file_path in json_files:
                try:
                    logger.info(f"å¼€å§‹å¤„ç†manualæ–‡ä»¶: {file_path}")
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å¤„ç†è¿‡
                    if self.is_manual_file_processed(file_path):
                        logger.info(f"manualæ–‡ä»¶å·²å¤„ç†è¿‡ï¼Œè·³è¿‡: {file_path}")
                        continue
                    
                    # è¯»å–JSONæ–‡ä»¶
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # éªŒè¯æ•°æ®æ ¼å¼
                    if self.validate_manual_result_data(data):
                        # ä¿å­˜åˆ°diagnostic_resultsè¡¨
                        self.save_manual_diagnostic_result(data, file_path)
                        processed_count += 1
                        logger.info(f"æˆåŠŸå¤„ç†manualæ–‡ä»¶: {file_path}")
                    else:
                        logger.warning(f"manualæ•°æ®æ ¼å¼æ— æ•ˆï¼Œè·³è¿‡æ–‡ä»¶: {file_path}")
                        
                except Exception as e:
                    logger.error(f"å¤„ç†manualæ–‡ä»¶ {file_path} å¤±è´¥: {e}")
                    continue
            
            logger.info(f"manualæ–‡ä»¶å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {processed_count} ä¸ªæ–‡ä»¶")
            
            return {
                "success": True,
                "processedCount": processed_count,
                "totalFiles": len(json_files)
            }
            
        except Exception as e:
            logger.error(f"ä»PVCæ”¶é›†manualç»“æœå¤±è´¥: {e}")
            return {
                "success": False,
                "error": f"ä»PVCæ”¶é›†manualç»“æœå¤±è´¥: {str(e)}"
            }
    
    def is_file_processed(self, file_path: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å¤„ç†è¿‡"""
        try:
            conn = sqlite3.connect(DB_PATH)
            cursor = conn.cursor()
            cursor.execute('SELECT COUNT(*) FROM gpu_inspections WHERE file_path = ?', (file_path,))
            count = cursor.fetchone()[0]
            conn.close()
            return count > 0
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ–‡ä»¶å¤„ç†çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def is_manual_file_processed(self, file_path: str) -> bool:
        """æ£€æŸ¥manualæ–‡ä»¶æ˜¯å¦å·²ç»å¤„ç†è¿‡"""
        try:
            conn = sqlite3.connect(DB_PATH)
            cursor = conn.cursor()
            
            # ä»æ–‡ä»¶è·¯å¾„ä¸­æå–job_idï¼ˆæ–‡ä»¶åæ ¼å¼ï¼šhd03-gpu2-0062_latest.jsonï¼‰
            filename = os.path.basename(file_path)
            if '_latest.json' in filename:
                # å¯¹äºlatestæ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡
                # ä»æ–‡ä»¶åä¸­æå–node_name
                node_name = filename.replace('_latest.json', '')
                
                # æ£€æŸ¥è¯¥èŠ‚ç‚¹æ˜¯å¦æœ‰ä»»ä½•è¯Šæ–­ç»“æœè®°å½•
                cursor.execute('SELECT COUNT(*) FROM diagnostic_results WHERE node_name = ?', (node_name,))
                count = cursor.fetchone()[0]
                
                conn.close()
                return count > 0
            else:
                # å¯¹äºå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶ï¼Œä»æ–‡ä»¶åä¸­æå–job_id
                # æ–‡ä»¶åæ ¼å¼: {node_name}_{timestamp}.json
                # éœ€è¦ä»æ–‡ä»¶åä¸­æå–job_idï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
                # å‡è®¾æ–‡ä»¶åæ ¼å¼ä¸º: {job_id}-{node_name}.json
                job_id = filename.replace('.json', '').split('-')[0] + '-' + filename.replace('.json', '').split('-')[1]
                
                cursor.execute('SELECT COUNT(*) FROM diagnostic_results WHERE job_id = ?', (job_id,))
                count = cursor.fetchone()[0]
                
                conn.close()
                return count > 0
                        
        except Exception as e:
            logger.error(f"æ£€æŸ¥manualæ–‡ä»¶å¤„ç†çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def validate_manual_result_data(self, data: Dict[str, Any]) -> bool:
        """éªŒè¯manualç»“æœæ•°æ®æ ¼å¼"""
        required_fields = ['job_id', 'node_name', 'gpu_type', 'test_results']
        
        for field in required_fields:
            if field not in data:
                logger.warning(f"æ•°æ®æ ¼å¼ä¸åŒ¹é…ï¼ŒæœŸæœ›å­—æ®µ: {required_fields}")
                logger.warning(f"å®é™…å­—æ®µ: {list(data.keys())}")
                return False
        
        return True
    
    def save_manual_diagnostic_result(self, data: Dict[str, Any], file_path: str):
        """ä¿å­˜manualè¯Šæ–­ç»“æœåˆ°æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(DB_PATH)
            cursor = conn.cursor()
            
            # ä»æ•°æ®ä¸­æå–å­—æ®µï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
            job_id = data['job_id']
            node_name = data['node_name']
            gpu_type = data.get('gpu_type', 'Unknown')
            enabled_tests = data.get('enabled_tests', [])
            dcgm_level = data.get('dcgm_level', 1)
            
            # è·å–test_resultså­—æ®µï¼Œåº”è¯¥å·²ç»æ˜¯å­—å…¸æ ¼å¼
            test_results = data.get('test_results', {})
            performance_pass = data.get('performance_pass', False)
            
            # æ ¹æ®æµ‹è¯•ç»“æœè®¡ç®—æ•´ä½“çŠ¶æ€ï¼ˆä¸åˆå¹¶å‰é€»è¾‘ä¸€è‡´ï¼‰
            # æ£€æŸ¥DCGMå’ŒIBæµ‹è¯•ç»“æœ - æ³¨æ„dcgmå’Œibå­—æ®µç›´æ¥æ˜¯å­—ç¬¦ä¸²å€¼
            dcgm_result = test_results.get('dcgm', 'Skipped')
            ib_result = test_results.get('ib', 'Skipped')
            
            # è®¡ç®—å¥åº·çŠ¶æ€ï¼ˆDCGMå’ŒIBéƒ½é€šè¿‡æ‰ç®—å¥åº·ï¼‰
            health_pass = (dcgm_result == 'Pass' or dcgm_result == 'Skipped') and (ib_result == 'Pass' or ib_result == 'Skipped')
            
            # è®¡ç®—æ•´ä½“æ£€æŸ¥ç»“æœ
            if performance_pass and health_pass:
                inspection_result = 'Pass'
            else:
                inspection_result = 'No Pass'
            
            # å…¶ä»–å­—æ®µä½¿ç”¨é»˜è®¤å€¼
            execution_time = data.get('execution_time', 'N/A')
            execution_log = data.get('execution_log', '')
            benchmark_data = data.get('benchmark', {})
            
            # å…ˆæ£€æŸ¥è®°å½•æ˜¯å¦å­˜åœ¨
            cursor.execute('SELECT created_at FROM diagnostic_results WHERE job_id = ? AND node_name = ?', (job_id, node_name))
            existing_record = cursor.fetchone()
            
            if existing_record:
                # è®°å½•å­˜åœ¨ï¼Œæ›´æ–°æ—¶ä¿æŒåŸæœ‰created_at
                cursor.execute('''
                    UPDATE diagnostic_results 
                    SET job_type = ?, gpu_type = ?, enabled_tests = ?, dcgm_level = ?,
                        inspection_result = ?, performance_pass = ?, health_pass = ?,
                        execution_time = ?, execution_log = ?, benchmark_data = ?,
                        test_results = ?, expires_at = ?, updated_at = datetime('now', 'localtime')
                    WHERE job_id = ? AND node_name = ?
                ''', (
                    'manual', node_name, gpu_type, json.dumps(enabled_tests),
                    dcgm_level, inspection_result, performance_pass, health_pass,
                    execution_time, execution_log, json.dumps(benchmark_data),
                    json.dumps(test_results), datetime.now() + timedelta(days=7),
                    job_id, node_name
                ))
            else:
                # è®°å½•ä¸å­˜åœ¨ï¼Œæ’å…¥æ–°è®°å½•
                cursor.execute('''
                    INSERT INTO diagnostic_results 
                    (job_id, job_type, node_name, gpu_type, enabled_tests, dcgm_level, 
                     inspection_result, performance_pass, health_pass, execution_time, 
                     execution_log, benchmark_data, test_results, expires_at, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now', 'localtime'))
                ''', (
                    job_id, 'manual', node_name, gpu_type, json.dumps(enabled_tests),
                    dcgm_level, inspection_result, performance_pass, health_pass,
                    execution_time, execution_log, json.dumps(benchmark_data),
                    json.dumps(test_results), datetime.now() + timedelta(days=7)
                ))
            
            # åŒæ—¶æ›´æ–°JobçŠ¶æ€ä¸ºcompleted
            cursor.execute('''
                UPDATE diagnostic_jobs 
                SET status = 'completed', completed_at = datetime('now', 'localtime')
                WHERE job_id = ?
            ''', (data['job_id'],))
            
            conn.commit()
            conn.close()
            
            logger.info(f"æˆåŠŸä¿å­˜manualè¯Šæ–­ç»“æœ: {data['job_id']}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜manualè¯Šæ–­ç»“æœå¤±è´¥: {e}")
            raise e
    
    def validate_result_data(self, data: Dict[str, Any]) -> bool:
        """éªŒè¯ç»“æœæ•°æ®æ ¼å¼"""
        has_hostname = 'hostname' in data
        has_test_results = 'test_results' in data
        has_timestamp = 'created_at' in data
        
        if not (has_hostname and has_test_results and has_timestamp):
            logger.warning(f"æ•°æ®æ ¼å¼ä¸åŒ¹é…ï¼ŒæœŸæœ›å­—æ®µ: hostname + test_results + created_at")
            logger.warning(f"å®é™…å­—æ®µ: {list(data.keys())}")
            return False
        
        data['timestamp'] = data.get('created_at')
        
        test_results = data.get('test_results', {})
        performance_pass = data.get('performance_pass', False)
        
        dcgm_result = test_results.get('dcgm', 'Skipped')
        ib_result = test_results.get('ib', 'Skipped')
        
        health_pass = (dcgm_result == 'Pass' or dcgm_result == 'Skipped') and (ib_result == 'Pass' or ib_result == 'Skipped')
        
        if performance_pass and health_pass:
            data['inspectionResult'] = 'Pass'
        else:
            data['inspectionResult'] = 'No Pass'
        
        data['executionLog'] = data.get('execution_log', 'æš‚æ— æ‰§è¡Œæ—¥å¿—æ•°æ®')
        data['executionTime'] = data.get('execution_time', data.get('timestamp', datetime.now().isoformat()))
        
        logger.info(f"æ•°æ®éªŒè¯é€šè¿‡ï¼Œå­—æ®µ: {list(data.keys())}")
        return True
    
    def save_inspection_result(self, data: Dict[str, Any], file_path: str):
        """ä¿å­˜æ£€æŸ¥ç»“æœåˆ°æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(DB_PATH)
            cursor = conn.cursor()
            
            node_name = data.get('node_name', 'unknown')
            pod_name = data.get('pod_name', 'unknown')
            gpu_type = data.get('gpu_type', 'unknown')
            
            test_results = data.get('test_results', {})
            bandwidth_test = test_results.get('bandwidth', {}).get('value', 'N/A')
            p2p_test = test_results.get('p2p', {}).get('value', 'N/A')
            nccl_test = test_results.get('nccl', {}).get('value', 'N/A')
            dcgm_diag = test_results.get('dcgm', 'N/A')
            ib_check = test_results.get('ib', 'N/A')
            
            inspection_result = data.get('inspectionResult', 'Unknown')
            timestamp = data.get('timestamp', datetime.now().isoformat())
            execution_time = data.get('execution_time', 'N/A')
            execution_log = data.get('execution_log', 'æš‚æ— æ‰§è¡Œæ—¥å¿—æ•°æ®')
            benchmark = data.get('benchmark', {})
            performance_pass = data.get('performance_pass', False)
            raw_results = test_results
            
            cursor.execute('''
                INSERT INTO gpu_inspections (
                    hostname, node_name, pod_name, gpu_type,
                    bandwidth_test, p2p_bandwidth_latency_test, nccl_tests,
                    dcgm_diag, ib_check, inspection_result, timestamp,
                    execution_time, execution_log, benchmark, performance_pass,
                    raw_results, file_path, created_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                data.get('hostname'),
                node_name,
                pod_name,
                gpu_type,
                bandwidth_test,
                p2p_test,
                nccl_test,
                dcgm_diag,
                ib_check,
                inspection_result,
                timestamp,
                execution_time,
                execution_log,
                json.dumps(benchmark),
                performance_pass,
                json.dumps(raw_results),
                file_path,
                datetime.now().isoformat()
            ))
            
            conn.commit()
            conn.close()
            logger.info(f"ç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“: {data.get('hostname')} - {node_name}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœåˆ°æ•°æ®åº“å¤±è´¥: {e}")
            logger.error(f"æ•°æ®å†…å®¹: {data}")
            raise
    
    def get_latest_results(self, hours: int = 24) -> List[Dict[str, Any]]:
        """è·å–æœ€æ–°çš„æ£€æŸ¥ç»“æœ - æ¯ä¸ªèŠ‚ç‚¹åªè¿”å›æœ€æ–°è®°å½•"""
        try:
            conn = sqlite3.connect(DB_PATH)
            cursor = conn.cursor()
            
            cutoff_time = datetime.now() - timedelta(hours=hours)
            
            cursor.execute('''
                SELECT * FROM gpu_inspections 
                WHERE created_at > ? 
                AND id IN (
                    SELECT MAX(id) 
                    FROM gpu_inspections 
                    WHERE created_at > ? 
                    GROUP BY node_name
                )
                ORDER BY created_at DESC
            ''', (cutoff_time.isoformat(), cutoff_time.isoformat()))
            
            rows = cursor.fetchall()
            conn.close()
            
            results = []
            for row in rows:
                try:
                    result = {
                        'id': row[0],
                        'hostname': row[1],
                        'nodeName': row[2],
                        'podName': row[3],
                        'gpuType': row[4],
                        'nvbandwidthTest': row[5],
                        'p2pBandwidthLatencyTest': row[6],
                        'ncclTests': row[7],
                        'dcgmDiag': row[8],
                        'ibCheck': row[9],
                        'inspectionResult': row[10],
                        'timestamp': row[11],
                        'executionTime': row[12],
                        'executionLog': row[13] if row[13] else 'æš‚æ— æ‰§è¡Œæ—¥å¿—æ•°æ®',
                        'benchmark': safe_json_loads(row[14], {}),
                        'performancePass': bool(row[15]),
                        'rawResults': safe_json_loads(row[16], {}),
                        'file_path': row[17],
                        'createdAt': row[18]
                    }
                    results.append(result)
                except Exception as e:
                    logger.error(f"å¤„ç†ç»“æœè¡Œå¤±è´¥: {e}")
                    continue
            
            return results
            
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°ç»“æœå¤±è´¥: {e}")
            return []
    
    def get_all_historical_results(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰å†å²æ£€æŸ¥ç»“æœ - æ¯ä¸ªèŠ‚ç‚¹åªè¿”å›æœ€æ–°è®°å½•ï¼Œæ”¯æŒå†å²è¿½æº¯"""
        try:
            conn = sqlite3.connect(DB_PATH)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT * FROM gpu_inspections 
                WHERE id IN (
                    SELECT MAX(id) 
                    FROM gpu_inspections 
                    GROUP BY node_name
                )
                ORDER BY execution_time DESC, created_at DESC
            ''')
            
            rows = cursor.fetchall()
            conn.close()
            
            results = []
            for row in rows:
                try:
                    result = {
                        'id': row[0],
                        'hostname': row[1],
                        'nodeName': row[2],
                        'podName': row[3],
                        'gpuType': row[4],
                        'nvbandwidthTest': row[5],
                        'p2pBandwidthLatencyTest': row[6],
                        'ncclTests': row[7],
                        'dcgmDiag': row[8],
                        'ibCheck': row[9],
                        'inspectionResult': row[10],
                        'timestamp': row[11],
                        'executionTime': row[12],
                        'executionLog': row[13] if row[13] else 'æš‚æ— æ‰§è¡Œæ—¥å¿—æ•°æ®',
                        'benchmark': safe_json_loads(row[14], {}),
                        'performancePass': bool(row[15]),
                        'rawResults': safe_json_loads(row[16], {}),
                        'file_path': row[17],
                        'createdAt': row[18]
                    }
                    results.append(result)
                except Exception as e:
                    logger.error(f"å¤„ç†å†å²ç»“æœè¡Œå¤±è´¥: {e}")
                    continue
            
            return results
            
        except Exception as e:
            logger.error(f"è·å–å†å²ç»“æœå¤±è´¥: {e}")
            return []
    
    def get_summary(self, hours: int = 24) -> Dict[str, Any]:
        """è·å–æ£€æŸ¥æ‘˜è¦"""
        try:
            conn = sqlite3.connect(DB_PATH)
            cursor = conn.cursor()
            
            cutoff_time = datetime.now() - timedelta(hours=hours)
            
            cursor.execute('''
                SELECT COUNT(*) FROM gpu_inspections 
                WHERE created_at > ?
            ''', (cutoff_time.isoformat(),))
            total_nodes = cursor.fetchone()[0]
            
            cursor.execute('''
                SELECT COUNT(*) FROM gpu_inspections 
                WHERE created_at > ? AND inspection_result = 'Pass'
            ''', (cutoff_time.isoformat(),))
            passed_nodes = cursor.fetchone()[0]
            
            failed_nodes = total_nodes - passed_nodes
            
            cursor.execute('''
                SELECT MAX(created_at) FROM gpu_inspections 
                WHERE created_at > ?
            ''', (cutoff_time.isoformat(),))
            last_updated = cursor.fetchone()[0]
            
            conn.close()
            
            return {
                'totalNodes': total_nodes,
                'passedNodes': passed_nodes,
                'failedNodes': failed_nodes,
                'lastUpdated': last_updated
            }
            
        except Exception as e:
            logger.error(f"è·å–æ‘˜è¦å¤±è´¥: {e}")
            return {
                'totalNodes': 0,
                'passedNodes': 0,
                'failedNodes': 0,
                'lastUpdated': None
            }

# ============================================================================
# Kuberneteså®¢æˆ·ç«¯åˆå§‹åŒ– (æ¥è‡ªgpu_cli)
# ============================================================================
def init_kubernetes_client():
    """åˆå§‹åŒ–Kuberneteså®¢æˆ·ç«¯"""
    if not KUBERNETES_AVAILABLE:
        logger.warning("KubernetesåŒ…ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨kubectlå‘½ä»¤")
        return None
    
    try:
        config_loaded = False
        
        try:
            config.load_kube_config()
            logger.info("ä»é»˜è®¤kubeconfigæ–‡ä»¶åŠ è½½Kubernetesé…ç½®")
            config_loaded = True
        except Exception as e:
            logger.debug(f"é»˜è®¤kubeconfigåŠ è½½å¤±è´¥: {e}")
        
        if not config_loaded:
            kubeconfig_paths = [
                "/root/.kube/config",
                os.path.expanduser("~/.kube/config"),
                "/etc/kubernetes/admin.conf"
            ]
            
            for kubeconfig_path in kubeconfig_paths:
                if os.path.exists(kubeconfig_path):
                    try:
                        config.load_kube_config(config_file=kubeconfig_path)
                        logger.info(f"ä» {kubeconfig_path} åŠ è½½Kubernetesé…ç½®")
                        config_loaded = True
                        break
                    except Exception as e:
                        logger.debug(f"ä» {kubeconfig_path} åŠ è½½é…ç½®å¤±è´¥: {e}")
        
        if not config_loaded:
            try:
                config.load_incluster_config()
                logger.info("ä½¿ç”¨in-cluster Kubernetesé…ç½®")
                config_loaded = True
            except Exception as e:
                logger.debug(f"in-clusteré…ç½®åŠ è½½å¤±è´¥: {e}")
        
        if not config_loaded:
            logger.error("æ— æ³•åŠ è½½ä»»ä½•Kubernetesé…ç½®")
            return None
        
        v1 = client.CoreV1Api()
        batch_v1 = client.BatchV1Api()
        
        try:
            namespaces = v1.list_namespace(limit=1)
            logger.info("Kuberneteså®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œè¿æ¥æµ‹è¯•é€šè¿‡")
            return v1, batch_v1
        except Exception as e:
            logger.error(f"Kubernetesè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return None
            
    except Exception as e:
        logger.error(f"Kuberneteså®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        return None

# åˆå§‹åŒ–Kuberneteså®¢æˆ·ç«¯
kubernetes_client = init_kubernetes_client()

# ============================================================================
# å…¨å±€å˜é‡å’Œåˆå§‹åŒ–
# ============================================================================
collector = GPUDataCollector()

# æ·»åŠ ç¼“å­˜æœºåˆ¶ï¼ˆæ¥è‡ªåˆå¹¶å‰çš„gpu_cli.pyï¼‰
job_list_cache = {}
job_list_cache_timeout = 5  # 5ç§’ç¼“å­˜
diagnostic_results_cache = {}
diagnostic_results_cache_timeout = 5  # 5ç§’ç¼“å­˜

def get_diagnostic_results_rate_limit_decorator():
    """è¯Šæ–­ç»“æœæŸ¥è¯¢çš„å®½æ¾é™æµè£…é¥°å™¨ - 1åˆ†é’Ÿå†…å…è®¸30æ¬¡è¯·æ±‚ï¼ˆæ¥è‡ªåˆå¹¶å‰çš„gpu_cli.pyï¼‰"""
    def decorator(f):
        @wraps(f)
        def decorated_function(*args, **kwargs):
            client_ip = request.remote_addr
            current_time = time.time()
            
            # ä¸ºè¯Šæ–­ç»“æœæŸ¥è¯¢æä¾›æ›´å®½æ¾çš„é™åˆ¶ - 1åˆ†é’Ÿå†…å…è®¸30æ¬¡è¯·æ±‚
            cache_key = f"{client_ip}_diagnostic_results_rate_limit"
            
            # è·å–å½“å‰æ—¶é—´çª—å£å†…çš„è¯·æ±‚è®°å½•
            if cache_key in job_list_cache:
                request_times, _ = job_list_cache[cache_key]
                if isinstance(request_times, (list, tuple)):
                    # æ¸…ç†è¶…è¿‡1åˆ†é’Ÿçš„è®°å½•
                    request_times = [t for t in request_times if current_time - t < 60]
                    
                    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶ï¼ˆ30æ¬¡/åˆ†é’Ÿï¼‰
                    if len(request_times) >= 30:
                        logger.warning(f"è¯Šæ–­ç»“æœæŸ¥è¯¢é¢‘ç‡é™åˆ¶: {client_ip} (1åˆ†é’Ÿå†…å·²è¯·æ±‚{len(request_times)}æ¬¡)")
                        return jsonify({"error": "è¯Šæ–­ç»“æœæŸ¥è¯¢è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•"}), 429
                    
                    # æ·»åŠ å½“å‰è¯·æ±‚æ—¶é—´
                    request_times.append(current_time)
                else:
                    # å…¼å®¹æ—§æ ¼å¼ï¼Œè½¬æ¢ä¸ºæ–°æ ¼å¼
                    request_times = [current_time]
            else:
                request_times = [current_time]
            
            # æ›´æ–°ç¼“å­˜
            job_list_cache[cache_key] = (request_times, {})
            return f(*args, **kwargs)
        return decorated_function
    return decorator

def clear_client_cache(client_ip: str):
    """æ¸…ç†æŒ‡å®šå®¢æˆ·ç«¯çš„ç¼“å­˜ï¼ˆæ¥è‡ªåˆå¹¶å‰çš„gpu_cli.pyï¼‰"""
    try:
        # æ¸…ç†Jobåˆ—è¡¨ç¼“å­˜
        job_cache_key = f"{client_ip}_job_list"
        if job_cache_key in job_list_cache:
            del job_list_cache[job_cache_key]
            logger.info(f"å·²æ¸…ç†å®¢æˆ·ç«¯ {client_ip} çš„Jobåˆ—è¡¨ç¼“å­˜")
        
        # æ¸…ç†è¯Šæ–­ç»“æœç¼“å­˜
        diagnostic_cache_key = f"{client_ip}_diagnostic_results"
        if diagnostic_cache_key in diagnostic_results_cache:
            del diagnostic_results_cache[diagnostic_cache_key]
            logger.info(f"å·²æ¸…ç†å®¢æˆ·ç«¯ {client_ip} çš„è¯Šæ–­ç»“æœç¼“å­˜")
            
    except Exception as e:
        logger.warning(f"æ¸…ç†å®¢æˆ·ç«¯ç¼“å­˜å¤±è´¥: {e}")

def clear_all_cache():
    """æ¸…ç†æ‰€æœ‰ç¼“å­˜ï¼ˆæ¥è‡ªåˆå¹¶å‰çš„gpu_cli.pyï¼‰"""
    try:
        job_list_cache.clear()
        diagnostic_results_cache.clear()
        logger.info("å·²æ¸…ç†æ‰€æœ‰ç¼“å­˜")
    except Exception as e:
        logger.warning(f"æ¸…ç†æ‰€æœ‰ç¼“å­˜å¤±è´¥: {e}")

# åˆå§‹åŒ–é™æµ
rate_limit_type = init_rate_limit(app, use_redis=False, use_flask_limiter=False)
setup_rate_limit_error_handlers(app)

# ============================================================================
# APIè·¯ç”± - æ•°æ®æ”¶é›†ç›¸å…³ (æ¥è‡ªgpu_collector_service)
# ============================================================================
@app.route('/api/gpu-inspection', methods=['GET'])
def get_gpu_inspection():
    """è·å–GPUæ£€æŸ¥ç»“æœ"""
    try:
        include_history = request.args.get('include_history', 'false').lower() == 'true'
        hours = int(request.args.get('hours', 24))
        
        if include_history:
            results = collector.get_all_historical_results()
            logger.info(f"è¿”å›æ‰€æœ‰å†å²æ•°æ®ï¼Œå…± {len(results)} æ¡è®°å½•")
        else:
            results = collector.get_latest_results(hours)
            logger.info(f"è¿”å›æœ€è¿‘ {hours} å°æ—¶æ•°æ®ï¼Œå…± {len(results)} æ¡è®°å½•")
        
        summary = collector.get_summary(hours)
        
        response = {
            'summary': summary,
            'nodes': results,
            'includeHistory': include_history
        }
        
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"è·å–GPUæ£€æŸ¥ç»“æœå¤±è´¥: {e}")
        return jsonify({
            'error': 'Failed to get GPU inspection results',
            'message': str(e)
        }), 500

@app.route('/api/gpu-inspection/history', methods=['GET'])
def get_gpu_inspection_history():
    """è·å–æ‰€æœ‰å†å²GPUæ£€æŸ¥ç»“æœ"""
    try:
        results = collector.get_all_historical_results()
        summary = collector.get_summary(24)
        
        response = {
            'summary': summary,
            'nodes': results,
            'includeHistory': True,
            'totalHistoricalNodes': len(results)
        }
        
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"è·å–å†å²GPUæ£€æŸ¥ç»“æœå¤±è´¥: {e}")
        return jsonify({
            'error': 'Failed to get historical GPU inspection results',
            'message': str(e)
        }), 500

@app.route('/api/gpu-inspection/summary', methods=['GET'])
def get_gpu_inspection_summary():
    """è·å–GPUæ£€æŸ¥æ‘˜è¦"""
    try:
        hours = int(request.args.get('hours', 24))
        summary = collector.get_summary(hours)
        return jsonify(summary)
        
    except Exception as e:
        logger.error(f"è·å–GPUæ£€æŸ¥æ‘˜è¦å¤±è´¥: {e}")
        return jsonify({
            'error': 'Failed to get GPU inspection summary',
            'message': str(e)
        }), 500

@app.route('/api/gpu-inspection/collect', methods=['POST'])
def trigger_collection():
    """æ‰‹åŠ¨è§¦å‘æ•°æ®æ”¶é›†"""
    try:
        logger.info("æ‰‹åŠ¨è§¦å‘æ•°æ®æ”¶é›†...")
        collector.collect_from_shared_pvc()
        
        return jsonify({
            'status': 'success',
            'message': 'Data collection completed'
        })
        
    except Exception as e:
        logger.error(f"æ•°æ®æ”¶é›†å¤±è´¥: {e}")
        return jsonify({
            'error': 'Failed to collect data',
            'message': str(e)
        }), 500

# ============================================================================
# APIè·¯ç”± - èŠ‚ç‚¹çŠ¶æ€å’ŒJobç®¡ç† (æ¥è‡ªgpu_cli)
# ============================================================================
@app.route('/api/gpu-inspection/node-status', methods=['GET'])
def get_gpu_node_status():
    """è·å–GPUèŠ‚ç‚¹çŠ¶æ€"""
    try:
        if kubernetes_client:
            v1, batch_v1 = kubernetes_client
            nodes = v1.list_node()
            
            gpu_nodes = []
            for node in nodes.items:
                if has_gpu_resources(node):
                    node_info = parse_node_info(node)
                    gpu_nodes.append(node_info)
            
            return jsonify({
                'nodes': gpu_nodes,
                'total': len(gpu_nodes)
            })
        else:
            return get_gpu_node_status_kubectl()
            
    except Exception as e:
        logger.error(f"è·å–GPUèŠ‚ç‚¹çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            'error': 'Failed to get GPU node status',
            'message': str(e)
        }), 500

def has_gpu_resources(node):
    """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æœ‰GPUèµ„æº"""
    try:
        if node.status and node.status.allocatable:
            for key in node.status.allocatable:
                if 'nvidia.com/gpu' in key or 'gpu' in key.lower():
                    return True
        return False
    except Exception:
        return False

def get_gpu_requested_count(node_name):
    """è·å–èŠ‚ç‚¹å·²è¯·æ±‚çš„GPUæ•°é‡"""
    try:
        if kubernetes_client:
            v1, batch_v1 = kubernetes_client
            # è·å–èŠ‚ç‚¹ä¸Šçš„æ‰€æœ‰Pod
            pods = v1.list_pod_for_all_namespaces(field_selector=f"spec.nodeName={node_name}")
            
            total_requested = 0
            for pod in pods.items:
                if pod.status.phase in ['Running', 'Pending']:
                    for container in pod.spec.containers:
                        if container.resources and container.resources.requests:
                            for key, value in container.resources.requests.items():
                                if 'nvidia.com/gpu' in key:
                                    try:
                                        total_requested += int(value)
                                    except ValueError:
                                        pass
            return total_requested
        else:
            # ä½¿ç”¨kubectlè·å–
            return get_gpu_requested_count_kubectl(node_name)
    except Exception as e:
        logger.error(f"è·å–èŠ‚ç‚¹ {node_name} å·²è¯·æ±‚GPUæ•°é‡å¤±è´¥: {e}")
        return 0

def get_gpu_requested_count_kubectl(node_name):
    """ä½¿ç”¨kubectlè·å–èŠ‚ç‚¹å·²è¯·æ±‚çš„GPUæ•°é‡"""
    try:
        result = subprocess.run([
            'kubectl', 'get', 'pods', '--all-namespaces', 
            '--field-selector', f'spec.nodeName={node_name}',
            '-o', 'json'
        ], capture_output=True, text=True, timeout=30)
        
        if result.returncode != 0:
            logger.warning(f"kubectlè·å–Podä¿¡æ¯å¤±è´¥: {result.stderr}")
            return 0
        
        pods_data = json.loads(result.stdout)
        total_requested = 0
        
        for pod in pods_data.get('items', []):
            pod_status = pod.get('status', {}).get('phase', '')
            if pod_status in ['Running', 'Pending']:
                containers = pod.get('spec', {}).get('containers', [])
                for container in containers:
                    requests = container.get('resources', {}).get('requests', {})
                    for key, value in requests.items():
                        if 'nvidia.com/gpu' in key:
                            try:
                                total_requested += int(value)
                            except ValueError:
                                pass
        
        return total_requested
    except Exception as e:
        logger.error(f"kubectlè·å–èŠ‚ç‚¹ {node_name} å·²è¯·æ±‚GPUæ•°é‡å¤±è´¥: {e}")
        return 0

def get_gpu_info_from_kubectl_resource_view(node_name=None):
    """ä½¿ç”¨kubectl-resource-viewå·¥å…·ç»Ÿä¸€è·å–GPUä¿¡æ¯"""
    try:
        # å¦‚æœæŒ‡å®šäº†èŠ‚ç‚¹åï¼ŒåªæŸ¥è¯¢è¯¥èŠ‚ç‚¹
        if node_name:
            cmd = ['/usr/local/bin/kubectl-resource-view', 'node', node_name, '-t', 'gpu', '--no-format']
        else:
            cmd = ['/usr/local/bin/kubectl-resource-view', 'node', '-t', 'gpu', '--no-format']
        
        # ä½¿ç”¨kubectl-resource-viewè·å–GPUä¿¡æ¯ - å¢åŠ è¶…æ—¶æ—¶é—´åˆ°2åˆ†é’Ÿ
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
        
        if result.returncode == 0:
            gpu_info_map = {}
            lines = result.stdout.strip().split('\n')
            
            # è·³è¿‡è¡¨å¤´è¡Œ
            data_lines = lines[1:] if len(lines) > 1 else []
            
            for line in data_lines:
                if line.strip():
                    # è§£ææ ¼å¼: hd03-gpu2-0011          0               0%                      0               0%                      nvidia.com/gpu-h200
                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æˆ–å›ºå®šä½ç½®è§£æ
                    import re
                    
                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…èŠ‚ç‚¹åå’ŒGPUä¿¡æ¯
                    # èŠ‚ç‚¹åé€šå¸¸åœ¨è¡Œé¦–ï¼ŒGPUç±»å‹åœ¨GPU MODELåˆ—
                    match = re.match(r'^(\S+)\s+(\d+)\s+\d+%\s+(\d+)\s+\d+%\s+(nvidia\.com/gpu-\S+|amd\.com/gpu-\S+|N/A)', line)
                    
                    if match:
                        current_node_name = match.group(1)
                        gpu_requested = int(match.group(2))
                        gpu_limit = int(match.group(3))
                        gpu_type = match.group(4)
                        
                        # è¿‡æ»¤æ‰è¡¨å¤´è¡Œå’Œæ²¡æœ‰GPUçš„èŠ‚ç‚¹
                        if current_node_name.upper() in ['NODE', 'NVIDIA/GPU REQ', 'NVIDIA/GPU REQ(%)', 'NVIDIA/GPU LIM', 'NVIDIA/GPU LIM(%)', 'GPU MODEL']:
                            continue
                        
                        # è·³è¿‡æ²¡æœ‰GPUçš„èŠ‚ç‚¹
                        if gpu_type == 'N/A':
                            continue
                        
                        gpu_info_map[current_node_name] = {
                            'gpu_type': gpu_type,
                            'gpu_requested': gpu_requested,
                            'gpu_limit': gpu_limit,
                            'gpu_count': gpu_limit  # GPUæ€»æ•°ç­‰äºé™åˆ¶æ•°
                        }
            
            # å¦‚æœæŒ‡å®šäº†èŠ‚ç‚¹åï¼Œåªè¿”å›è¯¥èŠ‚ç‚¹çš„ä¿¡æ¯
            if node_name and node_name in gpu_info_map:
                return gpu_info_map[node_name]
            elif node_name:
                # å¦‚æœæŒ‡å®šäº†èŠ‚ç‚¹åä½†æ²¡æ‰¾åˆ°ï¼Œè¿”å›é»˜è®¤å€¼
                return {
                    'gpu_type': 'Unknown',
                    'gpu_requested': 0,
                    'gpu_limit': 0,
                    'gpu_count': 0
                }
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šèŠ‚ç‚¹åï¼Œè¿”å›æ‰€æœ‰èŠ‚ç‚¹çš„ä¿¡æ¯
                return gpu_info_map
        
        # å¦‚æœkubectl-resource-viewå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
        return {
            'gpu_type': 'Unknown',
            'gpu_requested': 0,
            'gpu_limit': 0,
            'gpu_count': 0
        }
        
    except Exception as e:
        logger.warning(f"ä½¿ç”¨kubectl-resource-viewè·å–GPUä¿¡æ¯å¤±è´¥: {e}")
        return {
            'gpu_type': 'Unknown',
            'gpu_requested': 0,
            'gpu_limit': 0,
            'gpu_count': 0
        }

def parse_node_info(node):
    """è§£æèŠ‚ç‚¹ä¿¡æ¯ - ç»Ÿä¸€ä½¿ç”¨kubectl-resource-viewå·¥å…·è·å–GPUä¿¡æ¯"""
    try:
        # è·å–èŠ‚ç‚¹åŸºæœ¬ä¿¡æ¯
        node_name = node.metadata.name
        is_ready = any(condition.type == 'Ready' and condition.status == 'True' 
                      for condition in node.status.conditions) if node.status and node.status.conditions else False
        
        # ä½¿ç”¨kubectl-resource-viewå·¥å…·ç»Ÿä¸€è·å–GPUä¿¡æ¯
        gpu_info = get_gpu_info_from_kubectl_resource_view(node_name)
        
        # ç¡®å®šèŠ‚ç‚¹çŠ¶æ€
        node_status = 'idle' if gpu_info['gpu_requested'] == 0 else 'busy'
        
        # ç¡®ä¿GPUç±»å‹æ ¼å¼æ­£ç¡®ï¼ˆå®Œæ•´çš„èµ„æºåç§°ï¼‰
        gpu_type = gpu_info['gpu_type']
        if gpu_type and not gpu_type.startswith('nvidia.com/') and not gpu_type.startswith('amd.com/') and gpu_type != 'Unknown':
            # å¦‚æœGPUç±»å‹ä¸æ˜¯å®Œæ•´æ ¼å¼ï¼Œè¡¥å……å‰ç¼€
            if gpu_type.startswith('gpu-'):
                gpu_type = f'nvidia.com/{gpu_type}'
            else:
                gpu_type = f'nvidia.com/gpu-{gpu_type}'
        
        node_info = {
            'nodeName': node_name,
            'gpuType': gpu_type,
            'gpuRequested': gpu_info['gpu_requested'],
            'nodeStatus': node_status,
            'gpuCount': gpu_info['gpu_count'],
            'status': 'Ready' if is_ready else 'NotReady',
            'timestamp': datetime.now().isoformat()
        }
        
        return node_info
    except Exception as e:
        logger.error(f"è§£æèŠ‚ç‚¹ä¿¡æ¯å¤±è´¥: {e}")
        return {
            'nodeName': node.metadata.name if node.metadata else 'Unknown',
            'gpuType': 'Unknown',
            'gpuRequested': 0,
            'nodeStatus': 'unknown',
            'gpuCount': 0,
            'status': 'Unknown',
            'timestamp': datetime.now().isoformat()
        }

def get_gpu_node_status_kubectl():
    """ä½¿ç”¨kubectlè·å–GPUèŠ‚ç‚¹çŠ¶æ€"""
    try:
        result = subprocess.run([
            'kubectl', 'get', 'nodes', '-o', 'json'
        ], capture_output=True, text=True, timeout=30)
        
        if result.returncode != 0:
            raise Exception(f"kubectlå‘½ä»¤æ‰§è¡Œå¤±è´¥: {result.stderr}")
        
        nodes_data = json.loads(result.stdout)
        gpu_nodes = []
        
        for node in nodes_data.get('items', []):
            if has_gpu_resources_kubectl(node):
                node_info = parse_node_info_kubectl(node)
                gpu_nodes.append(node_info)
        
        return jsonify({
            'nodes': gpu_nodes,
            'total': len(gpu_nodes)
        })
        
    except Exception as e:
        logger.error(f"ä½¿ç”¨kubectlè·å–GPUèŠ‚ç‚¹çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            'error': 'Failed to get GPU node status via kubectl',
            'message': str(e)
        }), 500

def has_gpu_resources_kubectl(node):
    """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æœ‰GPUèµ„æº (kubectlç‰ˆæœ¬)"""
    try:
        allocatable = node.get('status', {}).get('allocatable', {})
        for key in allocatable:
            if 'nvidia.com/gpu' in key or 'gpu' in key.lower():
                return True
        return False
    except Exception:
        return False

def parse_node_info_kubectl(node):
    """è§£æèŠ‚ç‚¹ä¿¡æ¯ (kubectlç‰ˆæœ¬) - ç»Ÿä¸€ä½¿ç”¨kubectl-resource-viewå·¥å…·è·å–GPUä¿¡æ¯"""
    try:
        # è·å–èŠ‚ç‚¹åŸºæœ¬ä¿¡æ¯
        node_name = node.get('metadata', {}).get('name', 'Unknown')
        
        # æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
        conditions = node.get('status', {}).get('conditions', [])
        is_ready = False
        for condition in conditions:
            if condition.get('type') == 'Ready' and condition.get('status') == 'True':
                is_ready = True
                break
        
        # ä½¿ç”¨kubectl-resource-viewå·¥å…·ç»Ÿä¸€è·å–GPUä¿¡æ¯
        gpu_info = get_gpu_info_from_kubectl_resource_view(node_name)
        
        # ç¡®å®šèŠ‚ç‚¹çŠ¶æ€
        node_status = 'idle' if gpu_info['gpu_requested'] == 0 else 'busy'
        
        # ç¡®ä¿GPUç±»å‹æ ¼å¼æ­£ç¡®ï¼ˆå®Œæ•´çš„èµ„æºåç§°ï¼‰
        gpu_type = gpu_info['gpu_type']
        if gpu_type and not gpu_type.startswith('nvidia.com/') and not gpu_type.startswith('amd.com/') and gpu_type != 'Unknown':
            # å¦‚æœGPUç±»å‹ä¸æ˜¯å®Œæ•´æ ¼å¼ï¼Œè¡¥å……å‰ç¼€
            if gpu_type.startswith('gpu-'):
                gpu_type = f'nvidia.com/{gpu_type}'
            else:
                gpu_type = f'nvidia.com/gpu-{gpu_type}'
        
        node_info = {
            'nodeName': node_name,
            'gpuType': gpu_type,
            'gpuRequested': gpu_info['gpu_requested'],
            'nodeStatus': node_status,
            'gpuCount': gpu_info['gpu_count'],
            'status': 'Ready' if is_ready else 'NotReady',
            'timestamp': datetime.now().isoformat()
        }
        
        return node_info
    except Exception as e:
        logger.error(f"è§£æèŠ‚ç‚¹ä¿¡æ¯å¤±è´¥ (kubectl): {e}")
        return {
            'nodeName': 'Unknown',
            'gpuType': 'Unknown',
            'gpuRequested': 0,
            'nodeStatus': 'unknown',
            'gpuCount': 0,
            'status': 'Unknown',
            'timestamp': datetime.now().isoformat()
        }

def delete_job_internal(job_id):
    """å†…éƒ¨åˆ é™¤Jobå‡½æ•°ï¼Œä¸è¿”å›HTTPå“åº”"""
    try:
        logger.info(f"å¼€å§‹åˆ é™¤Job: {job_id}")
        
        # é¦–å…ˆä»æ•°æ®åº“ä¸­åˆ é™¤Jobè®°å½•
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            
            # 1. æŸ¥è¯¢è¯Šæ–­ç»“æœä¿¡æ¯ï¼Œç”¨äºåˆ é™¤PVCæ–‡ä»¶
            cursor.execute('''
                SELECT node_name FROM diagnostic_results 
                WHERE job_id = ?
            ''', (job_id,))
            
            result_info = cursor.fetchone()
            node_name = result_info[0] if result_info else None
            
            logger.info(f"æŸ¥è¯¢Job {job_id} çš„èŠ‚ç‚¹ä¿¡æ¯: {node_name}")
            
            # 2. å¦‚æœä»è¯Šæ–­ç»“æœä¸­æ— æ³•è·å–èŠ‚ç‚¹ä¿¡æ¯ï¼Œå°è¯•ä»Jobè®°å½•ä¸­è·å–
            if not node_name:
                cursor.execute('''
                    SELECT selected_nodes FROM diagnostic_jobs 
                    WHERE job_id = ?
                ''', (job_id,))
                
                job_info = cursor.fetchone()
                if job_info and job_info[0]:
                    try:
                        selected_nodes = json.loads(job_info[0])
                        if selected_nodes and len(selected_nodes) > 0:
                            node_name = selected_nodes[0]  # å–ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
                            logger.info(f"ä»Jobè®°å½•ä¸­è·å–åˆ°èŠ‚ç‚¹ä¿¡æ¯: {node_name}")
                    except Exception as json_error:
                        logger.warning(f"è§£æJobèŠ‚ç‚¹ä¿¡æ¯å¤±è´¥: {json_error}")
                
                if not node_name:
                    logger.warning(f"æ— æ³•è·å–Job {job_id} çš„èŠ‚ç‚¹ä¿¡æ¯ï¼Œå°†å°è¯•é€šè¿‡æ–‡ä»¶ååŒ¹é…åˆ é™¤PVCæ–‡ä»¶")
            
            # 3. åˆ é™¤ç›¸å…³çš„è¯Šæ–­ç»“æœ
            cursor.execute("DELETE FROM diagnostic_results WHERE job_id = ?", (job_id,))
            results_deleted = cursor.rowcount > 0
            
            # 4. åˆ é™¤Jobè®°å½•
            cursor.execute("DELETE FROM diagnostic_jobs WHERE job_id = ?", (job_id,))
            job_deleted = cursor.rowcount > 0
            
            conn.commit()
            conn.close()
            
            logger.info(f"æ•°æ®åº“æ¸…ç†å®Œæˆ: Jobè®°å½•={job_deleted}, è¯Šæ–­ç»“æœ={results_deleted}")
            
            # 5. åˆ é™¤ç›¸å…³çš„PVCæ–‡ä»¶
            try:
                if node_name:
                    logger.info(f"å¼€å§‹åˆ é™¤Job {job_id} ç›¸å…³çš„PVCæ–‡ä»¶ï¼ŒèŠ‚ç‚¹: {node_name}")
                    delete_pvc_files_for_job(job_id, node_name)
                    logger.info(f"æˆåŠŸåˆ é™¤Job {job_id} ç›¸å…³çš„PVCæ–‡ä»¶")
                else:
                    # å³ä½¿æ²¡æœ‰èŠ‚ç‚¹ä¿¡æ¯ï¼Œä¹Ÿå°è¯•é€šè¿‡job_idåˆ é™¤ç›¸å…³æ–‡ä»¶
                    logger.info(f"å°è¯•é€šè¿‡job_idåˆ é™¤Job {job_id} ç›¸å…³çš„PVCæ–‡ä»¶")
                    delete_pvc_files_for_job(job_id, "unknown")
                    logger.info(f"é€šè¿‡job_idåˆ é™¤Job {job_id} ç›¸å…³çš„PVCæ–‡ä»¶å®Œæˆ")
            except Exception as pvc_error:
                logger.warning(f"åˆ é™¤Job {job_id} ç›¸å…³çš„PVCæ–‡ä»¶å¤±è´¥: {pvc_error}")
            
        except Exception as e:
            logger.error(f"æ•°æ®åº“æ¸…ç†å¤±è´¥: {e}")
            # å³ä½¿æ•°æ®åº“æ¸…ç†å¤±è´¥ï¼Œä¹Ÿç»§ç»­å°è¯•åˆ é™¤Kubernetes Job
        
        # åˆ é™¤Kubernetes Job
        try:
            # æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³çš„Jobåç§°
            result = subprocess.run([
                'kubectl', 'get', 'jobs', '-n', 'gpu-health-expert', 
                '--field-selector', f'metadata.labels.job-id={job_id}',
                '-o', 'jsonpath={.items[*].metadata.name}'
            ], capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0 and result.stdout.strip():
                job_names = result.stdout.strip().split()
                logger.info(f"æ‰¾åˆ°ç›¸å…³Kubernetes Jobs: {job_names}")
                
                for job_name in job_names:
                    # åˆ é™¤Job
                    delete_result = subprocess.run([
                        'kubectl', 'delete', 'job', job_name, '-n', 'gpu-health-expert'
                    ], capture_output=True, text=True, timeout=180)
                    
                    if delete_result.returncode == 0:
                        logger.info(f"æˆåŠŸåˆ é™¤Kubernetes Job: {job_name}")
                    else:
                        logger.warning(f"åˆ é™¤Kubernetes Jobå¤±è´¥: {job_name}, é”™è¯¯: {delete_result.stderr}")
                        
                    # åˆ é™¤ç›¸å…³çš„Pod
                    pod_result = subprocess.run([
                        'kubectl', 'delete', 'pods', '-n', 'gpu-health-expert',
                        '--field-selector', f'job-name={job_name}'
                    ], capture_output=True, text=True, timeout=60)
                    
                    if pod_result.returncode == 0:
                        logger.info(f"æˆåŠŸåˆ é™¤ç›¸å…³Pod: {job_name}")
                    else:
                        logger.warning(f"åˆ é™¤ç›¸å…³Podå¤±è´¥: {job_name}, é”™è¯¯: {pod_result.stderr}")
            else:
                logger.info(f"æœªæ‰¾åˆ°ç›¸å…³çš„Kubernetes Job: {job_id}")
                
        except Exception as e:
            logger.warning(f"åˆ é™¤Kubernetes Jobå¤±è´¥: {e}")
        
        return {"success": True, "message": f"Job {job_id} åˆ é™¤æˆåŠŸ"}
        
    except Exception as e:
        logger.error(f"åˆ é™¤Jobå¤±è´¥: {e}")
        return {"success": False, "error": str(e)}

def delete_job_with_kubernetes_client(job_id):
    """ä½¿ç”¨Kuberneteså®¢æˆ·ç«¯åˆ é™¤Job"""
    try:
        if not KUBERNETES_AVAILABLE or not kubernetes_client:
            logger.warning("Kuberneteså®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œå›é€€åˆ°kubectlå‘½ä»¤")
            return delete_job_with_kubectl(job_id)
        
        v1, batch_v1 = kubernetes_client
        
        # æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³çš„Job
        jobs = batch_v1.list_namespaced_job(
            namespace='gpu-health-expert',
            label_selector=f'job-id={job_id}'
        )
        
        deleted_jobs = []
        for job in jobs.items:
            try:
                # åˆ é™¤Job
                batch_v1.delete_namespaced_job(
                    name=job.metadata.name,
                    namespace='gpu-health-expert',
                    grace_period_seconds=0,
                    propagation_policy='Background'
                )
                deleted_jobs.append(job.metadata.name)
                logger.info(f"æˆåŠŸåˆ é™¤Job: {job.metadata.name}")
                
                # åˆ é™¤ç›¸å…³çš„Pod
                pods = v1.list_namespaced_pod(
                    namespace='gpu-health-expert',
                    label_selector=f'job-name={job.metadata.name}'
                )
                
                for pod in pods.items:
                    try:
                        v1.delete_namespaced_pod(
                            name=pod.metadata.name,
                            namespace='gpu-health-expert',
                            grace_period_seconds=0
                        )
                        logger.info(f"æˆåŠŸåˆ é™¤Pod: {pod.metadata.name}")
                    except Exception as pod_error:
                        logger.warning(f"åˆ é™¤Pod {pod.metadata.name} å¤±è´¥: {pod_error}")
                        
            except Exception as job_error:
                logger.warning(f"åˆ é™¤Job {job.metadata.name} å¤±è´¥: {job_error}")
        
        if deleted_jobs:
            logger.info(f"æˆåŠŸåˆ é™¤ {len(deleted_jobs)} ä¸ªJob: {deleted_jobs}")
            return True, f"æˆåŠŸåˆ é™¤ {len(deleted_jobs)} ä¸ªJob"
        else:
            logger.warning(f"æœªæ‰¾åˆ°Job IDä¸º {job_id} çš„Job")
            return False, f"æœªæ‰¾åˆ°Job IDä¸º {job_id} çš„Job"
            
    except Exception as e:
        logger.error(f"ä½¿ç”¨Kuberneteså®¢æˆ·ç«¯åˆ é™¤Jobå¤±è´¥: {e}")
        return False, str(e)

def delete_job_with_kubectl(job_id):
    """ä½¿ç”¨kubectlå‘½ä»¤åˆ é™¤Jobï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
    try:
        # æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³çš„Job
        result = subprocess.run([
            'kubectl', 'get', 'jobs', '-n', 'gpu-health-expert', 
            '-l', f'job-id={job_id}', '--no-headers', '-o', 'custom-columns=:metadata.name'
        ], capture_output=True, text=True, timeout=60)
        
        if result.returncode != 0:
            logger.error(f"æŸ¥æ‰¾Jobå¤±è´¥: {result.stderr}")
            return False, result.stderr
        
        job_names = [name.strip() for name in result.stdout.split('\n') if name.strip()]
        
        if not job_names:
            logger.warning(f"æœªæ‰¾åˆ°Job IDä¸º {job_id} çš„Job")
            return False, f"æœªæ‰¾åˆ°Job IDä¸º {job_id} çš„Job"
        
        deleted_jobs = []
        for job_name in job_names:
            try:
                # åˆ é™¤Job
                delete_result = subprocess.run([
                    'kubectl', 'delete', 'job', job_name, '-n', 'gpu-health-expert'
                ], capture_output=True, text=True, timeout=60)
                
                if delete_result.returncode == 0:
                    deleted_jobs.append(job_name)
                    logger.info(f"æˆåŠŸåˆ é™¤Job: {job_name}")
                    
                    # åˆ é™¤ç›¸å…³çš„Pod
                    pod_result = subprocess.run([
                        'kubectl', 'delete', 'pods', '-n', 'gpu-health-expert',
                        '--field-selector', f'job-name={job_name}'
                    ], capture_output=True, text=True, timeout=60)
                    
                    if pod_result.returncode == 0:
                        logger.info(f"æˆåŠŸåˆ é™¤ç›¸å…³Pod: {job_name}")
                    else:
                        logger.warning(f"åˆ é™¤ç›¸å…³Podå¤±è´¥: {job_name}, é”™è¯¯: {pod_result.stderr}")
                else:
                    logger.warning(f"åˆ é™¤Jobå¤±è´¥: {job_name}, é”™è¯¯: {delete_result.stderr}")
                    
            except Exception as e:
                logger.warning(f"åˆ é™¤Job {job_name} å¤±è´¥: {e}")
        
        if deleted_jobs:
            logger.info(f"æˆåŠŸåˆ é™¤ {len(deleted_jobs)} ä¸ªJob: {deleted_jobs}")
            return True, f"æˆåŠŸåˆ é™¤ {len(deleted_jobs)} ä¸ªJob"
        else:
            logger.warning(f"æœªæ‰¾åˆ°Job IDä¸º {job_id} çš„Job")
            return False, f"æœªæ‰¾åˆ°Job IDä¸º {job_id} çš„Job"
            
    except Exception as e:
        logger.error(f"ä½¿ç”¨kubectlåˆ é™¤Jobå¤±è´¥: {e}")
        return False, str(e)

def delete_pvc_files_for_job(job_id: str, node_name: str):
    """åˆ é™¤æŒ‡å®šJobçš„PVCæ–‡ä»¶"""
    try:
        logger.info(f"å¼€å§‹åˆ é™¤PVCæ–‡ä»¶: job_id={job_id}, node_name={node_name}")
        
        pvc_path = '/shared/gpu-inspection-results/manual'
        logger.info(f"æ£€æŸ¥PVCè·¯å¾„: {pvc_path}")
        
        if not os.path.exists(pvc_path):
            logger.warning(f"PVCè·¯å¾„ä¸å­˜åœ¨: {pvc_path}")
            return
        
        logger.info(f"PVCè·¯å¾„å­˜åœ¨ï¼Œå¼€å§‹æŸ¥æ‰¾æ–‡ä»¶...")
        
        # åˆ—å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        all_files = os.listdir(pvc_path)
        logger.info(f"ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶: {all_files}")
        
        deleted_files = []
        
        # ç­–ç•¥1: æŸ¥æ‰¾åŒ…å«job_idçš„æ–‡ä»¶ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
        logger.info(f"ç­–ç•¥1: æŸ¥æ‰¾åŒ…å«job_id '{job_id}' çš„æ–‡ä»¶...")
        for filename in all_files:
            if filename.endswith('.json'):
                # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«job_id
                if job_id in filename:
                    file_path = os.path.join(pvc_path, filename)
                    logger.info(f"æ‰¾åˆ°åŒ¹é…æ–‡ä»¶: {file_path}")
                    try:
                        if os.path.exists(file_path):
                            os.remove(file_path)
                            # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„è¢«åˆ é™¤
                            if os.path.exists(file_path):
                                logger.warning(f"æ–‡ä»¶åˆ é™¤åä»ç„¶å­˜åœ¨: {file_path}")
                            else:
                                deleted_files.append(filename)
                                logger.info(f"æˆåŠŸåˆ é™¤PVCæ–‡ä»¶: {filename}")
                        else:
                            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ é™¤: {file_path}")
                    except Exception as e:
                        logger.warning(f"åˆ é™¤PVCæ–‡ä»¶å¤±è´¥: {filename}, é”™è¯¯: {e}")
        
        # ç­–ç•¥1.5: æŸ¥æ‰¾åŒ…å«node_nameçš„æ‰€æœ‰æ–‡ä»¶ï¼ˆå› ä¸ºæ–‡ä»¶ååªåŒ…å«node_nameï¼Œä¸åŒ…å«job_idï¼‰
        logger.info(f"ç­–ç•¥1.5: æŸ¥æ‰¾åŒ…å«node_name '{node_name}' çš„æ‰€æœ‰æ–‡ä»¶...")
        for filename in all_files:
            if filename.endswith('.json') and node_name in filename:
                # é¿å…é‡å¤åˆ é™¤ï¼ˆå¦‚æœç­–ç•¥1å·²ç»åˆ é™¤äº†ï¼‰
                if filename not in deleted_files:
                    file_path = os.path.join(pvc_path, filename)
                    logger.info(f"æ‰¾åˆ°node_nameåŒ¹é…æ–‡ä»¶: {file_path}")
                    try:
                        if os.path.exists(file_path):
                            os.remove(file_path)
                            # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„è¢«åˆ é™¤
                            if os.path.exists(file_path):
                                logger.warning(f"æ–‡ä»¶åˆ é™¤åä»ç„¶å­˜åœ¨: {file_path}")
                            else:
                                deleted_files.append(filename)
                                logger.info(f"æˆåŠŸåˆ é™¤PVCæ–‡ä»¶: {filename}")
                        else:
                            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ é™¤: {file_path}")
                    except Exception as e:
                        logger.warning(f"åˆ é™¤PVCæ–‡ä»¶å¤±è´¥: {filename}, é”™è¯¯: {e}")
        
        # ç­–ç•¥2: æŸ¥æ‰¾åŒ…å«node_nameçš„latestæ–‡ä»¶
        logger.info(f"ç­–ç•¥2: æŸ¥æ‰¾åŒ…å«node_name '{node_name}' çš„latestæ–‡ä»¶...")
        latest_pattern = f"{node_name}_latest.json"
        latest_file_path = os.path.join(pvc_path, latest_pattern)
        logger.info(f"æŸ¥æ‰¾latestæ–‡ä»¶: {latest_file_path}")
        
        if os.path.exists(latest_file_path):
            logger.info(f"æ‰¾åˆ°latestæ–‡ä»¶: {latest_file_path}")
            try:
                os.remove(latest_file_path)
                # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„è¢«åˆ é™¤
                if os.path.exists(latest_file_path):
                    logger.warning(f"latestæ–‡ä»¶åˆ é™¤åä»ç„¶å­˜åœ¨: {latest_file_path}")
                else:
                    deleted_files.append(latest_pattern)
                    logger.info(f"æˆåŠŸåˆ é™¤PVC latestæ–‡ä»¶: {latest_pattern}")
            except Exception as e:
                logger.warning(f"åˆ é™¤PVC latestæ–‡ä»¶å¤±è´¥: {latest_pattern}, é”™è¯¯: {e}")
        else:
            logger.info(f"latestæ–‡ä»¶ä¸å­˜åœ¨: {latest_file_path}")
        
        logger.info(f"PVCæ–‡ä»¶åˆ é™¤å®Œæˆï¼Œå…±åˆ é™¤ {len(deleted_files)} ä¸ªæ–‡ä»¶: {deleted_files}")
        
    except Exception as e:
        logger.error(f"åˆ é™¤PVCæ–‡ä»¶å¼‚å¸¸: {e}")

# ============================================================================
# Jobç®¡ç†API
# ============================================================================
@app.route('/api/gpu-inspection/delete-job', methods=['POST'])
@get_rate_limit_decorator()
def delete_job():
    """åˆ é™¤æŒ‡å®šçš„Job"""
    client_ip = request.remote_addr
    
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "è¯·æ±‚æ•°æ®ä¸ºç©º"}), 400
        
        job_id = data.get('jobId')
        if not job_id:
            return jsonify({"error": "ç¼ºå°‘Job IDå‚æ•°"}), 400
        
        logger.info(f"å¼€å§‹åˆ é™¤Job: {job_id}")
        
        # è°ƒç”¨å†…éƒ¨åˆ é™¤å‡½æ•°
        result = delete_job_internal(job_id)
        
        if result.get('success'):
            logger.info(f"Jobåˆ é™¤å®Œæˆ: {job_id}")
            return jsonify({
                "success": True,
                "message": f"Job {job_id} åˆ é™¤æˆåŠŸ",
                "jobId": job_id
            })
        else:
            logger.error(f"Jobåˆ é™¤å¤±è´¥: {job_id}, é”™è¯¯: {result.get('error')}")
            return jsonify({"error": f"åˆ é™¤Jobå¤±è´¥: {result.get('error')}"}), 500
        
    except Exception as e:
        logger.error(f"åˆ é™¤Jobå¼‚å¸¸: {e}")
        return jsonify({"error": f"åˆ é™¤Jobå¤±è´¥: {str(e)}"}), 500

@app.route('/api/gpu-inspection/delete-jobs', methods=['POST'])
@get_rate_limit_decorator()  # åº”ç”¨1åˆ†é’Ÿé¢‘ç‡é™åˆ¶
def delete_gpu_inspection_jobs():
    """åˆ é™¤GPUæ£€æŸ¥Job - åº”ç”¨1åˆ†é’Ÿé¢‘ç‡é™åˆ¶"""
    client_ip = request.remote_addr
    
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "è¯·æ±‚æ•°æ®ä¸ºç©º"}), 400
        
        job_ids = data.get('jobIds', [])
        if not job_ids:
            return jsonify({"error": "ç¼ºå°‘Job IDåˆ—è¡¨"}), 400
        
        if not isinstance(job_ids, list):
            return jsonify({"error": "Job IDå¿…é¡»æ˜¯åˆ—è¡¨æ ¼å¼"}), 400
        
        # è®°å½•å¼€å§‹åˆ é™¤
        logger.info(f"å¼€å§‹æ‰¹é‡åˆ é™¤Job: {job_ids}")
        
        success_count = 0
        failed_count = 0
        deleted_results_count = 0
        
        for job_id in job_ids:
            try:
                # 1. é¦–å…ˆæŸ¥è¯¢ç›¸å…³çš„è¯Šæ–­ç»“æœä¿¡æ¯ï¼Œç”¨äºåˆ é™¤PVCæ–‡ä»¶
                conn = get_db_connection()
                cursor = conn.cursor()
                
                # æŸ¥è¯¢è¯Šæ–­ç»“æœä¿¡æ¯
                cursor.execute('''
                    SELECT node_name FROM diagnostic_results 
                    WHERE job_id = ?
                ''', (job_id,))
                
                result_info = cursor.fetchone()
                node_name = result_info[0] if result_info else None
                
                logger.info(f"æŸ¥è¯¢Job {job_id} çš„èŠ‚ç‚¹ä¿¡æ¯: {node_name}")
                
                # å¦‚æœä»è¯Šæ–­ç»“æœä¸­æ— æ³•è·å–èŠ‚ç‚¹ä¿¡æ¯ï¼Œå°è¯•ä»Jobè®°å½•ä¸­è·å–
                if not node_name:
                    cursor.execute('''
                        SELECT selected_nodes FROM diagnostic_jobs 
                        WHERE job_id = ?
                    ''', (job_id,))
                    
                    job_info = cursor.fetchone()
                    if job_info and job_info[0]:
                        try:
                            selected_nodes = json.loads(job_info[0])
                            if selected_nodes and len(selected_nodes) > 0:
                                node_name = selected_nodes[0]  # å–ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
                                logger.info(f"ä»Jobè®°å½•ä¸­è·å–åˆ°èŠ‚ç‚¹ä¿¡æ¯: {node_name}")
                        except Exception as json_error:
                            logger.warning(f"è§£æJobèŠ‚ç‚¹ä¿¡æ¯å¤±è´¥: {json_error}")
                
                if not node_name:
                    logger.warning(f"æ— æ³•è·å–Job {job_id} çš„èŠ‚ç‚¹ä¿¡æ¯ï¼Œå°†å°è¯•é€šè¿‡æ–‡ä»¶ååŒ¹é…åˆ é™¤PVCæ–‡ä»¶")
                
                # åˆ é™¤ç›¸å…³çš„è¯Šæ–­ç»“æœ
                cursor.execute('''
                    DELETE FROM diagnostic_results 
                    WHERE job_id = ?
                ''', (job_id,))
                
                deleted_results = cursor.rowcount
                deleted_results_count += deleted_results
                
                if deleted_results > 0:
                    logger.info(f"åˆ é™¤Job {job_id} ç›¸å…³çš„è¯Šæ–­ç»“æœ: {deleted_results} æ¡")
                    
                    # åˆ é™¤ç›¸å…³çš„PVCæ–‡ä»¶
                    try:
                        if node_name:
                            logger.info(f"å¼€å§‹åˆ é™¤Job {job_id} ç›¸å…³çš„PVCæ–‡ä»¶ï¼ŒèŠ‚ç‚¹: {node_name}")
                            delete_pvc_files_for_job(job_id, node_name)
                            logger.info(f"æˆåŠŸåˆ é™¤Job {job_id} ç›¸å…³çš„PVCæ–‡ä»¶")
                        else:
                            # å³ä½¿æ²¡æœ‰èŠ‚ç‚¹ä¿¡æ¯ï¼Œä¹Ÿå°è¯•é€šè¿‡job_idåˆ é™¤ç›¸å…³æ–‡ä»¶
                            logger.info(f"å°è¯•é€šè¿‡job_idåˆ é™¤Job {job_id} ç›¸å…³çš„PVCæ–‡ä»¶")
                            delete_pvc_files_for_job(job_id, "unknown")
                            logger.info(f"é€šè¿‡job_idåˆ é™¤Job {job_id} ç›¸å…³çš„PVCæ–‡ä»¶å®Œæˆ")
                    except Exception as pvc_error:
                        logger.warning(f"åˆ é™¤Job {job_id} ç›¸å…³çš„PVCæ–‡ä»¶å¤±è´¥: {pvc_error}")
                
                # 2. åˆ é™¤Jobè®°å½•
                cursor.execute('''
                    DELETE FROM diagnostic_jobs 
                    WHERE job_id = ?
                ''', (job_id,))
                
                deleted_jobs = cursor.rowcount
                
                if deleted_jobs > 0:
                    success_count += 1
                    logger.info(f"æˆåŠŸåˆ é™¤Job: {job_id}")
                else:
                    failed_count += 1
                    logger.warning(f"æœªæ‰¾åˆ°è¦åˆ é™¤çš„Job: {job_id}")
                
                conn.commit()
                conn.close()
                
                # 3. å°è¯•åˆ é™¤Kubernetes Jobï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                try:
                    result = subprocess.run([
                        'kubectl', 'delete', 'job', f'ghx-manual-job-{job_id}', 
                        '--force', '--grace-period=0'
                    ], capture_output=True, text=True, timeout=60)
                    
                    if result.returncode == 0:
                        logger.info(f"æˆåŠŸåˆ é™¤Kubernetes Job: {job_id}")
                    else:
                        logger.warning(f"åˆ é™¤Kubernetes Jobå¤±è´¥: {job_id}, é”™è¯¯: {result.stderr}")
                        
                except subprocess.TimeoutExpired:
                    logger.warning(f"åˆ é™¤Kubernetes Jobè¶…æ—¶: {job_id}")
                except Exception as e:
                    logger.warning(f"åˆ é™¤Kubernetes Jobå¼‚å¸¸: {job_id}, é”™è¯¯: {e}")
                
            except Exception as e:
                failed_count += 1
                logger.error(f"åˆ é™¤Job {job_id} å¤±è´¥: {e}")
        
        # è®°å½•åˆ é™¤å®Œæˆ
        logger.info(f"æ‰¹é‡åˆ é™¤Jobå®Œæˆ: æˆåŠŸ={success_count}, å¤±è´¥={failed_count}, åˆ é™¤è¯Šæ–­ç»“æœ={deleted_results_count}æ¡")
        
        # è®°å½•æˆåŠŸè¯·æ±‚
        log_rate_limit_event(client_ip, "delete_gpu_inspection_jobs", "allowed")
        
        return jsonify({
            "success": True,
            "message": f"æ‰¹é‡åˆ é™¤å®Œæˆ: æˆåŠŸ={success_count}, å¤±è´¥={failed_count}, åˆ é™¤è¯Šæ–­ç»“æœ={deleted_results_count}æ¡",
            "deletedJobs": success_count,
            "failedJobs": failed_count,
            "deletedResults": deleted_results_count
        })
                
    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ é™¤Jobå¤±è´¥: {e}")
        return jsonify({
            "error": f"æ‰¹é‡åˆ é™¤Jobå¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/gpu-inspection/create-job', methods=['POST'])
@get_rate_limit_decorator()  # åº”ç”¨1åˆ†é’Ÿé¢‘ç‡é™åˆ¶
def create_gpu_inspection_job():
    """åˆ›å»ºGPUæ£€æŸ¥Job - åº”ç”¨1åˆ†é’Ÿé¢‘ç‡é™åˆ¶"""
    client_ip = request.remote_addr
    
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "è¯·æ±‚æ•°æ®ä¸ºç©º"}), 400
        
        # éªŒè¯å¿…éœ€å‚æ•°
        required_fields = ['selectedNodes', 'enabledTests']
        for field in required_fields:
            if field not in data:
                return jsonify({"error": f"ç¼ºå°‘å¿…éœ€å‚æ•°: {field}"}), 400
        
        selected_nodes = data['selectedNodes']
        enabled_tests = data['enabledTests']
        dcgm_level = data.get('dcgmLevel', 1)  # å¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼1
        
        # éªŒè¯å‚æ•°
        if not selected_nodes:
            return jsonify({"error": "å¿…é¡»é€‰æ‹©è‡³å°‘ä¸€ä¸ªèŠ‚ç‚¹"}), 400
        
        if not enabled_tests:
            return jsonify({"error": "å¿…é¡»é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ£€æŸ¥é¡¹ç›®"}), 400
        
        # åªæœ‰å½“é€‰æ‹©äº†dcgmæ£€æŸ¥é¡¹æ—¶æ‰éªŒè¯dcgmçº§åˆ«
        if 'dcgmDiag' in enabled_tests and dcgm_level not in [1, 2, 3, 4]:
            return jsonify({"error": "DCGMçº§åˆ«å¿…é¡»æ˜¯1-4ä¹‹é—´çš„æ•´æ•°"}), 400
        
        # ç”Ÿæˆå”¯ä¸€çš„Job ID
        job_id = f"manual-{int(time.time())}-{str(uuid.uuid4())[:8]}"
        
        # æ„å»ºç¯å¢ƒå˜é‡
        enabled_tests_str = ",".join(enabled_tests)
        selected_nodes_str = ",".join(selected_nodes)
        
        # è¯»å–Jobæ¨¡æ¿
        template_path = '/app/job-template.yaml'
        if not os.path.exists(template_path):
            template_path = 'job-template.yaml'  # å¼€å‘ç¯å¢ƒ
        
        try:
            with open(template_path, 'r', encoding='utf-8') as f:
                template_content = f.read()
        except FileNotFoundError:
            return jsonify({"error": "Jobæ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨"}), 500
        
        # ä¸ºæ¯ä¸ªé€‰ä¸­çš„èŠ‚ç‚¹åˆ›å»ºå•ç‹¬çš„Job
        created_jobs = []
        
        for node_name in selected_nodes:
            # åˆ›å»ºèŠ‚ç‚¹ç‰¹å®šçš„Job YAML
            node_job_yaml = template_content
            
            # å…ˆæ›¿æ¢Jobåç§°ï¼Œç¡®ä¿æ¯ä¸ªJobæœ‰å”¯ä¸€çš„åç§°
            node_job_name = f"ghx-manual-job-{job_id}-{node_name}"
            node_job_yaml = node_job_yaml.replace('ghx-manual-job-{JOB_ID}', node_job_name)
            
            # è·å–åŠ¨æ€èµ„æºä¿¡æ¯
            gpu_resource_name = get_gpu_resource_name(node_name)
            rdma_resources = get_rdma_resources(node_name)
            
            # ç„¶åæ›¿æ¢å…¶ä»–æ¨¡æ¿å˜é‡
            node_job_yaml = node_job_yaml.replace('{ENABLED_TESTS}', enabled_tests_str)
            node_job_yaml = node_job_yaml.replace('{DCGM_LEVEL}', str(dcgm_level))
            node_job_yaml = node_job_yaml.replace('{SELECTED_NODES}', selected_nodes_str)
            node_job_yaml = node_job_yaml.replace('{GPU_RESOURCE_NAME}', gpu_resource_name)
            
            # å¤„ç†RDMAèµ„æºï¼šå¦‚æœä¸ºç©ºåˆ™åˆ é™¤æ•´è¡Œï¼Œå¦åˆ™æ›¿æ¢å˜é‡
            if rdma_resources.strip():
                # å°†é€—å·åˆ†éš”çš„èµ„æºåç§°è½¬æ¢ä¸ºå¤šè¡Œæ ¼å¼ï¼Œæ¯ä¸ªèµ„æºéƒ½åŒ…å«æ•°é‡
                rdma_device_names = [name.strip() for name in rdma_resources.split(',') if name.strip()]
                rdma_resources_formatted = '\n'.join([f"            {name}: 1" for name in rdma_device_names])
                node_job_yaml = node_job_yaml.replace('            {RDMA_RESOURCES}: 1', rdma_resources_formatted)
            else:
                # åˆ é™¤åŒ…å« {RDMA_RESOURCES} çš„æ•´è¡Œ
                lines = node_job_yaml.split('\n')
                filtered_lines = []
                for line in lines:
                    if '{RDMA_RESOURCES}' not in line:
                        filtered_lines.append(line)
                node_job_yaml = '\n'.join(filtered_lines)
            
            # æ›¿æ¢åŸºç¡€Job IDæ ‡ç­¾ï¼Œæ‰€æœ‰Jobä½¿ç”¨ç›¸åŒçš„åŸºç¡€job_id
            node_job_yaml = node_job_yaml.replace('{BASE_JOB_ID}', job_id)
            
            # æ›¿æ¢Job IDç¯å¢ƒå˜é‡ï¼Œæ¯ä¸ªJobä½¿ç”¨å”¯ä¸€çš„job_id
            node_job_yaml = node_job_yaml.replace('{JOB_ID}', f"{job_id}-{node_name}")
            
            # æ›¿æ¢èŠ‚ç‚¹åç§°
            node_job_yaml = node_job_yaml.replace('{NODE_NAME}', node_name)
            
            logger.info(f"ä¸ºèŠ‚ç‚¹ {node_name} åˆ›å»ºJob: {node_job_name}")
            
            # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            temp_yaml_path = f"/tmp/job-{node_name}-{job_id}.yaml"
            with open(temp_yaml_path, 'w', encoding='utf-8') as f:
                f.write(node_job_yaml)
            
            try:
                # ä½¿ç”¨kubectlåˆ›å»ºJob
                result = subprocess.run([
                    'kubectl', 'apply', '-f', temp_yaml_path, '-n', 'gpu-health-expert'
                ], capture_output=True, text=True, timeout=60)
                
                if result.returncode == 0:
                    logger.info(f"æˆåŠŸåˆ›å»ºJob {node_job_name}")
                    created_jobs.append(node_job_name)
                else:
                    logger.error(f"åˆ›å»ºJob {node_job_name} å¤±è´¥: {result.stderr}")
                    raise Exception(f"åˆ›å»ºJob {node_job_name} å¤±è´¥: {result.stderr}")
                    
            except Exception as e:
                logger.error(f"åˆ›å»ºJob {node_job_name} å¼‚å¸¸: {e}")
                raise e
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_yaml_path):
                    os.remove(temp_yaml_path)
        
        if created_jobs:
            logger.info(f"æˆåŠŸåˆ›å»º {len(created_jobs)} ä¸ªJob: {created_jobs}")
            
            # å°†Jobä¿¡æ¯ä¿å­˜åˆ°æ•°æ®åº“
            try:
                conn = get_db_connection()
                cursor = conn.cursor()
                
                cursor.execute('''
                    INSERT INTO diagnostic_jobs 
                    (job_id, job_name, job_type, selected_nodes, enabled_tests, dcgm_level, status, expires_at, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, datetime('now', 'localtime'))
                ''', (
                    job_id,
                    f"ghx-manual-job-{job_id}",
                    'manual',
                    json.dumps(selected_nodes),
                    json.dumps(enabled_tests),
                    dcgm_level,
                    'pending',
                    datetime.now() + timedelta(days=7)
                ))
                
                conn.commit()
                logger.info(f"Jobä¿¡æ¯å·²ä¿å­˜åˆ°æ•°æ®åº“: {job_id}")
                
            except Exception as db_error:
                logger.error(f"ä¿å­˜Jobä¿¡æ¯åˆ°æ•°æ®åº“å¤±è´¥: {db_error}")
                # å³ä½¿æ•°æ®åº“ä¿å­˜å¤±è´¥ï¼ŒJobåˆ›å»ºä»ç„¶æˆåŠŸ
            
            finally:
                if conn:
                    conn.close()
            
            # é€šçŸ¥SSEå®¢æˆ·ç«¯JobçŠ¶æ€å˜åŒ–ï¼ˆä¸åˆå¹¶å‰é€»è¾‘ä¸€è‡´ï¼‰
            notify_job_status_change(job_id, 'pending')
            
            # è®°å½•æˆåŠŸè¯·æ±‚
            log_rate_limit_event(client_ip, "create_gpu_inspection_job", "allowed")
            
            return jsonify({
                "success": True,
                "jobId": job_id,
                "message": f"æˆåŠŸåˆ›å»º {len(created_jobs)} ä¸ªJob",
                "createdJobs": created_jobs,
                "timestamp": time.time()
            })
        else:
            return jsonify({
                "error": "æ²¡æœ‰æˆåŠŸåˆ›å»ºä»»ä½•Job"
            }), 500
                
    except Exception as e:
        logger.error(f"åˆ›å»ºGPUæ£€æŸ¥Jobå¤±è´¥: {e}")
        return jsonify({
            "error": f"åˆ›å»ºGPUæ£€æŸ¥Jobå¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/gpu-inspection/list-jobs', methods=['GET'])
@get_rate_limit_decorator()  # åº”ç”¨é¢‘ç‡é™åˆ¶
def list_gpu_inspection_jobs():
    """åˆ—å‡ºGPUæ£€æŸ¥Job - åº”ç”¨é¢‘ç‡é™åˆ¶"""
    client_ip = request.remote_addr
    
    try:
        # ä»æ•°æ®åº“è·å–Jobåˆ—è¡¨
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT * FROM diagnostic_jobs 
            ORDER BY created_at DESC
        ''')
        
        jobs_data = cursor.fetchall()
        jobs = []
        
        for job in jobs_data:
            # ç¡®ä¿æ—¶é—´å­—æ®µä½¿ç”¨localtime
            created_at = job['created_at']
            if created_at and isinstance(created_at, str):
                # å¦‚æœæ˜¯UTCæ—¶é—´å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºlocaltime
                try:
                    # å°è¯•è§£ææ—¶é—´å¹¶è½¬æ¢ä¸ºlocaltime
                    if 'T' in created_at:  # ISOæ ¼å¼
                        # ç§»é™¤Zåç¼€å¹¶è§£æ
                        time_str = created_at.replace('Z', '')
                        if '+' in time_str:
                            # å·²ç»æ˜¯å¸¦æ—¶åŒºçš„æ—¶é—´ï¼Œç›´æ¥è§£æ
                            dt = datetime.fromisoformat(time_str)
                        else:
                            # å‡è®¾æ˜¯UTCæ—¶é—´ï¼Œæ·»åŠ +00:00
                            dt = datetime.fromisoformat(time_str + '+00:00')
                        
                        # è½¬æ¢ä¸ºä¸œå…«åŒºæ—¶é—´
                        utc_time = dt.replace(tzinfo=timezone.utc)
                        local_dt = utc_time.astimezone(timezone(timedelta(hours=8)))
                        created_at = local_dt.strftime('%Y-%m-%d %H:%M:%S')
                    elif created_at.count(':') == 2:  # æ ‡å‡†æ ¼å¼
                        # å‡è®¾å·²ç»æ˜¯localtimeï¼Œç›´æ¥ä½¿ç”¨
                        pass
                except Exception as e:
                    logger.warning(f"æ—¶é—´è½¬æ¢å¤±è´¥: {created_at}, é”™è¯¯: {e}")
                    # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä¿æŒåŸå€¼
                    pass
            
            # è·å–æœ€æ–°çš„KubernetesçŠ¶æ€
            current_status = job['status']
            logger.info(f"ğŸ” Job {job['job_id']}: æ•°æ®åº“çŠ¶æ€={current_status}")
            k8s_status = get_kubernetes_job_status(job['job_id'])
            
            # å¦‚æœè·å–åˆ°KubernetesçŠ¶æ€ï¼Œä½¿ç”¨çœŸå®çŠ¶æ€
            if k8s_status:
                pod_status = k8s_status['pod_status']
                logger.info(f"Job {job['job_id']}: æ•°æ®åº“çŠ¶æ€={current_status}, K8sçŠ¶æ€={pod_status}")
                
                # æ€»æ˜¯é€šçŸ¥SSEå®¢æˆ·ç«¯æœ€æ–°çš„çŠ¶æ€ï¼Œç¡®ä¿å‰ç«¯åŒæ­¥
                if pod_status != current_status:
                    logger.info(f"JobçŠ¶æ€å‘ç”Ÿå˜åŒ–: {current_status} -> {pod_status}")
                    notify_job_status_change(job['job_id'], pod_status)
                    current_status = pod_status
                    
                    # å¦‚æœJobå·²å®Œæˆï¼Œè‡ªåŠ¨è§¦å‘è¯Šæ–­ç»“æœå…¥åº“
                    if pod_status in ['Completed', 'Succeeded', 'Failed']:
                        logger.info(f"æ£€æµ‹åˆ°Jobå®ŒæˆçŠ¶æ€: {pod_status}ï¼Œå¼€å§‹è‡ªåŠ¨å…¥åº“...")
                        handle_job_completion(job['job_id'])
                else:
                    # å³ä½¿çŠ¶æ€ç›¸åŒï¼Œä¹Ÿå‘é€å¿ƒè·³é€šçŸ¥ï¼Œç¡®ä¿å‰ç«¯è¿æ¥æ´»è·ƒ
                    logger.debug(f"JobçŠ¶æ€æœªå˜åŒ–ï¼Œå‘é€å¿ƒè·³: {pod_status}")
                    notify_job_status_change(job['job_id'], pod_status)
            else:
                # å¦‚æœæ— æ³•è·å–KubernetesçŠ¶æ€ï¼Œè¯´æ˜Jobå¯èƒ½å·²è¢«åˆ é™¤æˆ–ä¸å­˜åœ¨
                pod_status = 'unknown'
                logger.info(f"Job {job['job_id']}: æ— æ³•è·å–K8sçŠ¶æ€ï¼ŒJobå¯èƒ½å·²è¢«åˆ é™¤ï¼ŒçŠ¶æ€è®¾ä¸ºunknown")
                # å‘é€å¿ƒè·³é€šçŸ¥ï¼Œç¡®ä¿å‰ç«¯è¿æ¥æ´»è·ƒ
                notify_job_status_change(job['job_id'], 'unknown')
            
            job_info = {
                "name": job['job_name'],
                "jobId": job['job_id'],
                "status": current_status,
                "selectedNodes": json.loads(job['selected_nodes']) if job['selected_nodes'] else [],
                "enabledTests": json.loads(job['enabled_tests']) if job['enabled_tests'] else [],
                "dcgmLevel": job['dcgm_level'],
                "creationTimestamp": created_at,
                "completionTime": job['completed_at'],
                "startTime": job['started_at']
            }
            jobs.append(job_info)
        
        conn.close()
        
        response_data = {
            "success": True,
            "jobs": jobs,
            "total": len(jobs),
            "timestamp": time.time()
        }
        
        # è®°å½•æˆåŠŸè¯·æ±‚
        log_rate_limit_event(client_ip, "list_gpu_inspection_jobs", "allowed")
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"åˆ—å‡ºGPUæ£€æŸ¥Jobå¤±è´¥: {e}")
        return jsonify({
            "error": f"åˆ—å‡ºGPUæ£€æŸ¥Jobå¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/gpu-inspection/results', methods=['GET'])
@get_diagnostic_results_rate_limit_decorator()  # åº”ç”¨ä¸“é—¨çš„è¯Šæ–­ç»“æœé¢‘ç‡é™åˆ¶
def get_diagnostic_results():
    """è·å–è¯Šæ–­ç»“æœåˆ—è¡¨ - åº”ç”¨é¢‘ç‡é™åˆ¶å’Œç¼“å­˜"""
    client_ip = request.remote_addr
    
    # æ£€æŸ¥ç¼“å­˜ï¼ˆæ¥è‡ªåˆå¹¶å‰çš„gpu_cli.pyï¼‰
    current_time = time.time()
    cache_key = f"{client_ip}_diagnostic_results"
    
    if cache_key in diagnostic_results_cache:
        cache_data, cache_time = diagnostic_results_cache[cache_key]
        if current_time - cache_time < diagnostic_results_cache_timeout:
            logger.info(f"ä½¿ç”¨è¯Šæ–­ç»“æœç¼“å­˜æ•°æ®ï¼Œç¼“å­˜æ—¶é—´: {int(current_time - cache_time)}ç§’")
            return jsonify(cache_data)
    
    try:
        # æ¸…ç†è¿‡æœŸæ•°æ®ï¼ˆæ¥è‡ªåˆå¹¶å‰çš„gpu_cli.pyï¼‰
        cleanup_expired_data()
        
        # ä»æ•°æ®åº“è·å–è¯Šæ–­ç»“æœ
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT * FROM diagnostic_results 
            ORDER BY created_at DESC
        ''')
        
        results_data = cursor.fetchall()
        results = []
        
        for result in results_data:
            # è§£ææµ‹è¯•ç»“æœæ•°æ®ï¼ˆæ¥è‡ªåˆå¹¶å‰çš„gpu_cli.pyï¼‰
            test_results = json.loads(result['test_results']) if result['test_results'] else {}
            benchmark_data = json.loads(result['benchmark_data']) if result['benchmark_data'] else {}
            
            
            # æ„å»ºå®Œæ•´çš„æµ‹è¯•ç»“æœä¿¡æ¯ï¼ˆæ¥è‡ªåˆå¹¶å‰çš„gpu_cli.pyï¼‰
            result_info = {
                "id": result['id'],
                "jobId": result['job_id'],
                "jobType": result['job_type'],
                "nodeName": result['node_name'],
                "gpuType": result['gpu_type'],
                "enabledTests": json.loads(result['enabled_tests']) if result['enabled_tests'] else [],
                "dcgmLevel": result['dcgm_level'],
                "inspectionResult": result['inspection_result'],
                "performancePass": result['performance_pass'],
                "healthPass": result['health_pass'],
                "executionTime": result['execution_time'],
                "executionLog": result['execution_log'],  # æ·»åŠ æ‰§è¡Œæ—¥å¿—å­—æ®µ
                "createdAt": result['created_at'],
                "updatedAt": result['updated_at'] if 'updated_at' in result.keys() else result['created_at'],
                # æ·»åŠ å…·ä½“çš„æµ‹è¯•ç»“æœå€¼ï¼ˆæ¥è‡ªåˆå¹¶å‰çš„gpu_cli.pyï¼‰
                "nvbandwidthTest": test_results.get('bandwidth', {}).get('value', 'N/A') if test_results.get('bandwidth') else 'N/A',
                "p2pBandwidthLatencyTest": test_results.get('p2p', {}).get('value', 'N/A') if test_results.get('p2p') else 'N/A',
                "ncclTests": test_results.get('nccl', {}).get('value', 'N/A') if test_results.get('nccl') else 'N/A',
                "dcgmDiag": test_results.get('dcgm', 'N/A') if test_results.get('dcgm') else 'N/A',
                "ibCheck": test_results.get('ib', 'N/A') if test_results.get('ib') else 'N/A',
                # ä¿ç•™åŸå§‹æµ‹è¯•ç»“æœæ•°æ®
                "testResults": test_results,
                "benchmarkData": benchmark_data
            }
            results.append(result_info)
        
        conn.close()
        
        response_data = {
            "success": True,
            "results": results,
            "total": len(results),
            "timestamp": current_time,
            "cached": False
        }
        
        # æ›´æ–°ç¼“å­˜ï¼ˆæ¥è‡ªåˆå¹¶å‰çš„gpu_cli.pyï¼‰
        diagnostic_results_cache[cache_key] = (response_data, current_time)
        
        logger.info(f"æˆåŠŸè·å–{len(results)}æ¡è¯Šæ–­ç»“æœï¼Œå·²ç¼“å­˜")
        
        # è®°å½•æˆåŠŸè¯·æ±‚
        log_rate_limit_event(client_ip, "get_diagnostic_results", "allowed")
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"è·å–è¯Šæ–­ç»“æœå¤±è´¥: {e}")
        return jsonify({
            "error": f"è·å–è¯Šæ–­ç»“æœå¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/gpu-inspection/results/job/<job_id>', methods=['GET'])
@get_rate_limit_decorator()  # åº”ç”¨1åˆ†é’Ÿé¢‘ç‡é™åˆ¶
def get_diagnostic_result_by_job_id(job_id):
    """é€šè¿‡job_idè·å–è¯Šæ–­ç»“æœè¯¦æƒ… - åº”ç”¨1åˆ†é’Ÿé¢‘ç‡é™åˆ¶"""
    client_ip = request.remote_addr
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT * FROM diagnostic_results 
            WHERE job_id = ?
        ''', (job_id,))
        
        result = cursor.fetchone()
        conn.close()
        
        if not result:
            return jsonify({"error": "è¯Šæ–­ç»“æœä¸å­˜åœ¨"}), 404
        
        result_detail = {
            "id": result['id'],
            "jobId": result['job_id'],
            "jobType": result['job_type'],
            "nodeName": result['node_name'],
            "gpuType": result['gpu_type'],
            "enabledTests": json.loads(result['enabled_tests']) if result['enabled_tests'] else [],
            "dcgmLevel": result['dcgm_level'],
            "inspectionResult": result['inspection_result'],
            "performancePass": result['performance_pass'],
            "healthPass": result['health_pass'],
            "executionTime": result['execution_time'],
            "executionLog": result['execution_log'],
            "benchmarkData": json.loads(result['benchmark_data']) if result['benchmark_data'] else {},
            "testResults": json.loads(result['test_results']) if result['test_results'] else {},
            "createdAt": result['created_at'],
            "updatedAt": result['updated_at'] if 'updated_at' in result.keys() else result['created_at']
        }
        
        # è®°å½•æˆåŠŸè¯·æ±‚
        log_rate_limit_event(client_ip, "get_diagnostic_result_by_job_id", "allowed")
        
        return jsonify({
            "success": True,
            "result": result_detail,
            "timestamp": time.time()
        })
        
    except Exception as e:
        logger.error(f"è·å–è¯Šæ–­ç»“æœè¯¦æƒ…å¤±è´¥: {e}")
        return jsonify({
            "error": f"è·å–è¯Šæ–­ç»“æœè¯¦æƒ…å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/gpu-inspection/stop-job', methods=['POST'])
@get_rate_limit_decorator()  # åº”ç”¨1åˆ†é’Ÿé¢‘ç‡é™åˆ¶
def stop_gpu_inspection_job():
    """åœæ­¢GPUæ£€æŸ¥Job - åº”ç”¨1åˆ†é’Ÿé¢‘ç‡é™åˆ¶"""
    client_ip = request.remote_addr
    
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "è¯·æ±‚æ•°æ®ä¸ºç©º"}), 400
        
        job_id = data.get('jobId')
        if not job_id:
            return jsonify({"error": "ç¼ºå°‘Job ID"}), 400
        
        logger.info(f"å¼€å§‹åœæ­¢Job: {job_id}")
        
        # ä½¿ç”¨Kuberneteså®¢æˆ·ç«¯åˆ é™¤Job
        try:
            success, result = delete_job_with_kubernetes_client(job_id)
            
            if success:
                logger.info(f"æˆåŠŸåˆ é™¤Job: {result}")
            else:
                logger.warning(f"åˆ é™¤Jobå¤±è´¥: {result}")
                
        except Exception as e:
            logger.error(f"åˆ é™¤Jobå¼‚å¸¸: {e}")
            return jsonify({"error": f"åˆ é™¤Jobå¤±è´¥: {str(e)}"}), 500
        

        
        # æ›´æ–°æ•°æ®åº“çŠ¶æ€
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            
            cursor.execute('''
                UPDATE diagnostic_jobs 
                SET status = 'stopped', completed_at = datetime('now', 'localtime')
                WHERE job_id = ?
            ''', (job_id,))
            
            conn.commit()
            conn.close()
            logger.info(f"JobçŠ¶æ€å·²æ›´æ–°ä¸ºstopped: {job_id}")
            
        except Exception as db_error:
            logger.error(f"æ›´æ–°JobçŠ¶æ€å¤±è´¥: {db_error}")
        
        # è®°å½•æˆåŠŸè¯·æ±‚
        log_rate_limit_event(client_ip, "stop_gpu_inspection_job", "allowed")
        
        return jsonify({
            "success": True,
            "message": f"Job {job_id} å·²åœæ­¢",
            "jobId": job_id
        })
        
    except Exception as e:
        logger.error(f"åœæ­¢Jobå¤±è´¥: {e}")
        return jsonify({
            "error": f"åœæ­¢Jobå¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/gpu-inspection/check-job-status/<job_id>', methods=['POST'])
def manual_check_job_status(job_id):
    """æ‰‹åŠ¨æ£€æŸ¥æŒ‡å®šJobçš„çŠ¶æ€"""
    try:
        logger.info(f"æ‰‹åŠ¨æ£€æŸ¥JobçŠ¶æ€: {job_id}")
        
        # è·å–KubernetesçŠ¶æ€
        result = subprocess.run([
            'kubectl', 'get', 'jobs', '-n', 'gpu-health-expert',
            '--field-selector', f'metadata.labels.job-id={job_id}',
            '-o', 'jsonpath={.items[*].status.conditions[?(@.type=="Complete")].status}'
        ], capture_output=True, text=True, timeout=30)
        
        pod_status = 'unknown'
        if result.returncode == 0 and result.stdout.strip():
            if result.stdout.strip() == 'True':
                pod_status = 'completed'
            else:
                pod_status = 'running'
        
        logger.info(f"Job {job_id} å½“å‰KubernetesçŠ¶æ€: {pod_status}")
        
        # æ›´æ–°æ•°æ®åº“çŠ¶æ€
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('''
            UPDATE diagnostic_jobs 
            SET status = ?, updated_at = datetime('now', 'localtime')
            WHERE job_id = ?
        ''', (pod_status, job_id))
        conn.commit()
        conn.close()
        
        return jsonify({
            "success": True,
            "job_id": job_id,
            "status": pod_status,
            "message": f"JobçŠ¶æ€å·²æ›´æ–°ä¸º: {pod_status}"
        })
        
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨æ£€æŸ¥JobçŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "job_id": job_id,
            "error": f"æ£€æŸ¥JobçŠ¶æ€å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/gpu-inspection/node-status/all', methods=['GET'])
@get_rate_limit_decorator()  # åº”ç”¨é¢‘ç‡é™åˆ¶
def get_all_node_status():
    """è·å–æ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€ - åº”ç”¨é¢‘ç‡é™åˆ¶"""
    client_ip = request.remote_addr
    logger.info(f"æ‰§è¡Œå‘½ä»¤: /usr/local/bin/kubectl-resource-view node -t gpu")
    
    try:
        # æ‰§è¡Œkubectlå‘½ä»¤ - å¢åŠ è¶…æ—¶æ—¶é—´åˆ°2åˆ†é’Ÿ
        result = subprocess.run([
            '/usr/local/bin/kubectl-resource-view', 'node', '-t', 'gpu'
        ], capture_output=True, text=True, timeout=120)
        
        if result.returncode != 0:
            logger.error(f"æ‰§è¡Œkubectlå‘½ä»¤å¤±è´¥: {result.stderr}")
            return jsonify({
                "error": "æ‰§è¡Œkubectlå‘½ä»¤å¤±è´¥",
                "stderr": result.stderr
            }), 500
        
        # è§£ææ‰€æœ‰èŠ‚ç‚¹
        all_nodes = []
        for line in result.stdout.split('\n'):
            if line.strip() and '|' in line.strip():
                # è§£ææ ¼å¼: | hd03-gpu2-0011 | 0 | 0% | 0 | 0% | nvidia.com/gpu-h200 |
                parts = [part.strip() for part in line.strip().split('|') if part.strip()]
                
                if len(parts) >= 6:
                    node_name = parts[0]
                    
                    # è¿‡æ»¤æ‰è¡¨å¤´è¡Œï¼ˆNODE, NVIDIA/GPU REQç­‰ï¼‰
                    if node_name.upper() in ['NODE', 'NVIDIA/GPU REQ', 'NVIDIA/GPU REQ(%)', 'NVIDIA/GPU LIM', 'NVIDIA/GPU LIM(%)', 'GPU MODEL']:
                        logger.info(f"è·³è¿‡è¡¨å¤´è¡Œ: {node_name}")
                        continue
                    
                    # è¿‡æ»¤æ‰ä¸åŒ…å«å®é™…èŠ‚ç‚¹åç§°çš„è¡Œ
                    if not node_name.startswith('hd03-gpu2-'):
                        logger.info(f"è·³è¿‡éèŠ‚ç‚¹è¡Œ: {node_name}")
                        continue
                    
                    gpu_requested = int(parts[1]) if parts[1].isdigit() else 0
                    gpu_utilization = parts[2]
                    gpu_available = int(parts[3]) if parts[3].isdigit() else 0
                    gpu_capacity = parts[4]
                    gpu_type = parts[5]
                    
                    all_nodes.append({
                        "nodeName": node_name,
                        "gpuRequested": gpu_requested,
                        "gpuUtilization": gpu_utilization,
                        "gpuAvailable": gpu_available,
                        "gpuCapacity": gpu_capacity,
                        "gpuType": gpu_type,
                        "rawLine": line.strip()
                    })
        
        logger.info(f"æˆåŠŸè§£æ{len(all_nodes)}ä¸ªèŠ‚ç‚¹")
        
        # è®°å½•æˆåŠŸè¯·æ±‚
        log_rate_limit_event(client_ip, "get_all_node_status", "allowed")
        
        return jsonify({
            "success": True,
            "nodes": all_nodes,
            "total": len(all_nodes),
            "timestamp": time.time()
        })
        
    except subprocess.TimeoutExpired:
        logger.error("æ‰§è¡Œkubectlå‘½ä»¤è¶…æ—¶")
        return jsonify({
            "error": "æ‰§è¡Œkubectlå‘½ä»¤è¶…æ—¶"
        }), 500
    except Exception as e:
        logger.error(f"è·å–æ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            "error": f"è·å–æ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/gpu-inspection/results', methods=['POST'])
@get_rate_limit_decorator()  # åº”ç”¨1åˆ†é’Ÿé¢‘ç‡é™åˆ¶
def save_diagnostic_result():
    """ä¿å­˜è¯Šæ–­ç»“æœ - åº”ç”¨1åˆ†é’Ÿé¢‘ç‡é™åˆ¶"""
    client_ip = request.remote_addr
    
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "è¯·æ±‚æ•°æ®ä¸ºç©º"}), 400
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        required_fields = ['job_id', 'node_name', 'gpu_type', 'enabled_tests', 'dcgm_level', 'inspection_result', 'performance_pass', 'health_pass', 'execution_time', 'execution_log', 'benchmark_data', 'test_results']
        for field in required_fields:
            if field not in data:
                return jsonify({"error": f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"}), 400
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        conn = get_db_connection()
        cursor = conn.cursor()
        
        try:
            # å…ˆæ£€æŸ¥è®°å½•æ˜¯å¦å­˜åœ¨
            cursor.execute('SELECT created_at FROM diagnostic_results WHERE job_id = ? AND node_name = ?', (data['job_id'], data['node_name']))
            existing_record = cursor.fetchone()
            
            if existing_record:
                # è®°å½•å­˜åœ¨ï¼Œæ›´æ–°æ—¶ä¿æŒåŸæœ‰created_at
                cursor.execute('''
                    UPDATE diagnostic_results 
                    SET job_type = ?, gpu_type = ?, enabled_tests = ?, dcgm_level = ?,
                        inspection_result = ?, performance_pass = ?, health_pass = ?,
                        execution_time = ?, execution_log = ?, benchmark_data = ?,
                        test_results = ?, expires_at = ?, updated_at = datetime('now', 'localtime')
                    WHERE job_id = ? AND node_name = ?
                ''', (
                    data.get('job_type', 'manual'), data['node_name'], data['gpu_type'],
                    json.dumps(data['enabled_tests']), data['dcgm_level'],
                    data['inspection_result'], data['performance_pass'], data['health_pass'],
                    data['execution_time'], data['execution_log'],
                    json.dumps(data['benchmark_data']), json.dumps(data['test_results']),
                    datetime.now() + timedelta(days=7), data['job_id'], data['node_name']
                ))
            else:
                # è®°å½•ä¸å­˜åœ¨ï¼Œæ’å…¥æ–°è®°å½•
                cursor.execute('''
                    INSERT INTO diagnostic_results 
                    (job_id, job_type, node_name, gpu_type, enabled_tests, dcgm_level, 
                     inspection_result, performance_pass, health_pass, execution_time, 
                     execution_log, benchmark_data, test_results, expires_at, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now', 'localtime'))
                ''', (
                    data['job_id'], data.get('job_type', 'manual'), data['node_name'],
                    data['gpu_type'], json.dumps(data['enabled_tests']), data['dcgm_level'],
                    data['inspection_result'], data['performance_pass'], data['health_pass'],
                    data['execution_time'], data['execution_log'],
                    json.dumps(data['benchmark_data']), json.dumps(data['test_results']),
                    datetime.now() + timedelta(days=7)
                ))
            
            # åŒæ—¶æ›´æ–°JobçŠ¶æ€ä¸ºcompleted
            cursor.execute('''
                UPDATE diagnostic_jobs 
                SET status = 'completed', completed_at = datetime('now', 'localtime')
                WHERE job_id = ?
            ''', (data['job_id'],))
            
            conn.commit()
            logger.info(f"æˆåŠŸä¿å­˜è¯Šæ–­ç»“æœå¹¶æ›´æ–°JobçŠ¶æ€: {data['job_id']}")
            
            # é€šçŸ¥SSEå®¢æˆ·ç«¯JobçŠ¶æ€å˜åŒ–å’Œè¯Šæ–­ç»“æœæ›´æ–°
            notify_job_status_change(data['job_id'], 'completed')
            notify_diagnostic_results_update()
            
        except Exception as e:
            conn.rollback()
            raise e
        finally:
            conn.close()
        
        # è®°å½•æˆåŠŸè¯·æ±‚
        log_rate_limit_event(client_ip, "save_diagnostic_result", "allowed")
        
        return jsonify({
            "success": True,
            "message": "è¯Šæ–­ç»“æœä¿å­˜æˆåŠŸ",
            "job_id": data['job_id']
        })
                
    except Exception as e:
        logger.error(f"ä¿å­˜è¯Šæ–­ç»“æœå¤±è´¥: {e}")
        return jsonify({
            "error": f"ä¿å­˜è¯Šæ–­ç»“æœå¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/gpu-inspection/results/<int:result_id>', methods=['GET'])
@get_rate_limit_decorator()  # åº”ç”¨1åˆ†é’Ÿé¢‘ç‡é™åˆ¶
def get_diagnostic_result_detail(result_id):
    """è·å–è¯Šæ–­ç»“æœè¯¦æƒ… - åº”ç”¨1åˆ†é’Ÿé¢‘ç‡é™åˆ¶"""
    client_ip = request.remote_addr
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT * FROM diagnostic_results 
            WHERE id = ?
        ''', (result_id,))
        
        result = cursor.fetchone()
        conn.close()
        
        if not result:
            return jsonify({"error": "è¯Šæ–­ç»“æœä¸å­˜åœ¨"}), 404
        
        result_detail = {
            "id": result['id'],
            "jobId": result['job_id'],
            "jobType": result['job_type'],
            "nodeName": result['node_name'],
            "gpuType": result['gpu_type'],
            "enabledTests": json.loads(result['enabled_tests']) if result['enabled_tests'] else [],
            "dcgmLevel": result['dcgm_level'],
            "inspectionResult": result['inspection_result'],
            "performancePass": result['performance_pass'],
            "healthPass": result['health_pass'],
            "executionTime": result['execution_time'],
            "executionLog": result['execution_log'],
            "benchmarkData": json.loads(result['benchmark_data']) if result['benchmark_data'] else {},
            "testResults": json.loads(result['test_results']) if result['test_results'] else {},
            "createdAt": result['created_at']
        }
        
        # è®°å½•æˆåŠŸè¯·æ±‚
        log_rate_limit_event(client_ip, "get_diagnostic_result_detail", "allowed")
        
        return jsonify({
            "success": True,
            "result": result_detail
        })
        
    except Exception as e:
        logger.error(f"è·å–è¯Šæ–­ç»“æœè¯¦æƒ…å¤±è´¥: {e}")
        return jsonify({
            "error": f"è·å–è¯Šæ–­ç»“æœè¯¦æƒ…å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/gpu-inspection/results/delete', methods=['POST'])
@get_rate_limit_decorator()  # åº”ç”¨1åˆ†é’Ÿé¢‘ç‡é™åˆ¶
def delete_diagnostic_results_by_job():
    """åˆ é™¤è¯Šæ–­ç»“æœ - åº”ç”¨1åˆ†é’Ÿé¢‘ç‡é™åˆ¶"""
    client_ip = request.remote_addr
    
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "è¯·æ±‚æ•°æ®ä¸ºç©º"}), 400
        
        job_ids = data.get('jobIds', [])
        if not job_ids:
            return jsonify({"error": "ç¼ºå°‘è¦åˆ é™¤çš„Job IDåˆ—è¡¨"}), 400
        
        # ä»æ•°æ®åº“åˆ é™¤è¯Šæ–­ç»“æœ
        conn = get_db_connection()
        cursor = conn.cursor()
        
        placeholders = ','.join(['?' for _ in job_ids])
        cursor.execute(f'''
            DELETE FROM diagnostic_results 
            WHERE job_id IN ({placeholders})
        ''', job_ids)
        
        deleted_count = cursor.rowcount
        conn.commit()
        conn.close()
        
        logger.info(f"æˆåŠŸåˆ é™¤{deleted_count}æ¡è¯Šæ–­ç»“æœ")
        
        # è®°å½•æˆåŠŸè¯·æ±‚
        log_rate_limit_event(client_ip, "delete_diagnostic_results_by_job", "allowed")
        
        return jsonify({
            "success": True,
            "message": f"æˆåŠŸåˆ é™¤{deleted_count}æ¡è¯Šæ–­ç»“æœ",
            "deletedCount": deleted_count
        })
        
    except Exception as e:
        logger.error(f"åˆ é™¤è¯Šæ–­ç»“æœå¤±è´¥: {e}")
        return jsonify({
            "error": f"åˆ é™¤è¯Šæ–­ç»“æœå¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/gpu-inspection/delete-diagnostic-result', methods=['POST'])
@get_rate_limit_decorator()
def delete_diagnostic_result():
    """åˆ é™¤æŒ‡å®šçš„è¯Šæ–­ç»“æœ"""
    client_ip = request.remote_addr
    
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "è¯·æ±‚æ•°æ®ä¸ºç©º"}), 400
        
        result_id = data.get('resultId')
        if not result_id:
            return jsonify({"error": "ç¼ºå°‘ç»“æœIDå‚æ•°"}), 400
        
        logger.info(f"å¼€å§‹åˆ é™¤è¯Šæ–­ç»“æœ: {result_id}")
        
        # ä»æ•°æ®åº“ä¸­åˆ é™¤è¯Šæ–­ç»“æœ
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            
            # å…ˆæŸ¥è¯¢è¯Šæ–­ç»“æœä¿¡æ¯ï¼Œè·å–job_id
            cursor.execute('''
                SELECT job_id, node_name FROM diagnostic_results 
                WHERE id = ?
            ''', (result_id,))
            
            result_info = cursor.fetchone()
            
            if result_info:
                job_id = result_info[0]
                node_name = result_info[1]
                
                # åˆ é™¤è¯Šæ–­ç»“æœ
                cursor.execute("DELETE FROM diagnostic_results WHERE id = ?", (result_id,))
                result_deleted = cursor.rowcount > 0
                
                if result_deleted:
                    logger.info(f"è¯Šæ–­ç»“æœåˆ é™¤æˆåŠŸ: ID={result_id}, Job={job_id}, èŠ‚ç‚¹={node_name}")
                    
                    # åˆ é™¤å¯¹åº”çš„PVCæ–‡ä»¶
                    try:
                        delete_pvc_files_for_job(job_id, node_name)
                        logger.info(f"PVCæ–‡ä»¶åˆ é™¤æˆåŠŸ: Job={job_id}, èŠ‚ç‚¹={node_name}")
                    except Exception as pvc_error:
                        logger.warning(f"PVCæ–‡ä»¶åˆ é™¤å¤±è´¥: Job={job_id}, é”™è¯¯: {pvc_error}")
                    
                    conn.commit()
                    conn.close()
                    
                    return jsonify({
                        "success": True,
                        "message": f"è¯Šæ–­ç»“æœ {result_id} åˆ é™¤æˆåŠŸ",
                        "resultId": result_id
                    })
                else:
                    conn.close()
                    logger.warning(f"åˆ é™¤è¯Šæ–­ç»“æœå¤±è´¥: ID={result_id}")
                    return jsonify({"error": f"åˆ é™¤è¯Šæ–­ç»“æœå¤±è´¥: {result_id}"}), 500
            else:
                conn.close()
                logger.warning(f"æœªæ‰¾åˆ°è¦åˆ é™¤çš„è¯Šæ–­ç»“æœ: {result_id}")
                return jsonify({"error": f"æœªæ‰¾åˆ°è¯Šæ–­ç»“æœ: {result_id}"}), 404
            
        except Exception as e:
            logger.error(f"åˆ é™¤è¯Šæ–­ç»“æœå¤±è´¥: {e}")
            return jsonify({"error": f"åˆ é™¤è¯Šæ–­ç»“æœå¤±è´¥: {str(e)}"}), 500
        
    except Exception as e:
        logger.error(f"åˆ é™¤è¯Šæ–­ç»“æœå¼‚å¸¸: {e}")
        return jsonify({"error": f"åˆ é™¤è¯Šæ–­ç»“æœå¤±è´¥: {str(e)}"        }), 500

@app.route('/api/gpu-inspection/delete-diagnostic-results', methods=['POST'])
@get_rate_limit_decorator()
def delete_diagnostic_results():
    """æ‰¹é‡åˆ é™¤æŒ‡å®šçš„è¯Šæ–­ç»“æœ"""
    client_ip = request.remote_addr
    
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "è¯·æ±‚æ•°æ®ä¸ºç©º"}), 400
        
        result_ids = data.get('resultIds', [])
        if not result_ids or not isinstance(result_ids, list):
            return jsonify({"error": "ç¼ºå°‘ç»“æœIDså‚æ•°æˆ–æ ¼å¼é”™è¯¯"}), 400
        
        logger.info(f"å¼€å§‹æ‰¹é‡åˆ é™¤è¯Šæ–­ç»“æœ: {result_ids}")
        
        deleted_results = []
        failed_results = []
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        try:
            for result_id in result_ids:
                try:
                    # å…ˆæŸ¥è¯¢èŠ‚ç‚¹åç§°ï¼Œç”¨äºåˆ é™¤PVCæ–‡ä»¶
                    cursor.execute('''
                        SELECT node_name, job_id FROM diagnostic_results 
                        WHERE job_id = ?
                    ''', (result_id,))
                    
                    result_info = cursor.fetchone()
                    
                    if result_info:
                        node_name = result_info[0]
                        job_id = result_info[1]
                        
                        # åˆ é™¤è¯Šæ–­ç»“æœ
                        cursor.execute("DELETE FROM diagnostic_results WHERE job_id = ?", (result_id,))
                        result_deleted = cursor.rowcount > 0
                        
                        if result_deleted:
                            logger.info(f"è¯Šæ–­ç»“æœåˆ é™¤æˆåŠŸ: Job={result_id}, èŠ‚ç‚¹={node_name}")
                            
                            # åˆ é™¤å¯¹åº”çš„PVCæ–‡ä»¶
                            try:
                                delete_pvc_files_for_job(job_id, node_name)
                                logger.info(f"PVCæ–‡ä»¶åˆ é™¤æˆåŠŸ: Job={job_id}, èŠ‚ç‚¹={node_name}")
                            except Exception as pvc_error:
                                logger.warning(f"PVCæ–‡ä»¶åˆ é™¤å¤±è´¥: Job={job_id}, é”™è¯¯: {pvc_error}")
                            
                            deleted_results.append(result_id)
                        else:
                            logger.warning(f"åˆ é™¤è¯Šæ–­ç»“æœå¤±è´¥: Job={result_id}")
                            failed_results.append(result_id)
                    else:
                        logger.warning(f"æœªæ‰¾åˆ°è¦åˆ é™¤çš„è¯Šæ–­ç»“æœ: {result_id}")
                        failed_results.append(result_id)
                        
                except Exception as e:
                    logger.error(f"åˆ é™¤è¯Šæ–­ç»“æœ {result_id} å¤±è´¥: {e}")
                    failed_results.append(result_id)
            
            conn.commit()
            conn.close()
            
            logger.info(f"æ‰¹é‡åˆ é™¤å®Œæˆ: æˆåŠŸ={len(deleted_results)}, å¤±è´¥={len(failed_results)}")
            
            return jsonify({
                "success": True,
                "message": f"æ‰¹é‡åˆ é™¤å®Œæˆ: æˆåŠŸ{len(deleted_results)}ä¸ª, å¤±è´¥{len(failed_results)}ä¸ª",
                "deletedResults": deleted_results,
                "failedResults": failed_results,
                "deletedCount": len(deleted_results),
                "failedCount": len(failed_results)
            })
            
        except Exception as e:
            conn.rollback()
            conn.close()
            raise e
        
    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ é™¤è¯Šæ–­ç»“æœå¤±è´¥: {e}")
        return jsonify({"error": f"æ‰¹é‡åˆ é™¤è¯Šæ–­ç»“æœå¤±è´¥: {str(e)}"        }), 500

@app.route('/api/gpu-inspection/job-status/<job_id>', methods=['GET'])
@get_rate_limit_decorator()  # åº”ç”¨é¢‘ç‡é™åˆ¶
def get_job_status(job_id):
    """è·å–JobçŠ¶æ€ - åº”ç”¨é¢‘ç‡é™åˆ¶"""
    client_ip = request.remote_addr
    
    try:
        # é¦–å…ˆå°è¯•ä»æ•°æ®åº“è·å–åŸºç¡€ä¿¡æ¯
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT job_id, job_name, job_type, status, created_at, started_at, completed_at, error_message
            FROM diagnostic_jobs 
            WHERE job_id = ?
        ''', (job_id,))
        
        job = cursor.fetchone()
        conn.close()
        
        if not job:
            return jsonify({"error": "Jobä¸å­˜åœ¨"}), 404
        
        # é€šè¿‡kubectlè·å–çœŸå®çš„Kubernetes JobçŠ¶æ€
        k8s_status = get_kubernetes_job_status(job_id)
        if k8s_status:
            pod_status = k8s_status['pod_status']
        else:
            # å¦‚æœæ— æ³•è·å–KubernetesçŠ¶æ€ï¼Œè¯´æ˜Jobå¯èƒ½å·²è¢«åˆ é™¤æˆ–ä¸å­˜åœ¨
            pod_status = 'unknown'
        
        # å¦‚æœJobå·²å®Œæˆæˆ–å¤±è´¥ï¼Œè‡ªåŠ¨è§¦å‘å…¥åº“
        if (pod_status in ['Completed', 'Succeeded', 'Failed'] or 
            'Failed' in pod_status or 
            'Error' in pod_status):
            try:
                logger.info(f"æ£€æµ‹åˆ°JobçŠ¶æ€å˜åŒ–: {pod_status}ï¼Œå¼€å§‹è‡ªåŠ¨å…¥åº“...")
                
                auto_collect_result = collector.collect_manual_results_from_pvc_internal()
                if auto_collect_result.get('success'):
                    logger.info(f"JobçŠ¶æ€æ£€æŸ¥æ—¶è‡ªåŠ¨å…¥åº“æˆåŠŸ: {auto_collect_result.get('processedCount', 0)} ä¸ªæ–‡ä»¶")
                    
                    # å…¥åº“æˆåŠŸåï¼Œç«‹å³é€šçŸ¥å‰ç«¯åˆ·æ–°è¯Šæ–­ç»“æœ
                    logger.info("å…¥åº“æˆåŠŸï¼Œé€šçŸ¥å‰ç«¯åˆ·æ–°è¯Šæ–­ç»“æœ")
                    notify_diagnostic_results_update()
                else:
                    logger.warning(f"JobçŠ¶æ€æ£€æŸ¥æ—¶è‡ªåŠ¨å…¥åº“å¤±è´¥: {auto_collect_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            except Exception as collect_error:
                logger.warning(f"JobçŠ¶æ€æ£€æŸ¥æ—¶è‡ªåŠ¨å…¥åº“å¼‚å¸¸: {collect_error}")
        
        # å¯¹äºæ‰€æœ‰çŠ¶æ€å˜åŒ–ï¼Œéƒ½é€šçŸ¥å‰ç«¯æ›´æ–°JobçŠ¶æ€
        logger.info(f"JobçŠ¶æ€: {job_id} -> {pod_status}")
        notify_job_status_change(job_id, pod_status)
        
        # æ„å»ºå“åº”æ•°æ®
        job_info = {
            "job_id": job['job_id'],
            "job_name": job['job_name'],
            "job_type": job['job_type'],
            "status": pod_status,  # ä½¿ç”¨KubernetesçŠ¶æ€
            "created_at": job['created_at'],
            "started_at": job['started_at'],
            "completed_at": job['completed_at'],
            "error_message": job['error_message'],
            "k8s_status": {
                "pod_status": pod_status
            },
            "last_status_update": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # è®°å½•æˆåŠŸè¯·æ±‚
        log_rate_limit_event(client_ip, "get_job_status", "allowed")
        
        return jsonify({
            "success": True,
            "job": job_info
        })
        
    except Exception as e:
        logger.error(f"è·å–JobçŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            "error": f"è·å–JobçŠ¶æ€å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/gpu-inspection/job-status-stream', methods=['GET'])
def job_status_stream():
    """SSEæµç«¯ç‚¹ - å®æ—¶æ¨é€JobçŠ¶æ€å˜åŒ–"""
    def generate():
        client_queue = queue.Queue()
        sse_clients.add(client_queue)
        logger.info(f"æ–°çš„SSEå®¢æˆ·ç«¯å·²è¿æ¥ï¼Œå½“å‰è¿æ¥æ•°: {len(sse_clients)}")
        
        try:
            # å‘é€è¿æ¥ç¡®è®¤
            yield f"data: {json.dumps({'type': 'connected', 'message': 'SSEè¿æ¥å·²å»ºç«‹'})}\n\n"
            
            # ä¿æŒè¿æ¥æ´»è·ƒï¼ŒåŒæ—¶æ£€æŸ¥é˜Ÿåˆ—ä¸­çš„æ¶ˆæ¯
            while True:
                try:
                    # æ£€æŸ¥é˜Ÿåˆ—ä¸­æ˜¯å¦æœ‰æ¶ˆæ¯
                    try:
                        # éé˜»å¡æ–¹å¼æ£€æŸ¥é˜Ÿåˆ—
                        message = client_queue.get_nowait()
                        yield message
                    except queue.Empty:
                        # é˜Ÿåˆ—ä¸ºç©ºï¼Œå‘é€å¿ƒè·³
                        yield f"data: {json.dumps({'type': 'heartbeat', 'timestamp': time.time()})}\n\n"
                        time.sleep(30)  # 30ç§’å¿ƒè·³
                        
                except GeneratorExit:
                    break
                    
        except Exception as e:
            logger.error(f"SSEè¿æ¥å¼‚å¸¸: {e}")
        finally:
            sse_clients.discard(client_queue)
            logger.info(f"SSEè¿æ¥å·²å…³é—­ï¼Œå½“å‰è¿æ¥æ•°: {len(sse_clients)}")
    
    return app.response_class(
        generate(),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Headers': 'Cache-Control'
        }
    )

@app.route('/api/gpu-inspection/sse-test', methods=['GET'])
def sse_test():
    """SSEè¿æ¥æµ‹è¯•ç«¯ç‚¹"""
    try:
        test_message = {
            "type": "test",
            "message": "SSEè¿æ¥æµ‹è¯•æˆåŠŸ",
            "timestamp": time.time()
        }
        
        # å°è¯•å‘é€ç»™æ‰€æœ‰SSEå®¢æˆ·ç«¯
        if sse_clients:
            for client in sse_clients:
                try:
                    client.put(f"data: {json.dumps(test_message)}\n\n")
                except Exception as e:
                    logger.warning(f"å‘é€æµ‹è¯•æ¶ˆæ¯å¤±è´¥: {e}")
            
            logger.info(f"å·²å‘é€æµ‹è¯•æ¶ˆæ¯åˆ° {len(sse_clients)} ä¸ªSSEå®¢æˆ·ç«¯")
            return jsonify({
                "success": True,
                "message": f"æµ‹è¯•æ¶ˆæ¯å·²å‘é€åˆ° {len(sse_clients)} ä¸ªSSEå®¢æˆ·ç«¯",
                "sse_clients_count": len(sse_clients)
            })
        else:
            logger.warning("æ²¡æœ‰SSEå®¢æˆ·ç«¯è¿æ¥")
            return jsonify({
                "success": False,
                "message": "æ²¡æœ‰SSEå®¢æˆ·ç«¯è¿æ¥",
                "sse_clients_count": 0
            })
            
    except Exception as e:
        logger.error(f"SSEè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"SSEè¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/gpu-inspection/sse-status', methods=['GET'])
def get_sse_status():
    """è·å–SSEè¿æ¥çŠ¶æ€"""
    try:
        sse_status = {
            "clients_count": len(sse_clients) if sse_clients else 0,
            "clients_details": []
        }
        
        if sse_clients:
            for i, client in enumerate(sse_clients):
                try:
                    client_info = {
                        "client_id": i,
                        "type": type(client).__name__,
                        "queue_size": client.qsize() if hasattr(client, 'qsize') else "unknown"
                    }
                    sse_status["clients_details"].append(client_info)
                except Exception as e:
                    sse_status["clients_details"].append({
                        "client_id": i,
                        "error": str(e)
                    })
        
        return jsonify({
            "success": True,
            "sse_status": sse_status,
            "timestamp": time.time()
        })
        
    except Exception as e:
        logger.error(f"è·å–SSEçŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            "error": f"è·å–SSEçŠ¶æ€å¤±è´¥: {str(e)}"
        }), 500

# ============================================================================
# å¥åº·æ£€æŸ¥å’ŒçŠ¶æ€API
# ============================================================================
@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'service': 'GHX (GPU Health Expert) Unified Service'
    })

@app.route('/api/gpu-inspection/health', methods=['GET'])
def gpu_inspection_health():
    """GPUæ£€æŸ¥æœåŠ¡å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'service': 'GHX GPU Health Expert Data Collector'
    })

@app.route('/api/gpu-inspection/status', methods=['GET'])
def get_status():
    """è·å–æœåŠ¡çŠ¶æ€"""
    try:
        summary = collector.get_summary(24)
        return jsonify({
            'status': 'running',
            'timestamp': datetime.now().isoformat(),
            'lastUpdated': summary.get('lastUpdated'),
            'totalResults': summary.get('totalNodes', 0)
        })
        
    except Exception as e:
        logger.error(f"è·å–çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/rate-limit/stats', methods=['GET'])
def get_rate_limit_stats_api():
    """è·å–é™æµç»Ÿè®¡"""
    try:
        stats = get_rate_limit_stats()
        return jsonify(stats)
    except Exception as e:
        logger.error(f"è·å–é™æµç»Ÿè®¡å¤±è´¥: {e}")
        return jsonify({'error': str(e)        }), 500

@app.route('/api/gpu-inspection/collect-manual-results', methods=['POST'])
@get_rate_limit_decorator()  # åº”ç”¨1åˆ†é’Ÿé¢‘ç‡é™åˆ¶
def collect_manual_results_from_pvc():
    """ä»PVCæ”¶é›†manualç±»å‹çš„è¯Šæ–­ç»“æœæ–‡ä»¶å¹¶å…¥åº“"""
    client_ip = request.remote_addr
    
    try:
        logger.info("æ‰‹åŠ¨è§¦å‘manualç»“æœæ”¶é›†...")
        
        # ä»PVCè¯»å–manualç±»å‹çš„æ–‡ä»¶
        manual_result = collector.collect_manual_results_from_pvc_internal()
        
        if manual_result.get('success'):
            # é€šçŸ¥SSEå®¢æˆ·ç«¯è¯Šæ–­ç»“æœå·²æ›´æ–°
            notify_diagnostic_results_update()
            
            # è®°å½•æˆåŠŸè¯·æ±‚
            log_rate_limit_event(client_ip, "collect_manual_results_from_pvc", "allowed")
            
            return jsonify({
                "success": True,
                "message": f"æˆåŠŸå¤„ç† {manual_result.get('processedCount', 0)} ä¸ªmanualç»“æœæ–‡ä»¶",
                "processedCount": manual_result.get('processedCount', 0),
                "totalFiles": manual_result.get('totalFiles', 0)
            })
        else:
            return jsonify({
                "success": False,
                "error": manual_result.get('error', 'æœªçŸ¥é”™è¯¯')
            }), 500
                
    except Exception as e:
        logger.error(f"ä»PVCæ”¶é›†manualç»“æœå¤±è´¥: {e}")
        return jsonify({
            "error": f"ä»PVCæ”¶é›†manualç»“æœå¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/gpu-inspection/thread-status', methods=['GET'])
def get_thread_status():
    """è·å–çº¿ç¨‹çŠ¶æ€ä¿¡æ¯"""
    try:
        # æ£€æŸ¥å…¨å±€å˜é‡æ˜¯å¦å­˜åœ¨
        global_vars = {
            "status_check_thread_exists": 'status_check_thread' in globals(),
            "status_check_running_exists": 'status_check_running' in globals(),
            "sse_clients_exists": 'sse_clients' in globals()
        }
        
        # è·å–çº¿ç¨‹çŠ¶æ€
        thread_status = {
            "status_check_thread_alive": status_check_thread.is_alive() if status_check_thread else False,
            "status_check_running": status_check_running,
            "sse_clients_count": len(sse_clients) if sse_clients else 0
        }
        
        # è·å–çº¿ç¨‹ä¿¡æ¯
        thread_info = {}
        if status_check_thread:
            thread_info = {
                "thread_id": status_check_thread.ident,
                "thread_name": status_check_thread.name,
                "is_alive": status_check_thread.is_alive(),
                "is_daemon": status_check_thread.daemon
            }
        
        return jsonify({
            "success": True,
            "global_vars": global_vars,
            "thread_status": thread_status,
            "thread_info": thread_info,
            "timestamp": time.time()
        })
        
    except Exception as e:
        logger.error(f"è·å–çº¿ç¨‹çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            "error": f"è·å–çº¿ç¨‹çŠ¶æ€å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/gpu-inspection/results/cleanup', methods=['POST'])
@get_rate_limit_decorator()  # åº”ç”¨1åˆ†é’Ÿé¢‘ç‡é™åˆ¶
def cleanup_diagnostic_results():
    """æ¸…ç†è¿‡æœŸè¯Šæ–­ç»“æœ - åº”ç”¨1åˆ†é’Ÿé¢‘ç‡é™åˆ¶"""
    client_ip = request.remote_addr
    
    try:
        # æ¸…ç†è¿‡æœŸæ•°æ®
        cleanup_expired_data()
        
        # è®°å½•æˆåŠŸè¯·æ±‚
        log_rate_limit_event(client_ip, "cleanup_diagnostic_results", "allowed")
        
        return jsonify({
            "success": True,
            "message": "è¿‡æœŸæ•°æ®æ¸…ç†å®Œæˆ",
            "timestamp": time.time()
        })
        
    except Exception as e:
        logger.error(f"æ¸…ç†è¿‡æœŸè¯Šæ–­ç»“æœå¤±è´¥: {e}")
        return jsonify({
            "error": f"æ¸…ç†è¿‡æœŸè¯Šæ–­ç»“æœå¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/gpu-inspection/gpu-resource-info', methods=['GET'])
def get_gpu_resource_info():
    """è·å–GPUèµ„æºä¿¡æ¯"""
    try:
        gpu_resource_name = get_gpu_resource_name()
        return jsonify({
            "success": True,
            "gpuResourceName": gpu_resource_name,
            "timestamp": time.time()
        })
    except Exception as e:
        logger.error(f"è·å–GPUèµ„æºä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({
            "error": f"è·å–GPUèµ„æºä¿¡æ¯å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/gpu-inspection/rdma-resource-info', methods=['GET'])
def get_rdma_resource_info():
    """è·å–RDMAèµ„æºä¿¡æ¯"""
    try:
        rdma_resources = get_rdma_resources()
        return jsonify({
            "success": True,
            "rdmaResources": rdma_resources,
            "timestamp": time.time()
        })
    except Exception as e:
        logger.error(f"è·å–RDMAèµ„æºä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({
            "error": f"è·å–RDMAèµ„æºä¿¡æ¯å¤±è´¥: {str(e)}"
        }), 500

def get_gpu_resource_name(node_name=None):
    """è‡ªåŠ¨æ£€æµ‹GPUèµ„æºåç§° - ä¸è‡ªæ£€ä¸“åŒºä¿æŒä¸€è‡´çš„è§£æé€»è¾‘"""
    try:
        # å¦‚æœæŒ‡å®šäº†èŠ‚ç‚¹åï¼ŒåªæŸ¥è¯¢è¯¥èŠ‚ç‚¹
        if node_name:
            cmd = ['/usr/local/bin/kubectl-resource-view', 'node', node_name, '-t', 'gpu', '--no-format']
        else:
            cmd = ['/usr/local/bin/kubectl-resource-view', 'node', '-t', 'gpu']
        
        # ä½¿ç”¨kubectl-resource-viewè·å–GPUä¿¡æ¯ - å¢åŠ è¶…æ—¶æ—¶é—´åˆ°2åˆ†é’Ÿ
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
        
        if result.returncode == 0:
            # ä½¿ç”¨ä¸è‡ªæ£€ä¸“åŒºç›¸åŒçš„è§£æé€»è¾‘
            lines = result.stdout.strip().split('\n')
            
            # è·³è¿‡è¡¨å¤´è¡Œ
            data_lines = lines[1:] if len(lines) > 1 else []
            
            for line in data_lines:
                if line.strip():
                    # è§£ææ ¼å¼: hd03-gpu2-0011          0               0%                      0               0%                      nvidia.com/gpu-h200
                    import re
                    
                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…èŠ‚ç‚¹åå’ŒGPUä¿¡æ¯
                    match = re.match(r'^(\S+)\s+(\d+)\s+\d+%\s+(\d+)\s+\d+%\s+(nvidia\.com/gpu-\S+|amd\.com/gpu-\S+|N/A)', line)
                    
                    if match:
                        current_node_name = match.group(1)
                        gpu_type = match.group(4)
                        
                        # è¿‡æ»¤æ‰è¡¨å¤´è¡Œå’Œæ²¡æœ‰GPUçš„èŠ‚ç‚¹
                        if current_node_name.upper() in ['NODE', 'NVIDIA/GPU REQ', 'NVIDIA/GPU REQ(%)', 'NVIDIA/GPU LIM', 'NVIDIA/GPU LIM(%)', 'GPU MODEL']:
                            continue
                        
                        # è·³è¿‡æ²¡æœ‰GPUçš„èŠ‚ç‚¹
                        if gpu_type == 'N/A':
                            continue
                        
                        # å¦‚æœæŒ‡å®šäº†èŠ‚ç‚¹åï¼Œåªè¿”å›è¯¥èŠ‚ç‚¹çš„GPUç±»å‹
                        if node_name and current_node_name == node_name:
                            return gpu_type
                        # å¦‚æœæ²¡æœ‰æŒ‡å®šèŠ‚ç‚¹åï¼Œè¿”å›ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„GPUç±»å‹
                        elif not node_name and current_node_name.startswith('hd03-gpu2-'):
                            return gpu_type
        
        # å¦‚æœkubectl-resource-viewå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨kubectl describe nodes
        result = subprocess.run([
            'kubectl', 'describe', 'nodes'
        ], capture_output=True, text=True, timeout=60)
        
        if result.returncode == 0:
            lines = result.stdout.split('\n')
            for line in lines:
                if 'nvidia.com/gpu' in line:
                    # æå–GPUèµ„æºåç§°
                    parts = line.split()
                    for part in parts:
                        if part.startswith('nvidia.com/gpu'):
                            return part
                elif 'amd.com/gpu' in line:
                    # æå–AMD GPUèµ„æºåç§°
                    parts = line.split()
                    for part in parts:
                        if part.startswith('amd.com/gpu'):
                            return part
        
        # é»˜è®¤è¿”å›nvidia.com/gpu
        return 'nvidia.com/gpu'
        
    except Exception as e:
        logger.warning(f"è·å–GPUèµ„æºåç§°å¤±è´¥: {e}")
        return 'nvidia.com/gpu'

def get_rdma_resources(node_name=None):
    """è·å–RDMAèµ„æºä¿¡æ¯"""
    try:
        # å¦‚æœæŒ‡å®šäº†èŠ‚ç‚¹åï¼ŒåªæŸ¥è¯¢è¯¥èŠ‚ç‚¹
        if node_name:
            cmd = ['kubectl-resource-view', 'node', node_name, '-t', 'gpu', '--no-format']
        else:
            cmd = ['kubectl-resource-view', 'node', '-t', 'gpu']
        
        # ä½¿ç”¨kubectl-resource-viewè·å–RDMAä¿¡æ¯
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
        
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            rdma_resources = []
            
            if node_name:
                # å•ä¸ªèŠ‚ç‚¹æ¨¡å¼ï¼šè·³è¿‡è¡¨å¤´ï¼Œç›´æ¥è§£ææ•°æ®è¡Œ
                data_lines = lines[1:] if len(lines) > 1 else []
            else:
                # å¤šèŠ‚ç‚¹æ¨¡å¼ï¼šè·³è¿‡è¡¨å¤´
                data_lines = lines[1:] if len(lines) > 1 else []
            
            for line in data_lines:
                # æŸ¥æ‰¾åŒ…å«rdma/çš„è¡Œ
                if 'rdma/' in line:
                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–rdmaè®¾å¤‡ä¿¡æ¯
                    import re
                    # åŒ¹é…æ‰€æœ‰ rdma/ å¼€å¤´çš„è®¾å¤‡: Y æ ¼å¼ï¼ŒåŒæ—¶æ•è·è®¾å¤‡åç§°å’Œæ•°é‡
                    matches = re.findall(r'rdma/([^:\s]+):\s*(\d+)', line)
                    
                    for match in matches:
                        device_name = f"rdma/{match[0]}"
                        count = int(match[1])
                        
                        if count > 0:  # åªæ·»åŠ æœ‰æ•°é‡çš„è®¾å¤‡
                            # æ·»åŠ åˆ°èµ„æºåˆ—è¡¨ï¼Œåªè¿”å›èµ„æºåç§°
                            # æ•°é‡ç”±ç”¨æˆ·åœ¨æ¨¡æ¿ä¸­å®šä¹‰
                            rdma_resources.append(device_name)
            
            if rdma_resources:
                # å»é‡ï¼šåªä¿ç•™å”¯ä¸€çš„è®¾å¤‡åç§°
                unique_resources = list(set(rdma_resources))
                
                logger.info(f"å‘ç° {len(rdma_resources)} ä¸ªRDMAè®¾å¤‡ï¼Œå»é‡å {len(unique_resources)} ä¸ª")
                # è¿”å›å»é‡åçš„èµ„æºåç§°åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”
                return ','.join(unique_resources)
        
        # å¦‚æœkubectl-resource-viewå¤±è´¥ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ï¼ˆåˆ é™¤æ¨¡æ¿ä¸­çš„å˜é‡ï¼‰
        logger.warning("æ— æ³•è·å–RDMAèµ„æºä¿¡æ¯ï¼Œå°†åˆ é™¤æ¨¡æ¿ä¸­çš„RDMAèµ„æºé…ç½®")
        return ""
        
    except Exception as e:
        logger.error(f"è·å–RDMAèµ„æºå¤±è´¥: {e}")
        return ""


def cleanup_expired_data():
    """æ¸…ç†è¿‡æœŸæ•°æ®"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # æ¸…ç†è¿‡æœŸçš„è¯Šæ–­ç»“æœ
        cursor.execute('''
            DELETE FROM diagnostic_results 
            WHERE expires_at < datetime('now')
        ''')
        expired_results = cursor.rowcount
        
        # æ¸…ç†è¿‡æœŸçš„Jobè®°å½•
        cursor.execute('''
            DELETE FROM diagnostic_jobs 
            WHERE expires_at < datetime('now')
        ''')
        expired_jobs = cursor.rowcount
        
        conn.commit()
        
        if expired_results > 0 or expired_jobs > 0:
            logger.info(f"æ¸…ç†è¿‡æœŸæ•°æ®å®Œæˆ: è¯Šæ–­ç»“æœ {expired_results} æ¡, Jobè®°å½• {expired_jobs} æ¡")
        
    except Exception as e:
        logger.error(f"æ¸…ç†è¿‡æœŸæ•°æ®å¤±è´¥: {e}")
    finally:
        if conn:
            conn.close()

# ============================================================================
# åå°ä»»åŠ¡
# ============================================================================
def background_collection():
    """åå°æ•°æ®æ”¶é›†ä»»åŠ¡"""
    retention_days = int(os.environ.get('GPU_RESULT_RETENTION_DAYS', 7))
    while True:
        try:
            logger.info("æ‰§è¡Œåå°æ•°æ®æ”¶é›†...")
            # æ”¶é›†cronç±»å‹çš„æ–‡ä»¶
            collector.collect_from_shared_pvc()
            # æ”¶é›†manualç±»å‹çš„æ–‡ä»¶
            manual_result = collector.collect_manual_results_from_pvc_internal()
            if manual_result.get('success'):
                logger.info(f"manualæ–‡ä»¶æ”¶é›†æˆåŠŸ: {manual_result.get('processedCount', 0)} ä¸ªæ–‡ä»¶")
            else:
                logger.warning(f"manualæ–‡ä»¶æ”¶é›†å¤±è´¥: {manual_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            collector.cleanup_old_files(retention_days)
            time.sleep(300)  # æ¯5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
        except Exception as e:
            logger.error(f"åå°æ•°æ®æ”¶é›†å¤±è´¥: {e}")
            time.sleep(60)  # å¤±è´¥å1åˆ†é’Ÿé‡è¯•

def init_shared_directories():
    """åˆå§‹åŒ–å…±äº«ç›®å½•"""
    try:
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        directories = [
            '/shared/gpu-inspection-results',
            '/shared/gpu-inspection-results/cron',
            '/shared/gpu-inspection-results/manual'
        ]
        
        for directory in directories:
            if not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
                logger.info(f"åˆ›å»ºç›®å½•: {directory}")
            else:
                logger.debug(f"ç›®å½•å·²å­˜åœ¨: {directory}")
        
        logger.info("å…±äº«ç›®å½•åˆå§‹åŒ–å®Œæˆ")
        
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–å…±äº«ç›®å½•å¤±è´¥: {e}")

# ============================================================================
# çƒ§æœºæµ‹è¯•ç›¸å…³API
# ============================================================================

# çƒ§æœºæµ‹è¯•çŠ¶æ€å­˜å‚¨
burnin_jobs = {}  # job_id -> job_info
burnin_metrics = {}  # job_id -> gpu_metrics
burnin_clients = set()  # WebSocketå®¢æˆ·ç«¯

@app.route('/api/burnin/create', methods=['POST'])
@get_rate_limit_decorator()
def create_burnin_job():
    """åˆ›å»ºçƒ§æœºæµ‹è¯•Job"""
    client_ip = request.remote_addr
    
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # éªŒè¯å¿…éœ€å‚æ•°
        required_fields = ['nodeName', 'memoryType', 'memoryValue', 'duration']
        for field in required_fields:
            if field not in data:
                return jsonify({
                    'success': False,
                    'error': f'ç¼ºå°‘å¿…éœ€å‚æ•°: {field}'
                }), 400
        
        node_name = data['nodeName']
        memory_type = data['memoryType']  # 'fixed' or 'percentage'
        memory_value = data['memoryValue']
        duration = int(data['duration'])
        
        # éªŒè¯å‚æ•°
        if memory_type not in ['fixed', 'percentage']:
            return jsonify({
                'success': False,
                'error': 'å†…å­˜ç±»å‹å¿…é¡»æ˜¯fixedæˆ–percentage'
            }), 400
        
        if memory_type == 'percentage' and (memory_value < 1 or memory_value > 100):
            return jsonify({
                'success': False,
                'error': 'å†…å­˜ç™¾åˆ†æ¯”å¿…é¡»åœ¨1-100ä¹‹é—´'
            }), 400
        
        if memory_type == 'fixed' and (memory_value < 1 or memory_value > 100000):
            return jsonify({
                'success': False,
                'error': 'å›ºå®šå†…å­˜å€¼å¿…é¡»åœ¨1-100000MBä¹‹é—´'
            }), 400
        
        if duration < 60 or duration > 86400:  # 1åˆ†é’Ÿåˆ°24å°æ—¶ï¼ˆç§’ï¼‰
            return jsonify({
                'success': False,
                'error': 'æµ‹è¯•æ—¶é•¿å¿…é¡»åœ¨1-1440åˆ†é’Ÿä¹‹é—´'
            }), 400
        
        # ç”ŸæˆJob ID
        job_id = f"burnin-{int(time.time())}"
        
        # æ„å»ºå†…å­˜å‚æ•°
        if memory_type == 'percentage':
            memory_param = f"{memory_value}%"
        else:
            memory_param = f"{memory_value}MB"
        
        # è·å–GPUèµ„æºåç§°
        gpu_resource_name = get_gpu_resource_name(node_name)
        if not gpu_resource_name:
            return jsonify({
                'success': False,
                'error': f'æ— æ³•è·å–èŠ‚ç‚¹ {node_name} çš„GPUèµ„æºåç§°'
            }), 400
        
        # åˆ›å»ºJob
        job_info = {
            'job_id': job_id,
            'node_name': node_name,
            'memory_type': memory_type,
            'memory_value': memory_value,
            'memory_param': memory_param,
            'duration': duration,
            'status': 'creating',
            'created_at': datetime.now().isoformat(),
            'progress': 0.0,
            'gpu_metrics': {}
        }
        
        # å­˜å‚¨Jobä¿¡æ¯
        burnin_jobs[job_id] = job_info
        burnin_metrics[job_id] = []
        
        # åœ¨åå°çº¿ç¨‹ä¸­åˆ›å»ºKubernetes Job
        def create_k8s_job():
            try:
                success = create_burnin_k8s_job(job_info, gpu_resource_name)
                if success:
                    burnin_jobs[job_id]['status'] = 'running'
                    logger.info(f"çƒ§æœºæµ‹è¯•Jobåˆ›å»ºæˆåŠŸ: {job_id}")
                else:
                    burnin_jobs[job_id]['status'] = 'failed'
                    logger.error(f"çƒ§æœºæµ‹è¯•Jobåˆ›å»ºå¤±è´¥: {job_id}")
            except Exception as e:
                burnin_jobs[job_id]['status'] = 'failed'
                logger.error(f"åˆ›å»ºçƒ§æœºæµ‹è¯•Jobå¼‚å¸¸: {e}")
        
        # å¯åŠ¨åå°çº¿ç¨‹
        thread = threading.Thread(target=create_k8s_job)
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True,
            'job_id': job_id,
            'message': 'çƒ§æœºæµ‹è¯•Jobåˆ›å»ºä¸­...'
        })
        
    except Exception as e:
        logger.error(f"åˆ›å»ºçƒ§æœºæµ‹è¯•Jobå¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f'åˆ›å»ºçƒ§æœºæµ‹è¯•Jobå¤±è´¥: {str(e)}'
        }), 500

def create_burnin_k8s_job(job_info, gpu_resource_name):
    """åˆ›å»ºKubernetesçƒ§æœºæµ‹è¯•Job"""
    try:
        if not kubernetes_client:
            logger.error("Kuberneteså®¢æˆ·ç«¯ä¸å¯ç”¨")
            return False
        
        v1, batch_v1 = kubernetes_client
        
        # ä»æŒ‚è½½çš„æ–‡ä»¶è¯»å–Jobæ¨¡æ¿
        try:
            with open('/app/burnin-job-template.yaml', 'r', encoding='utf-8') as f:
                job_template = f.read()
            logger.info("ä»æŒ‚è½½æ–‡ä»¶æˆåŠŸè¯»å–çƒ§æœºæµ‹è¯•Jobæ¨¡æ¿")
        except Exception as e:
            logger.error(f"ä»æŒ‚è½½æ–‡ä»¶è¯»å–çƒ§æœºæµ‹è¯•Jobæ¨¡æ¿å¤±è´¥: {e}")
            # å›é€€åˆ°ConfigMap APIè¯»å–
            try:
                configmap = v1.read_namespaced_config_map(
                    name='job-template-config',
                    namespace='gpu-health-expert'
                )
                job_template = configmap.data['burnin-job-template.yaml']
                logger.info("å›é€€åˆ°ConfigMap APIè¯»å–çƒ§æœºæµ‹è¯•Jobæ¨¡æ¿")
            except Exception as configmap_e:
                logger.error(f"ConfigMap APIè¯»å–ä¹Ÿå¤±è´¥: {configmap_e}")
                # æœ€åå›é€€åˆ°æœ¬åœ°æ–‡ä»¶
                try:
                    with open('burnin-job-template.yaml', 'r', encoding='utf-8') as f:
                        job_template = f.read()
                    logger.info("æœ€åå›é€€åˆ°æœ¬åœ°æ–‡ä»¶è¯»å–çƒ§æœºæµ‹è¯•Jobæ¨¡æ¿")
                except Exception as file_e:
                    logger.error(f"æ‰€æœ‰è¯»å–æ–¹å¼éƒ½å¤±è´¥: {file_e}")
                    return False
        
        # æ›¿æ¢æ¨¡æ¿å˜é‡
        job_yaml = job_template.format(
            JOB_ID=job_info['job_id'],
            BASE_JOB_ID=job_info['job_id'],
            NODE_NAME=job_info['node_name'],
            MEMORY_PARAM=job_info['memory_param'],
            DURATION=job_info['duration'],
            GPU_RESOURCE_NAME=gpu_resource_name
        )
        
        # è§£æYAML
        import yaml
        job_spec = yaml.safe_load(job_yaml)
        
        # åˆ›å»ºJob
        batch_v1.create_namespaced_job(
            namespace='gpu-health-expert',
            body=job_spec
        )
        
        logger.info(f"Kubernetesçƒ§æœºæµ‹è¯•Jobåˆ›å»ºæˆåŠŸ: {job_info['job_id']}")
        return True
        
    except Exception as e:
        logger.error(f"åˆ›å»ºKubernetesçƒ§æœºæµ‹è¯•Jobå¤±è´¥: {e}")
        return False

@app.route('/api/burnin/jobs', methods=['GET'])
@get_rate_limit_decorator()
def get_burnin_jobs():
    """è·å–çƒ§æœºæµ‹è¯•Jobåˆ—è¡¨"""
    client_ip = request.remote_addr
    
    try:
        # è¿”å›Jobåˆ—è¡¨
        jobs = []
        for job_id, job_info in burnin_jobs.items():
            jobs.append({
                'job_id': job_id,
                'node_name': job_info['node_name'],
                'memory_param': job_info['memory_param'],
                'duration': job_info['duration'],
                'status': job_info['status'],
                'progress': job_info['progress'],
                'created_at': job_info['created_at'],
                'gpu_count': len(job_info['gpu_metrics'])
            })
        
        # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
        jobs.sort(key=lambda x: x['created_at'], reverse=True)
        
        return jsonify({
            'success': True,
            'jobs': jobs
        })
        
    except Exception as e:
        logger.error(f"è·å–çƒ§æœºæµ‹è¯•Jobåˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f'è·å–çƒ§æœºæµ‹è¯•Jobåˆ—è¡¨å¤±è´¥: {str(e)}'
        }), 500

@app.route('/api/burnin/jobs/<job_id>', methods=['GET'])
@get_rate_limit_decorator()
def get_burnin_job_status(job_id):
    """è·å–çƒ§æœºæµ‹è¯•JobçŠ¶æ€"""
    client_ip = request.remote_addr
    
    try:
        if job_id not in burnin_jobs:
            return jsonify({
                'success': False,
                'error': 'Jobä¸å­˜åœ¨'
            }), 404
        
        job_info = burnin_jobs[job_id]
        
        return jsonify({
            'success': True,
            'job': job_info
        })
        
    except Exception as e:
        logger.error(f"è·å–çƒ§æœºæµ‹è¯•JobçŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f'è·å–çƒ§æœºæµ‹è¯•JobçŠ¶æ€å¤±è´¥: {str(e)}'
        }), 500

@app.route('/api/burnin/stop', methods=['POST'])
@get_rate_limit_decorator()
def stop_burnin_job():
    """åœæ­¢çƒ§æœºæµ‹è¯•Job"""
    client_ip = request.remote_addr
    
    try:
        data = request.get_json()
        if not data or 'job_id' not in data:
            return jsonify({
                'success': False,
                'error': 'ç¼ºå°‘job_idå‚æ•°'
            }), 400
        
        job_id = data['job_id']
        
        if job_id not in burnin_jobs:
            return jsonify({
                'success': False,
                'error': 'Jobä¸å­˜åœ¨'
            }), 404
        
        # æ›´æ–°JobçŠ¶æ€
        burnin_jobs[job_id]['status'] = 'stopping'
        
        # åˆ é™¤Kubernetes Job
        success = delete_job_with_kubernetes_client(job_id)
        
        if success:
            burnin_jobs[job_id]['status'] = 'stopped'
            logger.info(f"çƒ§æœºæµ‹è¯•Jobåœæ­¢æˆåŠŸ: {job_id}")
        else:
            burnin_jobs[job_id]['status'] = 'stop_failed'
            logger.error(f"çƒ§æœºæµ‹è¯•Jobåœæ­¢å¤±è´¥: {job_id}")
        
        return jsonify({
            'success': success,
            'message': 'çƒ§æœºæµ‹è¯•Jobåœæ­¢æˆåŠŸ' if success else 'çƒ§æœºæµ‹è¯•Jobåœæ­¢å¤±è´¥'
        })
        
    except Exception as e:
        logger.error(f"åœæ­¢çƒ§æœºæµ‹è¯•Jobå¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f'åœæ­¢çƒ§æœºæµ‹è¯•Jobå¤±è´¥: {str(e)}'
        }), 500

@app.route('/api/burnin/metrics', methods=['POST'])
def receive_burnin_metrics():
    """æ¥æ”¶çƒ§æœºæµ‹è¯•æŒ‡æ ‡"""
    try:
        data = request.get_json()
        logger.info(f"æ”¶åˆ°çƒ§æœºæµ‹è¯•æŒ‡æ ‡æ•°æ®: {data}")
        
        if not data:
            return jsonify({
                'success': False,
                'error': 'è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º'
            }), 400
        
        job_id = data.get('job_id')
        if not job_id:
            return jsonify({
                'success': False,
                'error': 'ç¼ºå°‘job_idå‚æ•°'
            }), 400
        
        # æ›´æ–°JobæŒ‡æ ‡
        if job_id in burnin_jobs:
            # æ›´æ–°æ•´ä¸ªèŠ‚ç‚¹æ•°æ®
            burnin_jobs[job_id].update({
                'progress': data.get('progress', 0),
                'gpus': data.get('gpus', []),
                'total_gflops': data.get('total_gflops', 0),
                'total_errors': data.get('total_errors', 0),
                'avg_temperature': data.get('avg_temperature', 0),
                'gpu_count': data.get('gpu_count', 0),
                'last_update': data.get('timestamp', datetime.now().isoformat())
            })
            
            # æ›´æ–°çŠ¶æ€
            if data.get('status') == 'completed':
                burnin_jobs[job_id]['status'] = 'completed'
            elif data.get('status') == 'failed':
                burnin_jobs[job_id]['status'] = 'failed'
        
        # å­˜å‚¨åˆ°æŒ‡æ ‡å†å²
        if job_id not in burnin_metrics:
            burnin_metrics[job_id] = []
        
        burnin_metrics[job_id].append({
            'timestamp': datetime.now().isoformat(),
            'data': data
        })
        
        # ä¿æŒæœ€è¿‘1000æ¡è®°å½•
        if len(burnin_metrics[job_id]) > 1000:
            burnin_metrics[job_id] = burnin_metrics[job_id][-1000:]
        
        # é€šçŸ¥WebSocketå®¢æˆ·ç«¯
        notify_burnin_status_change(job_id, 'metrics_update', data)
        
        return jsonify({
            'success': True,
            'message': 'æŒ‡æ ‡æ¥æ”¶æˆåŠŸ'
        })
        
    except Exception as e:
        logger.error(f"æ¥æ”¶çƒ§æœºæµ‹è¯•æŒ‡æ ‡å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f'æ¥æ”¶çƒ§æœºæµ‹è¯•æŒ‡æ ‡å¤±è´¥: {str(e)}'
        }), 500

@app.route('/api/burnin/stream', methods=['GET'])
def burnin_status_stream():
    """çƒ§æœºæµ‹è¯•çŠ¶æ€æµ"""
    def generate():
        client_queue = queue.Queue()
        burnin_clients.add(client_queue)
        logger.info(f"æ–°çš„çƒ§æœºæµ‹è¯•SSEå®¢æˆ·ç«¯å·²è¿æ¥ï¼Œå½“å‰è¿æ¥æ•°: {len(burnin_clients)}")
        
        try:
            while True:
                try:
                    # å‘é€å¿ƒè·³
                    yield f"data: {json.dumps({'type': 'heartbeat', 'timestamp': datetime.now().isoformat()})}\n\n"
                    
                    # æ£€æŸ¥é˜Ÿåˆ—ä¸­çš„æ¶ˆæ¯
                    try:
                        message = client_queue.get(timeout=30)
                        yield f"data: {json.dumps(message)}\n\n"
                    except queue.Empty:
                        continue
                        
                except Exception as e:
                    logger.error(f"çƒ§æœºæµ‹è¯•SSEæµé”™è¯¯: {e}")
                    break
        finally:
            burnin_clients.discard(client_queue)
            logger.info(f"çƒ§æœºæµ‹è¯•SSEå®¢æˆ·ç«¯æ–­å¼€è¿æ¥ï¼Œå½“å‰è¿æ¥æ•°: {len(burnin_clients)}")
    
    return Response(generate(), mimetype='text/event-stream')

def notify_burnin_status_change(job_id, status, data=None):
    """é€šçŸ¥çƒ§æœºæµ‹è¯•çŠ¶æ€å˜åŒ–"""
    global burnin_clients
    
    message = {
        "type": "burnin_status_change",
        "job_id": job_id,
        "status": status,
        "timestamp": datetime.now().isoformat(),
        "data": data or {}
    }
    
    # å‘é€ç»™æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯
    for client_queue in list(burnin_clients):
        try:
            client_queue.put(message)
        except Exception as e:
            logger.error(f"å‘é€çƒ§æœºæµ‹è¯•çŠ¶æ€å˜åŒ–é€šçŸ¥å¤±è´¥: {e}")
            burnin_clients.discard(client_queue)

if __name__ == '__main__':
    # åˆå§‹åŒ–æ•°æ®åº“
    init_db()
    
    # åˆå§‹åŒ–å…±äº«ç›®å½•
    init_shared_directories()
    
    # å¯åŠ¨åå°æ”¶é›†ä»»åŠ¡
    collection_thread = threading.Thread(target=background_collection, daemon=True)
    collection_thread.start()
    
    # å¯åŠ¨çŠ¶æ€æ£€æŸ¥çº¿ç¨‹ï¼ˆå¤šæ–¹æ¡ˆå¤‡é€‰ï¼‰
    start_status_check_thread()
    
    # å¯åŠ¨Flaskåº”ç”¨
    logger.info("å¯åŠ¨GHX (GPU Health Expert) ç»Ÿä¸€æœåŠ¡...")
    app.run(
        host='0.0.0.0',
        port=5000,
        debug=False,
        threaded=True
    )
